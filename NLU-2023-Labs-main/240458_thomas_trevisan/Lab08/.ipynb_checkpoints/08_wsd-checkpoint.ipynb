{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Sense Disambiguation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Understanding\n",
    "    - Lexical Relations\n",
    "    - Word senses in WordNet\n",
    "    - Semantic Similarity (in WordNet)\n",
    "    \n",
    "- Learning how to disambiguate word senses\n",
    "    - Dictionary-based Word Sense Disambiguation with WordNet\n",
    "        - Lesk Algorithm\n",
    "        - Graph-based Methods\n",
    "    - Supervised Word Sense Disambiguation\n",
    "        - Feature Extractions for Word Sense Classification\n",
    "            - Bag-of-Words\n",
    "            - Collocational Features\n",
    "        - Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covered Material\n",
    "\n",
    "- SLP\n",
    "    - [Chapter 23: Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/23.pdf)\n",
    "- NLTK\n",
    "    - [Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "        - Section 5: WordNet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](https://www.nltk.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Word Sense Disambiguation\n",
    "\n",
    "In natural language processing, word sense disambiguation (WSD) is the problem of determining which \"sense\" (meaning) of a word is activated by the use of the word in a particular context, a process which appears to be largely unconscious in people. \n",
    "\n",
    "WSD is a natural classification problem: \n",
    "Given a word and its possible senses, as defined by a dictionary, the objective of WSD is to classify an occurrence of the word in context into one or more of its sense classes. The features of the context (such as neighboring words) provide are used as features for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Human Language is ambiguous\n",
    "    - Syntacting ambiguity\n",
    "        - Resolved by POS-tagging\n",
    "        - Syntactic Parsing\n",
    "    - Lexical ambiguity\n",
    "        - Resolved by Word Sense Disambiguation\n",
    "        - Semantics work at level of word __senses__, not __words__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example__:\n",
    "- NOUN\n",
    "    - 'they pulled the canoe up on the __bank__'\n",
    "    - 'he cashed a check at the __bank__'\n",
    "- VERB\n",
    "    - 'the plane __banked__ steeply'\n",
    "    - '__bank__ on your good education'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Task Variants\n",
    "- __Lexical sample subtask__: only a small selection of words has to be disambiguated\n",
    "    - Supervised machine learning: train a classifier for each word\n",
    "- __All words subtask__: each and every content word in the test corpus has to be disambiguated.\n",
    "    - Data sparseness issue, can't train a classifier for each word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1.2. Evaluation\n",
    "Precision, recall, F1-measure against gold standard data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.3. Lexical Relations\n",
    "Relation between word senses.\n",
    "\n",
    "- __Homonymy__: senses are not related\n",
    "- __Polysemy__: senses are related\n",
    "- __Metonymy__: a thing or concept is referred to by the name of something closely associated with that thing or concept. (e.g. *Rome* for Italian Government)\n",
    "    - It is a subtype of polysemy\n",
    "\n",
    "\n",
    "- __Synonymy__: senses are identical\n",
    "- __Antonymy__: senses are opposite\n",
    "- __Hyponymy__ (specific) (*car is hyponym of vehicle*) and __Hypernymy__ (generic): class-inclusion relationships (*vehicle is hypernymy of car*)\n",
    "- __Meronymy__ (part)(*wheel is part of car*) and __Holonymy__ (whole): the part-whole relation (*car ia holonymy of wheel*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[WordNet](https://wordnet.princeton.edu/) is a lexical database of semantic relations between words that links words into semantic relations including synonyms, hyponyms, and meronyms. \n",
    "\n",
    "Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. \n",
    "\n",
    "\n",
    "__Summary__\n",
    "\n",
    "WordNet is a:\n",
    "- Graph (4 graphs for each of nouns, verbs, adjectives, and adverbs)\n",
    "- Nodes are Synsets (synonyms)\n",
    "- Labeled Edges are Relations between Synsets\n",
    "\n",
    "    - PART-OF\n",
    "    - KIND-OF (IS-A)\n",
    "    - ENTAILMENT\n",
    "    - ANTONYMY\n",
    "    \n",
    "> Senses in WordNet are generally ordered from most to least frequently used, with the most common sense numbered 1.\n",
    "\n",
    "[WordNet Site](https://wordnet.princeton.edu/documentation/wndb5wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import WordNet\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bank.n.01'), Synset('depository_financial_institution.n.01'), Synset('bank.n.03'), Synset('bank.n.04'), Synset('bank.n.05'), Synset('bank.n.06'), Synset('bank.n.07'), Synset('savings_bank.n.02'), Synset('bank.n.09'), Synset('bank.n.10'), Synset('bank.v.01'), Synset('bank.v.02'), Synset('bank.v.03'), Synset('bank.v.04'), Synset('bank.v.05'), Synset('deposit.v.02'), Synset('bank.v.07'), Synset('trust.v.01')]\n"
     ]
    }
   ],
   "source": [
    "# printing senses of a word (including honomymy & polysemy)\n",
    "senses = wordnet.synsets('bank')\n",
    "print(senses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Synset \n",
    "The entity `bank.n.01` is called a __synset__, or \"synonym set\", a collection of synonymous words (or \"lemmas\").\n",
    "\n",
    "The name is composed as `<lemma>.<pos>.<number>` string where: \n",
    "- `<lemma>` is the word's morphological stem \n",
    "- `<pos>` is one of the module attributes `ADJ`, `ADJ_SAT`, `ADV`, `NOUN` or `VERB` \n",
    "- `<number>` is the sense number, counting from `0`\n",
    "\n",
    "Part-of-speech tags appear as below:\n",
    "\n",
    "| POS | in Synset Name |\n",
    "|:----|:---------------|\n",
    "| `wn.NOUN`    | `n`\n",
    "| `wn.VERB`    | `v`\n",
    "| `wn.ADV`     | `r`\n",
    "| `wn.ADJ`     | `a`\n",
    "| `wn.ADJ_SAT` | `s` (satelite adjective, ignore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bank.n.01'),\n",
      " Synset('depository_financial_institution.n.01'),\n",
      " Synset('bank.n.03'),\n",
      " Synset('bank.n.04'),\n",
      " Synset('bank.n.05'),\n",
      " Synset('bank.n.06'),\n",
      " Synset('bank.n.07'),\n",
      " Synset('savings_bank.n.02'),\n",
      " Synset('bank.n.09'),\n",
      " Synset('bank.n.10')]\n",
      "\n",
      "POS: n\n"
     ]
    }
   ],
   "source": [
    "# it's possible to provide part of speech to filter senses as well\n",
    "senses = wordnet.synsets('bank', wordnet.NOUN)\n",
    "pprint(senses)\n",
    "print('')\n",
    "print(\"POS:\",senses[0].pos())  # part-of-speech tag of a synset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each word of a synset can have several meanings, synset represents the single meaning that is common to all words in it. \n",
    "Each synset has a __definition__ and __example__ sentences, that can be accessed using `definition()` and `examples()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sloping land (especially the slope beside a body of water)\n",
      "['they pulled the canoe up on the bank', 'he sat on the bank of the river and watched the currents']\n"
     ]
    }
   ],
   "source": [
    "print(senses[0].definition())\n",
    "print(senses[0].examples())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Lemmatization\n",
    "`wordnet.synsets()` method expects a word to be a __lemma__, i.e. canonical (dictionary) form of a word. In case it does find a word in WordNet, it internally applies morphological transformation rules to strip off affixes untill it finds the form.\n",
    "\n",
    "```\n",
    "MORPHOLOGICAL_SUBSTITUTIONS = {\n",
    "    NOUN: [(\"s\", \"\"), (\"ses\", \"s\"), (\"ves\", \"f\"), (\"xes\", \"x\"), (\"zes\", \"z\"), \n",
    "           (\"ches\", \"ch\"), (\"shes\", \"sh\"), (\"men\", \"man\"), (\"ies\", \"y\"), ],\n",
    "    VERB: [(\"s\", \"\"), (\"ies\", \"y\"), (\"es\", \"e\"), (\"es\", \"\"), \n",
    "           (\"ed\", \"e\"), (\"ed\", \"\"), \n",
    "           (\"ing\", \"e\"), (\"ing\", \"\"), ],\n",
    "    ADJ: [(\"er\", \"\"), (\"est\", \"\"), (\"er\", \"e\"), (\"est\", \"e\")],\n",
    "    ADV: [],\n",
    "}\n",
    "```\n",
    "\n",
    "Those could be applied calling `wordnet.morphy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bank'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.morphy('banked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that only verb synsets are listed\n",
    "wordnet.synsets('banked') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`wordnet.morphy()` is the basis of the WordNet-based Lemmatizer in NLTK. The Lemmatizer can be used as follows, optionally providing a part-of-speech (default is NOUN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank\n",
      "bank\n",
      "bnked\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "print(lem.lemmatize('banks'))\n",
    "print(lem.lemmatize('banked', pos=wordnet.VERB))\n",
    "print(lem.lemmatize('bnked', pos=wordnet.VERB))  # returns the word itself if it cannot find it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.2.1. Lemmas in WordNet\n",
    "In WordNet __Lemma__ is a pairing of words with a synset: `bank.n.01` + `bank`.\n",
    "\n",
    "From a __synset__ we can get:\n",
    "- all its lemmas (`lemmas()`)\n",
    "- all its lemma names (`lemma_names()`)\n",
    "\n",
    "From a __lemma__ we can get:\n",
    "- its name (`name()`)\n",
    "- synset it belongs to (`synset()`)\n",
    "\n",
    "Similar to synsets, we can get all lemmas for a word as well using `lemmas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('bank.n.01.bank'),\n",
      " Lemma('depository_financial_institution.n.01.bank'),\n",
      " Lemma('bank.n.03.bank'),\n",
      " Lemma('bank.n.04.bank'),\n",
      " Lemma('bank.n.05.bank'),\n",
      " Lemma('bank.n.06.bank'),\n",
      " Lemma('bank.n.07.bank'),\n",
      " Lemma('savings_bank.n.02.bank'),\n",
      " Lemma('bank.n.09.bank'),\n",
      " Lemma('bank.n.10.bank'),\n",
      " Lemma('bank.v.01.bank'),\n",
      " Lemma('bank.v.02.bank'),\n",
      " Lemma('bank.v.03.bank'),\n",
      " Lemma('bank.v.04.bank'),\n",
      " Lemma('bank.v.05.bank'),\n",
      " Lemma('deposit.v.02.bank'),\n",
      " Lemma('bank.v.07.bank'),\n",
      " Lemma('trust.v.01.bank')]\n"
     ]
    }
   ],
   "source": [
    "lemmas = wordnet.lemmas('bank')\n",
    "pprint(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank\n",
      "Synset('bank.n.01')\n"
     ]
    }
   ],
   "source": [
    "# Look up lemma directly\n",
    "lemma = wordnet.lemma('bank.n.01.bank')\n",
    "print(lemma.name())\n",
    "print(lemma.synset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('bank.n.01.bank')]\n",
      "['bank']\n"
     ]
    }
   ],
   "source": [
    "# Get Lemmas of a synset\n",
    "print(senses[0].lemmas())\n",
    "print(senses[0].lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Lexical Relations beween Synsets\n",
    "\n",
    "WordNet synsets correspond to abstract concepts that are linked together in a hierarchy from very general (such as `Entity`, `State`, `Event` a.k.a *unique beginners* or *root synsets*) to very specific. \n",
    "\n",
    "Hypernymy/Hyponymy relations are used to navigate the taxonomy using `hypernyms()` and `hyponyms()` methods.\n",
    "\n",
    "- `hypernym_paths()` gets the lists of the hypernym synsets to the root (several paths are possible)\n",
    "- `root_hypernyms()` gets the root synset\n",
    "- `hypernym_distances()` get the path(s) from the synset to the root, counting the distance of each node from the initial node on the way\n",
    "\n",
    "- `max_depth()` returns the length of the longest hypernym path from the synset to the root.\n",
    "- `min_depth()` returns the length of the shortest hypernym path from the synset to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('riverbank.n.01'), Synset('waterside.n.01')]\n",
      "[Synset('slope.n.01')]\n"
     ]
    }
   ],
   "source": [
    "pprint(senses[0].hyponyms())\n",
    "pprint(senses[0].hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Synset('entity.n.01'),\n",
      "  Synset('physical_entity.n.01'),\n",
      "  Synset('object.n.01'),\n",
      "  Synset('geological_formation.n.01'),\n",
      "  Synset('slope.n.01'),\n",
      "  Synset('bank.n.01')]]\n",
      "{(Synset('bank.n.01'), 0),\n",
      " (Synset('entity.n.01'), 5),\n",
      " (Synset('geological_formation.n.01'), 2),\n",
      " (Synset('object.n.01'), 3),\n",
      " (Synset('physical_entity.n.01'), 4),\n",
      " (Synset('slope.n.01'), 1)}\n",
      "[Synset('entity.n.01')]\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# getting paths to the root of the taxonomy\n",
    "pprint(senses[0].hypernym_paths())\n",
    "# getting hypernyms with distances\n",
    "pprint(senses[0].hypernym_distances())\n",
    "# getting the root node\n",
    "pprint(senses[0].root_hypernyms())\n",
    "print(senses[0].max_depth())\n",
    "print(senses[0].min_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read about other relations defined for synsets and lemmas in the [NLTK documentation](http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.wordnet).\n",
    "\n",
    "__Whole description of WordNet methods and structure is out of the scope of the lab.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Lesk Algorithm\n",
    "\n",
    "> \"What we try is to guess the correct word sense by counting overlaps between dictionary definitions of the various senses.\" \n",
    "\n",
    "(Lesk, Michael. \"Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.\" Proceedings of the 5th Annual International Conference on Systems Documentation. ACM, 1986.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.1. Simplified Lesk Algorithm\n",
    "\n",
    "Kilgarriff and Rosenzweig (2000) [English SENSEVAL](http://www.lrec-conf.org/proceedings/lrec2000/pdf/8.pdf)\n",
    "\n",
    "```\n",
    "For each sense s of that word,\n",
    "    set weight(s) to zero.\n",
    "\n",
    "Identify set of unique words W in surrounding sentence.\n",
    "\n",
    "For each word w in W,\n",
    "    for each sense s,\n",
    "        if w occurs in the definition or example sentences of s,\n",
    "            add weight(w) to weight(s).\n",
    "Choose sense with greatest weight(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> `weight(w)` is defined as the [inverse document frequency](https://en.wikipedia.org/wiki/Tf–idf) (IDF) of the word `w` over the definitions and example sentences in the dictionary. The IDF of a word `w` is computed as `-log(p(w))`, where `p(w)` is estimated as the fraction of dictionary \"documents\" -- definition or examples -- which contain the word. \n",
    "\n",
    "$$ IDF = -\\log {|\\{d \\in D : w \\in d\\}| \\over |D|}$$\n",
    "\n",
    "where `w` is the word and `D` is the set of documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.2. Lesk Plus Corpus\n",
    "\n",
    "> LESK-PLUS-CORPUS is as LESK, but also considers the tagged training data, so can be compared with supervised\n",
    "systems. For each word in the sentence containing the test item, it tests whether `w` occurs in the dictionary entry or corpus instances for each candidate sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.3. Simple Lesk with Equal Weights\n",
    "\n",
    "If all words are equally weighted, we compute an overlap.\n",
    "The algorithm becomes simpler.\n",
    "\n",
    "```\n",
    "function SIMPLIFIED LESK(word, sentence) returns best sense of word\n",
    "    best-sense := most frequent sense for word (i.e. first in WordNet)\n",
    "    max-overlap := 0\n",
    "    context := set of words in sentence\n",
    "    for each sense in senses of word do\n",
    "        signature := set of words in gloss and examples of sense\n",
    "        overlap := COMPUTE_OVERLAP(signature, context)\n",
    "        if overlap > max-overlap then\n",
    "            max-overlap := overlap\n",
    "            best-sense := sense\n",
    "    end\n",
    "return(best-sense)\n",
    "```\n",
    "\n",
    "```\n",
    "COMPUTE OVERLAP returns the number of words in common between two sets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Improvements\n",
    "\n",
    "- Removing stop words\n",
    "    - IDF makes them weight less in Simplified Lesk by Kilgarriff and Rosenzweig (2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.4. Using Lesk in NLTK\n",
    "NLTK provide the implementation of the Lesk Algorithm is [`wsd` module](https://www.nltk.org/_modules/nltk/wsd.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01')\n",
      "sloping land (especially the slope beside a body of water)\n",
      "Synset('bank.n.01')\n",
      "Synset('riverbank.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "sense = lesk('Jane sat on the sloping bank of a river beside the water'.split(), 'bank')\n",
    "print(sense)\n",
    "print(sense.definition())\n",
    "\n",
    "# possible to specify the POS\n",
    "print(lesk('Jane sat on the sloping bank of a river beside the water'.split(), \n",
    "           'bank', \n",
    "           pos=wordnet.NOUN))\n",
    "\n",
    "# possible to specify the synsets to choose from\n",
    "print(lesk('Jane sat on the sloping bank of a river beside the water'.split(), \n",
    "           'bank', \n",
    "           synsets=wordnet.synsets('riverbank')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3.5. Alternative Implementations of Lesk in `pywsd`\n",
    "\n",
    "[`pywsd` library](https://github.com/alvations/pywsd) provides several variants of the Lesk algorithm.\n",
    "\n",
    "\n",
    "\n",
    "- Original Lesk (Lesk, 1986) -- also *simplified*\n",
    "- Adapted/Extended Lesk (Banerjee and Pederson, 2002/2003)\n",
    "- Simple Lesk (with definition, example(s) and hyper+hyponyms)\n",
    "- Cosine Lesk (use cosines to calculate overlaps instead of using raw counts)\n",
    "\n",
    "Unfortunatelly, it has some compatibility issues. However, can be consulted for implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercises\n",
    "Even though NLTK states that it implements Original Lesk Algorithm, in fact it is a Simplified Lesk Algorithm, that doesn't consider examples, and computes overlaps like the original. \n",
    "\n",
    "In the original algorithm context is computed differently. <mark style=\"background-color: rgba(0, 255, 0, 0.2)\">Instead of comparing a target word's signature with the context words, the target signature is compared with the signatures of each of the context words. </mark>\n",
    "\n",
    "Implement the Original Lesk Algorithm (modifying NLTK's, see pseudocode above)\n",
    "Todo list:\n",
    "- Complete lesk simplified\n",
    "- Preprocessing:\n",
    "    - compute pos-tag with `nltk.pos_tag`\n",
    "    - remove stopwords\n",
    "        - `from nltk.corpus import stopwords`\n",
    "        - `stopwords.words('english')`\n",
    "\n",
    "- take the majority decision (the sense predicted most frequently)\n",
    "\n",
    "POS tags reminder:\n",
    "\n",
    "| POS | in Synset Name |\n",
    "|:----|:---------------|\n",
    "| `wn.NOUN`    | `n`\n",
    "| `wn.VERB`    | `v`\n",
    "| `wn.ADV`     | `r`\n",
    "| `wn.ADJ`     | `a`\n",
    "| `wn.ADJ_SAT` | `s` (satelite adjective, ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Lesk simplified\n",
    "def lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "\n",
    "    context = set(context_sentence)\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = wordnet.synsets(ambiguous_word)\n",
    "    # Filter by pos-tag\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    #print(context)\n",
    "    #for ss in synsets:\n",
    "    #    print(len(context & set(nltk.word_tokenize(ss.definition()))))\n",
    "    len_overlap_list = [len(context & set(nltk.word_tokenize(ss.definition()))) for ss in synsets]\n",
    "    sense = len_overlap_list.index(max(len_overlap_list))\n",
    "\n",
    "    return synsets[sense]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# A bit of preprocessing \n",
    "def preprocess(text):\n",
    "    mapping = {\"NOUN\": wordnet.NOUN, \"VERB\": wordnet.VERB, \"ADJ\": wordnet.ADJ, \"ADV\": wordnet.ADV}\n",
    "    sw_list = stopwords.words('english')\n",
    "    lem = WordNetLemmatizer()\n",
    "    # tokenize, if input is text\n",
    "    tokens = nltk.word_tokenize(text) if type(text) is str else text\n",
    "    # compute pos-tag\n",
    "    tagged = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "    # lowercase\n",
    "    tagged = [(w.lower(), p) for w, p in tagged]\n",
    "    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)\n",
    "    tagged = [(w, p) for w, p in tagged if p in mapping]\n",
    "    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)\n",
    "    tagged = [(w, mapping.get(p, p)) for w, p in tagged]\n",
    "    # remove stopwords\n",
    "    tagged = [(w, p) for w, p in tagged if w not in sw_list]\n",
    "    # lemmatize\n",
    "    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]\n",
    "    # unique the list\n",
    "    tagged = list(set(tagged))\n",
    "    \n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sense_definitions(context):\n",
    "    # input is text or list of strings\n",
    "    lemma_tags = preprocess(context)\n",
    "\n",
    "    # let's get senses for each\n",
    "    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]\n",
    "\n",
    "    # let's get their definitions\n",
    "    definitions = []\n",
    "    for raw_word, sense_list in senses:\n",
    "        if len(sense_list) > 1:\n",
    "            # let's tokenize, lowercase & remove stop words \n",
    "            def_list = []\n",
    "            for s in sense_list:\n",
    "                defn = s.definition()\n",
    "                # let's use the same preprocessing\n",
    "                tags = preprocess(defn)\n",
    "                toks = [l for w, l, p in tags]\n",
    "                def_list.append((s, toks))\n",
    "            definitions.append((raw_word, def_list))\n",
    "    return definitions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sense(words, sense_list):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)\n",
    "    return val, sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False):\n",
    "    \n",
    "    context_senses = get_sense_definitions(context_sentence)\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            score, sense = get_top_sense(sense[1], synsets)\n",
    "            scores.append((score, sense))\n",
    "        \n",
    "    if majority:\n",
    "        # We remove 0 scores, senses without overlapping\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter(scores).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = sorted(scores)[0]\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense from lesk original Synset('savings_bank.n.02')\n",
      "Sense from lesk simplified Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "text = \"Jane sat on the sloping bank of a river beside the water\"\n",
    "word = \"bank\"\n",
    "print(\"Sense from lesk original\", original_lesk(text, word, majority=True))\n",
    "print(\"Sense from lesk simplified\", lesk(text, word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Graph-based Methods on WordNet for WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4.1. Maximum Relatedness Disambiguation\n",
    "\n",
    "Pedersen et al. (2003) [Maximizing Semantic Relatedness to Perform Word Sense Disambiguation](https://www.d.umn.edu/~tpederse/Pubs/max-sem-relate.pdf)\n",
    "\n",
    "\n",
    "```\n",
    "w = words\n",
    "\n",
    "foreach sense s[t][i] of target word w[t]$\n",
    "    set score[i] = 0\n",
    "    foreach word w[j] in window of context\n",
    "        skip to next word if j == t\n",
    "\n",
    "        foreach sense s[j][k] of w[j]\n",
    "            temp_score[j] = relatedness(s[t][i], s[j][k])\n",
    "\n",
    "        winning_score = highest score in array temp_score[]\n",
    "\n",
    "        if (winning_score > threshold)\n",
    "            set score[i] = score[i] + winning_score\n",
    "            \n",
    "return i, such that score[i] >= score[j] , for all j, 1 <= j <= n, n = number of words in sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.1. How do we define relatedness?\n",
    "\n",
    "- Similar words are near-synonyms: e.g. *car*, *motorcycle*\n",
    "- Related words can be related any way: e.g. *car*, *fuel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Thesaurus-based similarity\n",
    "    - words have similar definitions (Lesk)\n",
    "    - words are close to each other in hypernym hierarchy (graph-based)\n",
    "- Distributional similarity\n",
    "    - do words apprear in similar distributional contexts\n",
    "    - __distributional (vector) semantics__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the similarity between *dime* and *nickel* and between *nickel* and *credit card*: \n",
    "\n",
    "![](https://i.postimg.cc/tJn0NMgm/Screenshot-2023-01-03-at-10-26-20.png)\n",
    "\n",
    "[*Original source (Resnik, 1995)*](https://arxiv.org/pdf/cmp-lg/9511007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.2. Path-based Similarity\n",
    "\n",
    "Two concepts (senses/synsets) are similar if they are near each other in the thesaurus hierarchy\n",
    "- have a __short path__ between them (1 + number of edges between nodes)\n",
    "- path to themselves has distance `1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### NLTK Path Based Metrics\n",
    "\n",
    "- `synset1.path_similarity(synset2)`: Return a score denoting how similar two word senses are, based on the __shortest path__ that connects the senses in the is-a (hypernym/hypnoym) taxonomy. The score is in the range 0 to 1, computed as `1/path_length`\n",
    "- `synset1.lch_similarity(synset2)`: __Leacock-Chodorow Similarity__: Return a score denoting how similar two word senses are, based on the shortest path that connects the senses and the maximum depth of the taxonomy in which the senses occur. The relationship is given as `-log(p/2d)` where `p` is the shortest path length and `d` the taxonomy depth.\n",
    "- `synset1.wup_similarity(synset2)`: __Wu-Palmer Similarity__: Return a score denoting how similar two word senses are, based on the depth of the two senses in the taxonomy and that of their __Least Common Subsumer__ (most specific ancestor node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a large natural stream of water (larger than a creek)\n",
      "sloping land (especially the slope beside a body of water)\n",
      "a financial institution that accepts deposits and channels the money into lending activities\n",
      "0.1111111111111111\n",
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "bank_r = wordnet.synsets('bank')[0]\n",
    "bank_f = wordnet.synsets('bank')[1]\n",
    "river = wordnet.synsets('river')[0]\n",
    "\n",
    "print(river.definition())\n",
    "print(bank_r.definition())\n",
    "print(bank_f.definition())\n",
    "\n",
    "print(bank_r.path_similarity(river))\n",
    "print(bank_f.path_similarity(river))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4403615823901665\n",
      "1.072636802264849\n",
      "0.3333333333333333\n",
      "0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "print(bank_r.lch_similarity(river))\n",
    "print(bank_f.lch_similarity(river))\n",
    "\n",
    "print(bank_r.wup_similarity(river))\n",
    "print(bank_f.wup_similarity(river))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 4.1.3. Information Content Similarity\n",
    "\n",
    "- Path-based similarity issues\n",
    "    - each edge is has equal distance; however nodes high in hierarchy are more abstract\n",
    "- Better metric\n",
    "    - each edge has independent cost\n",
    "    - nodes connected through higher-level (abstract) nodes are less similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Information Content\n",
    "- Trained on a corpus\n",
    "- `P(c)` the probability of a concept `c` in a corpus\n",
    "    $$ P(c) = \\frac{\\sum_{w \\in \\text{words}(c)}\\text{count}(c)}{N}$$\n",
    "    where $\\text{words}(c)$ is set of all words that are children of concept $c$. $N$ is the total number of nouns observed. \n",
    "- All words are members of the root node (e.g. `Entity`); thus, `P(root) = 1`\n",
    "- The lower a node in hierarchy, the lower its probability\n",
    "\n",
    "- Information Content $$IC(c) = -log(P(c))$$\n",
    "- Most Informative Subsumer (Lowest Common Subsumer) $LCS(c_1, c_2)$ is the lowest node in the hierarchy subsuming both $c_1$ and $c_2$\n",
    "\n",
    "If you are further interested in this you should read the paper of [Resnik](https://arxiv.org/pdf/cmp-lg/9511007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### NLTK Information Content Based Metrics\n",
    "- `res_similarity(other, ic)`: __Resnik Similarity__: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node). Computed as `IC(lcs) = -log(P(lcs))`. Lower is more similar.\n",
    "- `lin_similarity(other, ic)`: __Lin Similarity__: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation `2 * IC(lcs) / (IC(s1) + IC(s2))`.\n",
    "- `jcn_similarity(other, ic)`: __Jiang-Conrath Similarity__: Return a score denoting how similar two word senses are, based on the Information Content (IC) of the Least Common Subsumer (most specific ancestor node) and that of the two input Synsets. The relationship is given by the equation `1 / (IC(s1) + IC(s2) - 2 * IC(lcs))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/thomas/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# getting pre-computed ic of the semcor corpus (large sense tagged corpus)\n",
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6143639493869085\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(bank_r.res_similarity(river, semcor_ic))\n",
    "print(bank_f.res_similarity(river, semcor_ic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "print(bank_r.lin_similarity(bank_r, semcor_ic))\n",
    "print(bank_f.lin_similarity(river, semcor_ic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e+300\n",
      "0.06248754962684728\n"
     ]
    }
   ],
   "source": [
    "print(bank_r.jcn_similarity(bank_r, semcor_ic))\n",
    "print(bank_f.jcn_similarity(river, semcor_ic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "Extend Lesk algorithm (function) to use similarity metrics instead of just overlaps\n",
    "- make it a keyword argument to allow different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef pedersen(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \\n                    synsets=None, threshold=0.1):\\n    \\n    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\\n\\n    if synsets is None:\\n        synsets = get_sense_definitions(ambiguous_word)[0][1]\\n\\n    if pos:\\n        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\\n\\n    if not synsets:\\n        return None\\n    \\n    synsets_scores = {}\\n    for ss_tup in synsets:\\n        ss = ss_tup[0]\\n        if ss not in synsets_scores:\\n            synsets_scores[ss] = 0\\n        for senses in context_senses:\\n            scores = []\\n            for sense in senses[1]:\\n                if similarity == \"path\":\\n                    try:\\n                        # Append path similarity similarity between ambiguous word and senses from the context\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))    \\n                elif similarity == \"lch\":\\n                    try:\\n                        # Append LCH similarity similarity between ambiguous word and senses from the context\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))\\n                elif similarity == \"wup\":\\n                    try:\\n                        # Append WUP similarity similarity between ambiguous word and senses from the context\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))\\n                elif similarity == \"resnik\":\\n                    try:\\n                        # Append Resnik similarity similarity between ambiguous word and senses from the context\\n                        # Don\\'t forget semicor_ic\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))\\n                elif similarity == \"lin\":\\n                    try:\\n                        # Append lin similarity similarity between ambiguous word and senses from the context\\n                        # Don\\'t forget semicor_ic\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))\\n                elif similarity == \"jiang\":\\n                    try:\\n                        # Append Jiang similarity similarity between ambiguous word and senses from the context\\n                        # Don\\'t forget semicor_ic\\n                        scores.append((#Add similarity, ss))\\n                    except:\\n                        scores.append((0, ss))\\n                else:\\n                    print(\"Similarity metric not found\")\\n                    return None\\n            value, sense = max(scores)\\n            if value > threshold:\\n                synsets_scores[sense] = synsets_scores[sense] + value\\n\\n    values = list(synsets_scores.values())\\n    if sum(values) == 0:\\n        print(\\'Warning: all the scores are 0\\')\\n    senses = list(synsets_scores.keys())\\n    best_sense_id = values.index(max(values))\\n                            \\n    return senses[best_sense_id]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "def get_top_sense_sim(context_sense, sense_list, similarity):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    scores = []\n",
    "    for sense in sense_list:\n",
    "        ss = sense[0]\n",
    "        if similarity == \"path\":\n",
    "            try:\n",
    "                scores.append((ss.path_similarity(context_sense, semcoir_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))    \n",
    "        elif similarity == \"lch\":\n",
    "            try:\n",
    "                scores.append((ss.lch_similarity(context_sense, semcoir_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"wup\":\n",
    "            try:\n",
    "                scores.append((ss.wup_similarity(context_sense), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"resnik\":\n",
    "            try:\n",
    "                scores.append((ss.res_similarity(context_sense, semcoir_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"lin\":\n",
    "            try:\n",
    "                scores.append((ss.lin_similarity(context_sense, semcoir_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"jiang\":\n",
    "            try:\n",
    "                scores.append((ss.jcn_similarity(context_sense, semcoir_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        else:\n",
    "            print(\"Similarity metric not found\")\n",
    "            return None\n",
    "    val, sense = max(scores)\n",
    "    return val, sense\n",
    "\n",
    "    \n",
    "def lesk_similarity(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, synsets=None, majority=True):\n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense_sim(sense[0], synsets, similarity))\n",
    "                    \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "                    \n",
    "    # Majority voting as before    \n",
    "    if majority:\n",
    "        # We remove 0 scores, senses without overlapping\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0][1]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter(scores).most_common(1)[0][0][1]\n",
    "    else:\n",
    "        best_sense = sorted(scores)[0]\n",
    "    \n",
    "    return best_sense\n",
    "        \n",
    "'''\n",
    "def pedersen(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, threshold=0.1):\n",
    "    \n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    synsets_scores = {}\n",
    "    for ss_tup in synsets:\n",
    "        ss = ss_tup[0]\n",
    "        if ss not in synsets_scores:\n",
    "            synsets_scores[ss] = 0\n",
    "        for senses in context_senses:\n",
    "            scores = []\n",
    "            for sense in senses[1]:\n",
    "                if similarity == \"path\":\n",
    "                    try:\n",
    "                        # Append path similarity similarity between ambiguous word and senses from the context\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))    \n",
    "                elif similarity == \"lch\":\n",
    "                    try:\n",
    "                        # Append LCH similarity similarity between ambiguous word and senses from the context\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"wup\":\n",
    "                    try:\n",
    "                        # Append WUP similarity similarity between ambiguous word and senses from the context\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"resnik\":\n",
    "                    try:\n",
    "                        # Append Resnik similarity similarity between ambiguous word and senses from the context\n",
    "                        # Don't forget semicor_ic\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"lin\":\n",
    "                    try:\n",
    "                        # Append lin similarity similarity between ambiguous word and senses from the context\n",
    "                        # Don't forget semicor_ic\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"jiang\":\n",
    "                    try:\n",
    "                        # Append Jiang similarity similarity between ambiguous word and senses from the context\n",
    "                        # Don't forget semicor_ic\n",
    "                        scores.append((#Add similarity, ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                else:\n",
    "                    print(\"Similarity metric not found\")\n",
    "                    return None\n",
    "            value, sense = max(scores)\n",
    "            if value > threshold:\n",
    "                synsets_scores[sense] = synsets_scores[sense] + value\n",
    "\n",
    "    values = list(synsets_scores.values())\n",
    "    if sum(values) == 0:\n",
    "        print('Warning: all the scores are 0')\n",
    "    senses = list(synsets_scores.keys())\n",
    "    best_sense_id = values.index(max(values))\n",
    "                            \n",
    "    return senses[best_sense_id]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lesk Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Symplified lesk Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Graph-based lesk Synset('savings_bank.n.02') savings_bank.n.02\n"
     ]
    }
   ],
   "source": [
    "text = \"Jane sat on the sloping bank of a river beside the water\".split()\n",
    "word = \"bank\"\n",
    "sense = original_lesk(text, word, majority=True)\n",
    "print('Original lesk', sense, sense.definition())\n",
    "sense = lesk(text, word)\n",
    "print('Symplified lesk', sense, sense.definition())\n",
    "sense = lesk_similarity(text, word, \"resnik\")\n",
    "print('Graph-based lesk', sense, sense.definition())\n",
    "#sense = pedersen(text, word, similarity=\"path\", threshold=0.1)\n",
    "#print(\"Pedersen\", sense, sense.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Evaluation on Senseval 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 5.1. Senseval Corpus\n",
    "The Senseval 2 Corpus contains data intended to train word-sense disambiguation classifiers. \n",
    "It contains data for four words: `hard`, `interest`, `line`, and `serve`. Let's use `interest` portion to illustrate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('senseval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Corpus instances are stored as:\n",
    "- `context` - POS-tagged context sentence\n",
    "- `position` - index of the target word in a context sentence\n",
    "- `senses` - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import senseval\n",
    "\n",
    "inst = senseval.instances('interest.pos')[0]\n",
    "\n",
    "print(inst.position, inst.context, inst.senses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.1. Mapping Senseval Senses to WordNet\n",
    "\n",
    "Senseval labels are not compatible with WordNet 3.0; thus, let's manually create a mapping.\n",
    "\n",
    "__Senses for *interest* in Longman Dictionary__\n",
    "- Sense 1 =  361 occurrences (15%) - readiness to give attention\n",
    "- Sense 2 =   11 occurrences (01%) - quality of causing attention to be given to\n",
    "- Sense 3 =   66 occurrences (03%) - activity, etc. that one gives attention to\n",
    "- Sense 4 =  178 occurrences (08%) - advantage, advancement or favor\n",
    "- Sense 5 =  500 occurrences (21%) - a share in a company or business\n",
    "- Sense 6 = 1252 occurrences (53%) - money paid for the use of money"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# definitions of \"interest\"'s synsets in WordNet\n",
    "iss = wordnet.synsets('interest', pos='n')\n",
    "for ss in iss:\n",
    "    print(ss, ss.definition())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create mapping from convenience\n",
    "mapping = {\n",
    "    'interest_1': 'interest.n.01',\n",
    "    'interest_2': 'interest.n.03',\n",
    "    'interest_3': 'pastime.n.01',\n",
    "    'interest_4': 'sake.n.01',\n",
    "    'interest_5': 'interest.n.05',\n",
    "    'interest_6': 'interest.n.04',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.1.2. Evaluation\n",
    "\n",
    "- Let's use accuracy for simplicity\n",
    "- Also demonstrating per-class precision, recall, and f-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure, accuracy\n",
    "\n",
    "refs = {k: set() for k in mapping.values()}\n",
    "hyps = {k: set() for k in mapping.values()}\n",
    "refs_list = []\n",
    "hyps_list = []\n",
    "\n",
    "# since WordNet defines more senses, let's restrict predictions\n",
    "synsets = [ss for ss in wordnet.synsets('interest', pos='n') if ss.name() in mapping.values()]\n",
    "\n",
    "for i, inst in enumerate(senseval.instances('interest.pos')):\n",
    "    txt = [t[0] for t in inst.context]\n",
    "    raw_ref = inst.senses[0] # let's get first sense\n",
    "    hyp = lesk(txt, txt[inst.position], synsets=synsets).name()\n",
    "    \n",
    "    ref = mapping.get(raw_ref)\n",
    "    \n",
    "    # for precision, recall, f-measure        \n",
    "    refs[ref].add(i)\n",
    "    hyps[hyp].add(i)\n",
    "    \n",
    "    # for accuracy\n",
    "    refs_list.append(ref)\n",
    "    hyps_list.append(hyp)\n",
    "\n",
    "print(\"Acc:\", round(accuracy(refs_list, hyps_list), 3))\n",
    "\n",
    "for cls in hyps.keys():\n",
    "    p = precision(refs[cls], hyps[cls])\n",
    "    r = recall(refs[cls], hyps[cls])\n",
    "    f = f_measure(refs[cls], hyps[cls], alpha=1)\n",
    "    \n",
    "    print(\"{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}\".format(cls, p, r, f, len(refs[cls])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "- Evaluate Original Lesk (your implementation on Senseval's `interest`)\n",
    "- You can also easily evalutate Lesk similarity that we have seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import precision, recall, f_measure, accuracy\n",
    "\n",
    "refs = {k: set() for k in mapping.values()}\n",
    "hyps = {k: set() for k in mapping.values()}\n",
    "refs_list = []\n",
    "hyps_list = []\n",
    "\n",
    "# since WordNet defines more senses, let's restrict predictions\n",
    "\n",
    "synsets = []\n",
    "for ss in wordnet.synsets('interest', pos='n'):\n",
    "    if ss.name() in mapping.values():\n",
    "        # You need to preporecess the definitions\n",
    "        # Give a look at the preprocessing function that we defined above \n",
    "        defn = # estract the defitions\n",
    "        tags = # Preproccess the definition\n",
    "        toks = # From tags extract the tokens\n",
    "        synsets.append((ss,toks))\n",
    "\n",
    "for i, inst in enumerate(senseval.instances('interest.pos')):\n",
    "    txt = [t[0] for t in inst.context]\n",
    "    raw_ref = inst.senses[0] # let's get first sense\n",
    "    hyp = # Use original LESK or similarity LESK, for input parameters copy paste from above.\n",
    "    ref = mapping.get(raw_ref)\n",
    "    \n",
    "    # for precision, recall, f-measure        \n",
    "    refs[ref].add(i)\n",
    "    hyps[hyp].add(i)\n",
    "    \n",
    "    # for accuracy\n",
    "    refs_list.append(ref)\n",
    "    hyps_list.append(hyp)\n",
    "\n",
    "print(\"Acc:\", round(accuracy(refs_list, hyps_list), 3))\n",
    "\n",
    "for cls in hyps.keys():\n",
    "    p = precision(refs[cls], hyps[cls])\n",
    "    r = recall(refs[cls], hyps[cls])\n",
    "    f = f_measure(refs[cls], hyps[cls], alpha=1)\n",
    "    \n",
    "    print(\"{:15s}: p={:.3f}; r={:.3f}; f={:.3f}; s={}\".format(cls, p, r, f, len(refs[cls])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Supervised Learning for WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 6.1. Features for WSD\n",
    "- Bag-of-Words (already covered)\n",
    "- Collocational features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 6.1.1. Bag-of-Words (BOW) Classification (recap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selecinsttion import StratifiedKFold\n",
    "\n",
    "data = [\" \".join([t[0] for t in inst.context]) for inst in senseval.instances('interest.pos')]\n",
    "lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]\n",
    "\n",
    "print(data[0])\n",
    "print(lbls[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "lblencoder = LabelEncoder()\n",
    "\n",
    "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "\n",
    "# encoding labels for multi-calss\n",
    "lblencoder.fit(lbls)\n",
    "labels = lblencoder.transform(lbls)\n",
    "\n",
    "scores = cross_validate(classifier, vectors, labels, cv=stratified_split, scoring=['f1_micro'])\n",
    "\n",
    "print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 6.1.2. Collocational Features\n",
    "- Assume +/-n words window from target\n",
    "\n",
    "e.g. n=2\n",
    "\n",
    "`... managers expect further [declines in] [interest] [rates .]`\n",
    "\n",
    "- $w_{-1}$ : `declines`\n",
    "- $w_{-2}$ : `in`\n",
    "- $w_0$ __target__ : `interest`\n",
    "- $w_{+1}$ : `rates`\n",
    "- $w_{+2}$ : `.`\n",
    "\n",
    "- POS-tags of these words\n",
    "- word ngrams in window +/-3 are common\n",
    "    - ngram(-3): declines in interest\n",
    "    - ngram(-2): in interest\n",
    "    - ngram(1): interest\n",
    "    - ngram(2): interest rates\n",
    "    - ngram(3): interest rates .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##### Using Collocational Features in scikit-learn\n",
    "- represent features as dict\n",
    "- use `DictVectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def collocational_features(inst):\n",
    "    p = inst.position\n",
    "    return {\n",
    "        \"w-2_word\": 'NULL' if p < 2 else inst.context[p-2][0],\n",
    "        \"w-1_word\": 'NULL' if p < 1 else inst.context[p-1][0],\n",
    "        \"w+1_word\": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],\n",
    "        \"w+2_word\": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data_col = [collocational_features(inst) for inst in senseval.instances('interest.pos')]\n",
    "print(data_col[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "dvectors = dvectorizer.fit_transform(data_col)\n",
    "\n",
    "scores = cross_validate(classifier, dvectors, labels, cv=stratified_split, scoring=['f1_micro'])\n",
    "\n",
    "print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 6.1.3. Concatenating Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's check shape's for sanity & types (for illustration)\n",
    "print(vectors.shape, type(vectors))\n",
    "print(dvectors.shape, type(dvectors))\n",
    "\n",
    "# types of CountVectorizer and DictVectorizer outputs are different \n",
    "# we need to convert them to the same format\n",
    "uvectors = np.concatenate((vectors.toarray(), dvectors), axis=1)\n",
    "\n",
    "print(uvectors.shape, type(uvectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# cross-validating classifier the usual way\n",
    "scores = cross_validate(classifier, uvectors, labels, cv=stratified_split, scoring=['f1_micro'])\n",
    "\n",
    "print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lab Exercise\n",
    "- Extend collocational features with\n",
    "    - POS-tags\n",
    "    - Ngrams within window\n",
    "- Concatenate BOW and new collocational feature vectors & evaluate\n",
    "- Evaluate Lesk Original and Graph-based (Lesk Similarity or Pedersen) metrics on the same split & compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINISHED: PRINT METRICS AS ONE AND NOT FOR EVERY BATCH. \n",
    "#THEN, MAKE IT EXECUTE BOTH ORIGINAL_LESK AND PEDERSEN TOGETHER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /home/thomas/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210221139864944\n",
      "\n",
      "[   1    4    5    6   24   33   47   48   59   62   65   67   68   69\n",
      "   71   81   87   92   96  100  106  107  115  124  125  127  135  150\n",
      "  169  174  184  202  205  211  213  219  232  235  243  246  257  274\n",
      "  285  291  292  294  295  296  304  309  315  317  329  333  339  349\n",
      "  354  356  358  375  392  396  397  398  407  411  418  422  424  443\n",
      "  445  446  455  460  465  475  476  479  480  483  484  498  509  515\n",
      "  517  525  528  532  535  541  543  547  548  554  560  562  570  574\n",
      "  577  580  588  589  593  598  609  610  612  618  620  623  625  634\n",
      "  641  645  655  662  689  691  698  710  711  712  716  717  718  726\n",
      "  732  736  737  738  746  748  758  762  774  781  782  785  788  790\n",
      "  796  799  811  812  814  819  825  831  835  842  845  848  852  860\n",
      "  865  876  878  884  885  889  902  909  910  912  914  919  924  932\n",
      "  942  947  952  958  962  970  973  981  986  992  999 1001 1004 1005\n",
      " 1006 1011 1019 1022 1023 1025 1033 1034 1046 1052 1055 1056 1059 1064\n",
      " 1070 1072 1073 1076 1082 1087 1091 1094 1097 1106 1108 1115 1117 1119\n",
      " 1121 1130 1141 1144 1147 1153 1164 1168 1187 1188 1192 1194 1203 1205\n",
      " 1208 1215 1220 1222 1228 1232 1233 1234 1236 1239 1244 1245 1248 1262\n",
      " 1263 1268 1269 1270 1273 1277 1280 1284 1285 1289 1293 1297 1298 1299\n",
      " 1300 1304 1305 1306 1316 1320 1331 1332 1339 1344 1353 1354 1358 1360\n",
      " 1365 1392 1394 1396 1397 1402 1409 1417 1430 1431 1436 1446 1447 1450\n",
      " 1452 1461 1464 1465 1467 1468 1472 1473 1476 1485 1487 1490 1492 1507\n",
      " 1510 1514 1516 1519 1521 1523 1525 1532 1544 1547 1549 1553 1554 1557\n",
      " 1563 1567 1568 1569 1570 1580 1583 1591 1592 1593 1595 1601 1607 1618\n",
      " 1633 1639 1643 1645 1649 1653 1657 1661 1668 1669 1674 1676 1685 1690\n",
      " 1691 1696 1698 1703 1705 1709 1715 1717 1723 1726 1728 1730 1734 1742\n",
      " 1749 1752 1762 1772 1777 1779 1786 1792 1796 1803 1807 1817 1821 1822\n",
      " 1823 1827 1829 1830 1835 1838 1841 1843 1857 1863 1864 1865 1867 1868\n",
      " 1872 1887 1888 1895 1900 1918 1924 1926 1933 1935 1939 1942 1943 1944\n",
      " 1946 1952 1960 1965 1971 1973 1974 1982 1990 1992 1994 1999 2004 2010\n",
      " 2011 2018 2019 2021 2024 2027 2029 2032 2033 2035 2037 2043 2051 2061\n",
      " 2063 2070 2072 2076 2078 2081 2085 2087 2092 2093 2094 2100 2124 2128\n",
      " 2138 2151 2159 2167 2179 2188 2189 2199 2201 2207 2211 2216 2217 2222\n",
      " 2224 2230 2235 2238 2265 2267 2269 2275 2277 2279 2280 2281 2298 2306\n",
      " 2307 2313 2320 2321 2335 2341 2342 2346 2347 2353 2363 2366]\n",
      "hyps cls is {2230} and ref cls is {1285, 1926, 135, 396, 525, 2320, 914, 532, 150, 790, 1046, 1942, 1690, 1691, 796, 1946, 543, 1056, 1952, 1698, 1827, 2211, 2085, 1830, 2335, 2341, 2346, 2347, 1709, 2222, 560, 2353, 691, 1205, 1592, 825, 315, 1339, 1468, 574, 1087, 1215, 1472, 1601, 67, 68, 580, 842, 970, 1354, 1485, 1487, 81, 1106, 852, 1749, 87, 354, 1507, 2018, 1510, 2151, 2279, 106, 107, 235, 618, 1130, 1263, 1519, 2027, 1653, 1273}\n",
      "hyps cls is {1, 4, 5, 2063, 2070, 24, 2072, 2076, 2078, 33, 2081, 2085, 2087, 2093, 47, 48, 59, 62, 65, 67, 68, 69, 2124, 2128, 87, 2138, 92, 96, 100, 106, 107, 2159, 124, 127, 135, 2189, 150, 2201, 2211, 2216, 169, 2217, 174, 2222, 2224, 184, 2235, 2238, 202, 211, 213, 219, 2267, 2269, 2275, 2277, 2279, 232, 2280, 2281, 235, 246, 257, 2306, 2307, 2313, 2321, 274, 291, 292, 2341, 294, 295, 2346, 2347, 2353, 309, 315, 317, 2366, 329, 339, 349, 354, 356, 358, 396, 398, 407, 418, 422, 446, 455, 460, 465, 475, 479, 483, 484, 515, 517, 525, 528, 532, 535, 541, 543, 547, 548, 554, 560, 562, 577, 580, 588, 589, 593, 598, 609, 612, 620, 634, 641, 645, 655, 662, 689, 691, 710, 712, 717, 718, 726, 732, 736, 737, 746, 762, 781, 782, 785, 788, 790, 796, 799, 811, 814, 819, 825, 835, 842, 848, 852, 860, 876, 878, 884, 885, 889, 902, 909, 910, 914, 919, 932, 942, 947, 952, 958, 962, 970, 981, 986, 992, 999, 1001, 1004, 1011, 1022, 1023, 1025, 1046, 1052, 1059, 1072, 1073, 1076, 1082, 1087, 1094, 1097, 1106, 1108, 1115, 1117, 1119, 1121, 1130, 1147, 1153, 1164, 1168, 1187, 1192, 1194, 1203, 1205, 1208, 1220, 1222, 1228, 1232, 1233, 1234, 1236, 1239, 1244, 1245, 1248, 1262, 1263, 1268, 1269, 1270, 1273, 1277, 1280, 1284, 1285, 1289, 1297, 1298, 1299, 1300, 1304, 1305, 1306, 1316, 1320, 1331, 1339, 1353, 1354, 1360, 1365, 1392, 1396, 1397, 1402, 1409, 1417, 1431, 1436, 1446, 1447, 1450, 1452, 1461, 1464, 1465, 1467, 1468, 1472, 1473, 1476, 1485, 1487, 1490, 1492, 1507, 1510, 1514, 1516, 1519, 1521, 1523, 1525, 1544, 1547, 1549, 1553, 1554, 1563, 1567, 1568, 1569, 1570, 1591, 1592, 1593, 1601, 1607, 1618, 1633, 1639, 1643, 1645, 1649, 1653, 1657, 1668, 1685, 1690, 1691, 1696, 1698, 1703, 1705, 1723, 1726, 1728, 1730, 1734, 1742, 1749, 1752, 1762, 1772, 1777, 1779, 1786, 1792, 1796, 1817, 1821, 1822, 1827, 1829, 1835, 1838, 1841, 1843, 1857, 1863, 1865, 1887, 1888, 1895, 1924, 1926, 1935, 1939, 1942, 1946, 1952, 1960, 1965, 1971, 1973, 1974, 1982, 1990, 1992, 1994, 1999, 2004, 2011, 2018, 2019, 2021, 2024, 2027, 2029, 2032, 2033, 2035, 2037, 2043} and ref cls is {1473, 2230}\n",
      "hyps cls is {1056, 1091, 1830, 1807, 1823, 924, 1055} and ref cls is {1762, 484, 1703, 2087, 1417, 1450, 47, 1777, 1841, 1823, 1019, 1436, 799}\n",
      "hyps cls is set() and ref cls is {1164, 1676, 2189, 1807, 1935, 788, 2201, 924, 2076, 2078, 554, 1965, 562, 1332, 1593, 570, 2366, 577, 329, 588, 589, 716, 1742, 1232, 593, 2128, 211, 2021, 876, 1772, 1392, 1779, 2037, 762, 1918}\n",
      "hyps cls is {2051, 6, 1033, 1034, 2061, 1557, 1064, 1580, 2092, 1070, 1583, 2094, 2100, 570, 1595, 574, 71, 81, 610, 2151, 618, 623, 625, 115, 1141, 2167, 1144, 1661, 125, 2179, 1669, 1674, 1676, 2188, 2199, 2207, 1188, 1709, 1715, 1717, 698, 1215, 711, 716, 2265, 738, 748, 243, 758, 2298, 774, 1803, 2320, 285, 2335, 2342, 296, 812, 304, 1332, 2363, 831, 1344, 1864, 1867, 1868, 333, 845, 1358, 1872, 865, 1900, 375, 1918, 392, 397, 1933, 912, 1430, 1943, 1944, 411, 424, 443, 445, 973, 2010, 476, 480, 1005, 1006, 498, 1019, 1532, 509} and ref cls is {1025, 2051, 4, 517, 1549, 1554, 24, 1055, 1567, 547, 548, 1072, 1073, 1076, 1591, 1082, 62, 1091, 1607, 598, 2138, 1119, 1121, 620, 623, 625, 2167, 1144, 1657, 1661, 641, 2179, 2188, 655, 662, 1696, 1187, 1188, 2216, 169, 2217, 174, 689, 1723, 2235, 1726, 202, 717, 1233, 726, 732, 1248, 2277, 2298, 1796, 1803, 781, 782, 274, 1299, 285, 1316, 1320, 1843, 2363, 831, 1863, 1864, 1865, 1867, 1868, 1872, 878, 884, 397, 1933, 407, 1943, 1944, 411, 418, 422, 1960, 942, 1971, 443, 1476, 455, 460, 1490, 1492, 981, 2010, 1516, 2029, 1006, 2032, 1525, 2043, 509}\n",
      "hyps cls is {1293, 205, 1394} and ref cls is {1, 515, 5, 6, 1544, 1033, 1034, 1547, 2061, 2063, 528, 1553, 1557, 2070, 535, 2072, 1563, 1052, 541, 1568, 33, 1569, 1059, 1570, 2081, 1064, 1580, 2092, 1070, 1583, 48, 2093, 2094, 2100, 59, 1595, 65, 69, 1094, 71, 1097, 2124, 1618, 1108, 1115, 92, 1117, 96, 609, 610, 1633, 100, 612, 1639, 1643, 1645, 2159, 1649, 115, 1141, 634, 1147, 124, 125, 127, 1153, 1668, 645, 1669, 1674, 1168, 1685, 2199, 2207, 1192, 1705, 1194, 2224, 1203, 1715, 1717, 184, 1208, 698, 2238, 1728, 1730, 1220, 710, 711, 712, 1222, 1734, 1228, 205, 718, 1234, 1236, 213, 1239, 1752, 2265, 219, 1244, 1245, 2267, 2269, 736, 737, 738, 2275, 232, 2280, 746, 2281, 748, 1262, 243, 1268, 1269, 246, 758, 1270, 1786, 1277, 1280, 257, 1792, 2306, 1284, 2307, 774, 1289, 2313, 1293, 785, 1297, 1298, 1300, 2321, 1304, 1305, 1306, 1817, 1821, 1822, 291, 292, 1829, 294, 295, 296, 2342, 811, 812, 1835, 814, 1838, 304, 819, 1331, 309, 317, 1344, 1857, 835, 1353, 333, 845, 1358, 848, 1360, 339, 1365, 860, 349, 1887, 1888, 865, 356, 358, 1895, 1900, 1394, 1396, 885, 1397, 375, 889, 1402, 1409, 1924, 902, 392, 909, 398, 910, 912, 1939, 1430, 919, 1431, 932, 1446, 1447, 424, 1452, 947, 1461, 1973, 1974, 952, 1464, 1465, 1467, 445, 446, 958, 1982, 962, 1990, 1992, 1994, 973, 1999, 465, 2004, 986, 475, 476, 2011, 479, 480, 992, 483, 2019, 999, 2024, 1001, 1514, 1004, 1005, 1521, 498, 1011, 1523, 2033, 2035, 1532, 1022, 1023}\n",
      "[   2    8   12   14   16   17   20   25   31   34   44   45   49   56\n",
      "   63   72   74   77   94  101  109  114  117  123  126  130  143  153\n",
      "  159  164  166  173  175  182  189  190  197  198  200  206  214  217\n",
      "  223  230  238  242  247  250  255  258  266  272  278  280  287  298\n",
      "  299  300  311  323  331  337  338  345  346  350  352  359  363  365\n",
      "  376  382  388  389  391  395  399  404  412  421  427  428  431  432\n",
      "  435  437  449  461  473  481  485  486  490  492  496  502  503  516\n",
      "  518  521  523  539  553  555  559  565  568  581  595  596  599  606\n",
      "  607  608  611  613  614  616  619  621  622  624  629  633  636  649\n",
      "  650  651  657  660  661  664  666  670  673  679  681  684  688  694\n",
      "  695  699  702  715  721  724  725  744  745  757  760  761  766  770\n",
      "  772  777  795  805  817  822  824  832  837  840  846  856  861  862\n",
      "  863  871  874  879  886  890  892  894  896  897  900  903  905  906\n",
      "  913  915  920  931  941  948  955  960  961  963  971  976  977  983\n",
      "  991  993  994  998 1007 1008 1014 1015 1016 1031 1036 1058 1061 1065\n",
      " 1067 1069 1071 1079 1080 1081 1084 1085 1089 1102 1104 1109 1118 1125\n",
      " 1128 1134 1137 1146 1149 1150 1156 1158 1159 1169 1172 1173 1179 1181\n",
      " 1184 1185 1186 1190 1191 1198 1199 1201 1202 1210 1219 1226 1230 1242\n",
      " 1246 1251 1256 1260 1266 1275 1281 1283 1286 1290 1309 1310 1313 1319\n",
      " 1324 1325 1334 1345 1357 1362 1364 1372 1373 1378 1381 1384 1391 1393\n",
      " 1400 1403 1414 1419 1427 1429 1432 1441 1444 1449 1453 1466 1469 1474\n",
      " 1481 1483 1484 1493 1494 1500 1506 1508 1509 1511 1513 1515 1524 1527\n",
      " 1534 1537 1543 1556 1558 1561 1572 1579 1596 1599 1600 1608 1614 1623\n",
      " 1628 1652 1658 1666 1678 1684 1704 1706 1707 1708 1719 1722 1727 1733\n",
      " 1744 1746 1747 1748 1754 1757 1758 1766 1767 1778 1782 1783 1789 1791\n",
      " 1797 1802 1805 1809 1811 1813 1818 1820 1826 1839 1855 1856 1858 1861\n",
      " 1869 1874 1875 1876 1880 1883 1890 1891 1893 1894 1899 1902 1903 1909\n",
      " 1911 1914 1915 1919 1922 1938 1949 1977 1978 1983 1984 1987 1991 1995\n",
      " 2000 2023 2025 2026 2028 2031 2040 2048 2049 2050 2055 2056 2066 2068\n",
      " 2083 2086 2089 2101 2102 2104 2109 2110 2111 2115 2120 2127 2129 2133\n",
      " 2134 2135 2139 2142 2144 2145 2150 2160 2166 2172 2181 2191 2208 2210\n",
      " 2212 2215 2220 2221 2228 2234 2237 2239 2240 2242 2244 2245 2250 2256\n",
      " 2260 2264 2271 2278 2285 2286 2294 2295 2296 2308 2309 2311 2314 2317\n",
      " 2322 2323 2327 2328 2329 2331 2337 2338 2339 2348 2350 2355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps cls is {486} and ref cls is {130, 772, 1286, 1290, 1783, 1036, 16, 1427, 1172, 1684, 2068, 2040, 153, 25, 2331, 1949, 287, 673, 1441, 1826, 2210, 2337, 553, 1065, 299, 300, 45, 684, 559, 1324, 1453, 1202, 1579, 2348, 437, 565, 2104, 1977, 1978, 2234, 1084, 1085, 1469, 449, 1474, 1219, 837, 840, 337, 338, 1748, 983, 1880, 1372, 1628, 2271, 352, 2145, 1251, 614, 1511, 2150, 619, 1134, 1902, 1782, 1527, 1400, 2294, 250, 890, 1791}\n",
      "hyps cls is {2, 8, 2056, 12, 14, 16, 17, 2066, 2068, 25, 31, 34, 2083, 2086, 2089, 44, 45, 49, 2101, 2102, 56, 2104, 2109, 63, 2111, 2115, 74, 77, 2127, 2135, 2142, 2144, 101, 2150, 109, 2166, 123, 126, 130, 2181, 143, 2191, 159, 2210, 164, 2212, 166, 2215, 2220, 173, 2221, 175, 2228, 182, 2234, 189, 190, 2237, 2239, 2240, 2242, 2244, 197, 198, 2245, 200, 2250, 2256, 2260, 214, 2264, 217, 223, 2271, 2278, 2285, 238, 242, 2294, 247, 2295, 2296, 250, 255, 258, 2308, 2309, 2311, 266, 2314, 2317, 272, 2322, 278, 2327, 280, 287, 2337, 2339, 298, 299, 300, 2348, 2355, 311, 323, 331, 337, 338, 345, 346, 350, 352, 359, 363, 365, 376, 388, 389, 391, 395, 399, 404, 428, 431, 432, 435, 437, 461, 473, 481, 490, 492, 496, 502, 503, 516, 518, 523, 539, 553, 555, 559, 565, 568, 595, 596, 599, 606, 607, 613, 614, 616, 619, 621, 624, 633, 636, 649, 650, 651, 657, 664, 666, 670, 673, 679, 684, 694, 695, 699, 702, 715, 721, 744, 745, 760, 766, 770, 772, 777, 805, 822, 824, 832, 837, 840, 874, 886, 890, 892, 894, 896, 897, 900, 903, 905, 906, 915, 920, 931, 941, 948, 955, 960, 961, 963, 971, 976, 977, 983, 993, 994, 1008, 1014, 1016, 1031, 1036, 1058, 1061, 1069, 1081, 1084, 1085, 1089, 1102, 1104, 1109, 1118, 1125, 1134, 1137, 1146, 1149, 1150, 1156, 1158, 1159, 1169, 1172, 1173, 1179, 1181, 1185, 1186, 1198, 1199, 1201, 1202, 1210, 1219, 1226, 1230, 1246, 1251, 1256, 1266, 1281, 1283, 1286, 1310, 1313, 1319, 1324, 1325, 1345, 1362, 1373, 1378, 1381, 1384, 1391, 1393, 1403, 1414, 1419, 1427, 1429, 1432, 1441, 1444, 1449, 1453, 1466, 1469, 1474, 1483, 1484, 1500, 1506, 1508, 1509, 1511, 1513, 1515, 1527, 1537, 1556, 1558, 1561, 1579, 1596, 1599, 1608, 1614, 1628, 1652, 1658, 1666, 1678, 1684, 1704, 1706, 1707, 1708, 1719, 1722, 1727, 1733, 1744, 1746, 1747, 1754, 1757, 1758, 1766, 1767, 1778, 1782, 1791, 1797, 1802, 1809, 1811, 1818, 1820, 1826, 1839, 1855, 1858, 1861, 1880, 1883, 1890, 1893, 1894, 1899, 1902, 1903, 1909, 1915, 1919, 1938, 1949, 1977, 1978, 1984, 1987, 1991, 1995, 2000, 2023, 2025, 2026, 2028, 2031, 2040} and ref cls is {1744, 670}\n",
      "hyps cls is {998, 725, 1334, 2329, 2331, 1309} and ref cls is {1089, 2242, 1158, 2311, 1513, 266, 395, 1515, 2191, 948, 404, 1915, 189, 1246}\n",
      "hyps cls is set() and ref cls is {1537, 388, 518, 1414, 1802, 2317, 657, 2066, 2329, 666, 1179, 539, 1310, 2089, 555, 1708, 1325, 1722, 1984, 1345, 1481, 971, 1230, 976, 595, 724, 2260, 998, 359, 1256, 490, 2028, 1393, 1658, 1275}\n",
      "hyps cls is {2048, 2049, 2050, 1543, 2055, 521, 20, 1572, 1065, 1067, 1071, 1079, 1080, 2110, 1600, 581, 72, 2120, 2129, 2133, 2134, 1623, 2139, 94, 608, 2145, 611, 1128, 622, 2160, 114, 117, 629, 2172, 660, 661, 153, 2208, 1190, 1191, 681, 688, 206, 724, 1748, 1242, 230, 1260, 2286, 757, 1783, 761, 1275, 1789, 1290, 1805, 2323, 1813, 2328, 795, 2338, 2350, 817, 1856, 1357, 846, 1869, 1874, 1875, 1364, 1876, 856, 1372, 861, 862, 863, 1891, 871, 879, 1911, 1400, 1914, 382, 1922, 913, 412, 421, 427, 1983, 449, 1481, 1493, 1494, 991, 485, 1007, 1524, 1015, 1534} and ref cls is {2048, 2049, 2050, 1031, 2055, 2056, 1556, 1558, 2101, 2102, 568, 2109, 2110, 1600, 2115, 581, 72, 74, 2134, 2135, 606, 607, 1128, 109, 621, 622, 624, 1137, 114, 117, 629, 2166, 651, 1169, 660, 661, 1173, 664, 1181, 1185, 679, 175, 688, 1719, 699, 702, 2250, 715, 2256, 1746, 1747, 1757, 1758, 1766, 744, 760, 1789, 255, 770, 777, 2314, 1313, 2339, 805, 298, 2350, 1861, 331, 1869, 846, 1362, 1874, 1364, 1875, 1876, 1378, 1891, 1384, 365, 886, 382, 1919, 1922, 1429, 412, 931, 421, 1449, 435, 1987, 461, 2000, 1493, 1494, 1500, 993, 485, 486, 2026, 1008}\n",
      "hyps cls is {1184} and ref cls is {2, 516, 1543, 8, 521, 523, 12, 14, 17, 20, 1561, 31, 34, 1058, 1572, 1061, 2083, 2086, 1067, 44, 1069, 1071, 49, 1079, 56, 1080, 1081, 1596, 63, 1599, 2111, 1608, 2120, 77, 1102, 1614, 1104, 2127, 2129, 596, 1109, 2133, 599, 1623, 2139, 94, 1118, 608, 2142, 2144, 611, 101, 613, 1125, 616, 2160, 1652, 633, 1146, 123, 636, 1149, 126, 1150, 2172, 1666, 1156, 2181, 1159, 649, 650, 1678, 143, 159, 1184, 2208, 1186, 164, 2212, 166, 1190, 1191, 681, 1704, 1706, 1707, 173, 1198, 1199, 2215, 1201, 2220, 2221, 2228, 182, 694, 695, 1210, 2237, 190, 1727, 2239, 2240, 2244, 197, 198, 1733, 200, 2245, 1226, 206, 721, 725, 214, 2264, 217, 1242, 1754, 223, 230, 1767, 2278, 745, 1260, 2285, 238, 2286, 242, 1266, 1778, 757, 247, 2295, 761, 2296, 766, 1281, 258, 1283, 2308, 1797, 2309, 1805, 272, 1809, 2322, 1811, 2323, 1813, 278, 2327, 280, 2328, 1818, 795, 1820, 1309, 2338, 1319, 1839, 817, 2355, 822, 311, 824, 1334, 1855, 832, 1856, 1858, 323, 1357, 856, 345, 346, 1883, 861, 350, 862, 863, 1373, 1890, 1381, 1893, 871, 1894, 874, 363, 1899, 879, 1391, 1903, 1909, 1911, 376, 1914, 1403, 892, 894, 896, 897, 900, 389, 391, 903, 905, 906, 1419, 399, 913, 1938, 915, 920, 1432, 1444, 427, 428, 941, 431, 432, 1466, 955, 1983, 960, 961, 963, 1991, 1483, 1484, 1995, 977, 473, 1016, 991, 481, 994, 1506, 1508, 1509, 2023, 2025, 492, 1007, 496, 2031, 1015, 1524, 502, 503, 1014, 1534}\n",
      "[   7   15   18   22   28   32   35   39   41   42   50   55   57   60\n",
      "   64   66   83   84   88   89   95   98  102  103  105  108  112  116\n",
      "  118  120  133  136  137  140  142  145  148  149  161  165  170  171\n",
      "  172  178  181  183  185  186  188  194  201  204  209  210  220  231\n",
      "  245  252  253  254  264  273  283  301  303  305  312  314  321  322\n",
      "  330  335  344  347  348  353  367  368  380  385  387  403  409  414\n",
      "  419  429  447  448  451  454  469  477  487  504  507  510  511  512\n",
      "  513  519  524  526  529  531  550  552  556  557  561  567  575  576\n",
      "  582  583  587  590  592  597  601  604  605  626  632  644  648  652\n",
      "  653  656  663  669  672  675  683  687  693  697  700  704  705  706\n",
      "  720  722  723  728  729  730  731  733  735  750  756  765  769  771\n",
      "  776  780  802  803  807  809  813  815  821  827  828  830  836  838\n",
      "  850  854  855  857  858  859  864  866  867  881  882  883  891  911\n",
      "  916  918  927  929  933  934  935  936  945  957  964  967  969  972\n",
      "  979  980  985  988  989  990  996 1012 1020 1021 1024 1027 1028 1030\n",
      " 1037 1039 1040 1042 1043 1048 1050 1051 1054 1060 1066 1083 1090 1095\n",
      " 1096 1098 1100 1101 1111 1114 1122 1129 1131 1145 1151 1154 1166 1170\n",
      " 1171 1174 1175 1178 1193 1196 1200 1213 1214 1218 1223 1227 1237 1250\n",
      " 1253 1254 1255 1257 1259 1261 1271 1279 1295 1296 1301 1302 1303 1314\n",
      " 1315 1318 1321 1322 1341 1350 1356 1374 1385 1387 1398 1399 1405 1407\n",
      " 1412 1413 1415 1420 1426 1434 1435 1437 1439 1445 1448 1456 1459 1460\n",
      " 1470 1475 1477 1478 1482 1488 1491 1501 1503 1505 1520 1526 1528 1531\n",
      " 1538 1541 1545 1552 1571 1582 1586 1587 1588 1589 1590 1604 1609 1615\n",
      " 1625 1627 1632 1634 1640 1648 1650 1654 1655 1656 1659 1660 1663 1664\n",
      " 1673 1683 1686 1687 1720 1724 1729 1731 1745 1765 1768 1770 1771 1775\n",
      " 1780 1784 1790 1793 1801 1804 1808 1814 1815 1833 1837 1842 1846 1848\n",
      " 1850 1854 1859 1860 1862 1884 1886 1897 1898 1905 1908 1912 1913 1916\n",
      " 1917 1923 1927 1929 1931 1951 1955 1958 1962 1963 1966 1967 1969 1975\n",
      " 1976 1985 1989 1993 1997 1998 2006 2008 2014 2022 2036 2038 2039 2041\n",
      " 2054 2057 2060 2065 2069 2073 2074 2079 2082 2090 2098 2103 2116 2117\n",
      " 2125 2126 2131 2136 2137 2140 2143 2147 2149 2153 2157 2161 2164 2169\n",
      " 2173 2175 2178 2180 2192 2193 2196 2197 2200 2202 2205 2214 2223 2233\n",
      " 2251 2254 2258 2262 2268 2272 2276 2283 2287 2290 2291 2299 2310 2312\n",
      " 2315 2316 2318 2325 2326 2330 2332 2340 2345 2354 2357 2361]\n",
      "hyps cls is {1321, 1322} and ref cls is {512, 385, 513, 1793, 1413, 1927, 264, 2057, 2315, 524, 1039, 1296, 145, 18, 1042, 1426, 1687, 1048, 2073, 1050, 1439, 672, 1660, 1951, 35, 1315, 1916, 41, 42, 556, 301, 557, 2223, 1460, 693, 821, 55, 185, 1850, 1083, 700, 321, 706, 451, 836, 1218, 582, 838, 1475, 1859, 1098, 1997, 2258, 597, 2262, 88, 2136, 1627, 988, 2143, 2272, 1250, 2147, 1131, 2157, 2290, 2291, 2039, 1528, 2299, 1020, 511}\n",
      "hyps cls is {2054, 7, 15, 18, 2069, 22, 2073, 2074, 28, 2079, 2082, 35, 39, 41, 42, 2090, 2098, 2103, 57, 60, 64, 66, 2116, 2117, 2126, 2131, 88, 2137, 2140, 95, 2143, 98, 2147, 2149, 102, 105, 2153, 108, 2157, 112, 116, 2164, 2169, 2173, 2175, 2180, 137, 142, 2192, 145, 148, 149, 2196, 2197, 2200, 2202, 161, 165, 2214, 170, 172, 2223, 178, 183, 185, 186, 2233, 188, 194, 201, 2251, 204, 2254, 209, 210, 2258, 2262, 220, 2272, 2276, 231, 2283, 2287, 2290, 2291, 245, 2299, 252, 254, 2310, 264, 2312, 2315, 2316, 273, 2325, 2330, 283, 2332, 2340, 2345, 301, 303, 312, 2361, 321, 322, 330, 335, 344, 347, 353, 367, 368, 385, 387, 403, 409, 414, 429, 447, 448, 451, 454, 469, 477, 487, 504, 507, 510, 511, 512, 513, 524, 529, 531, 550, 556, 557, 567, 576, 582, 587, 592, 597, 601, 604, 632, 644, 648, 652, 653, 663, 669, 672, 675, 687, 693, 697, 700, 704, 705, 706, 720, 722, 729, 731, 733, 735, 750, 756, 769, 771, 802, 803, 807, 813, 821, 827, 828, 830, 836, 838, 850, 854, 857, 858, 881, 882, 883, 891, 911, 916, 918, 927, 929, 933, 945, 964, 967, 972, 979, 980, 985, 988, 990, 1012, 1020, 1021, 1024, 1027, 1028, 1030, 1037, 1039, 1040, 1042, 1043, 1048, 1050, 1051, 1054, 1060, 1083, 1098, 1101, 1111, 1114, 1129, 1131, 1145, 1151, 1154, 1166, 1170, 1171, 1174, 1175, 1178, 1196, 1200, 1214, 1218, 1223, 1227, 1250, 1253, 1254, 1255, 1257, 1259, 1261, 1271, 1279, 1295, 1296, 1301, 1302, 1303, 1314, 1318, 1356, 1374, 1387, 1398, 1399, 1405, 1407, 1413, 1415, 1434, 1435, 1437, 1445, 1448, 1456, 1459, 1460, 1475, 1477, 1478, 1482, 1488, 1491, 1501, 1520, 1526, 1528, 1531, 1541, 1545, 1552, 1571, 1582, 1586, 1587, 1588, 1589, 1590, 1615, 1627, 1640, 1648, 1650, 1654, 1655, 1656, 1659, 1660, 1663, 1664, 1683, 1686, 1687, 1745, 1765, 1770, 1780, 1790, 1793, 1801, 1804, 1808, 1814, 1833, 1837, 1842, 1846, 1848, 1854, 1862, 1884, 1886, 1897, 1898, 1908, 1913, 1916, 1917, 1923, 1927, 1929, 1931, 1951, 1955, 1962, 1963, 1966, 1967, 1969, 1976, 1985, 1989, 1993, 1997, 1998, 2006, 2014, 2022, 2036, 2038, 2039, 2041} and ref cls is {220, 1790, 567}\n",
      "hyps cls is {1090, 1634, 1604, 1859, 140, 1775, 561, 120, 1850, 1439} and ref cls is {576, 2169, 1955, 1448, 2254, 2192, 178, 723, 2098, 149, 22, 57, 1663}\n",
      "hyps cls is set() and ref cls is {769, 7, 1545, 1801, 2316, 656, 1808, 403, 1814, 1815, 802, 39, 1967, 1969, 1214, 1729, 2117, 583, 967, 587, 972, 590, 1998, 592, 735, 1253, 2022, 1259, 368, 756, 245, 1654, 1655, 1780, 2041, 1917}\n",
      "hyps cls is {1538, 519, 2057, 2060, 526, 2065, 552, 1066, 50, 55, 575, 583, 1095, 1096, 1609, 1100, 2125, 590, 83, 84, 2136, 89, 1625, 605, 1632, 1122, 103, 2161, 626, 118, 2178, 133, 136, 1673, 656, 2193, 2205, 1193, 171, 683, 181, 1720, 1724, 1213, 1729, 1731, 723, 1237, 728, 730, 2268, 1768, 1771, 1784, 765, 253, 776, 780, 2318, 1815, 1315, 809, 815, 305, 2354, 2357, 314, 1341, 1860, 1350, 855, 859, 348, 864, 866, 867, 1385, 1905, 1912, 380, 1412, 1420, 1426, 419, 934, 935, 936, 1958, 1975, 957, 1470, 969, 2008, 989, 1503, 1505, 996} and ref cls is {1538, 1027, 1541, 1030, 2054, 526, 2065, 2074, 2082, 550, 552, 561, 50, 1589, 1590, 2103, 1090, 2116, 1609, 2125, 601, 2137, 1634, 2161, 626, 116, 118, 1656, 120, 632, 1659, 1145, 2173, 133, 652, 653, 1170, 663, 1178, 171, 683, 687, 1724, 704, 705, 2251, 209, 210, 722, 1745, 730, 731, 2268, 1765, 1254, 1768, 1257, 1771, 1775, 780, 1804, 2330, 2340, 1321, 1322, 813, 303, 1842, 1846, 1848, 314, 1854, 1860, 1862, 330, 850, 367, 882, 380, 1931, 409, 414, 929, 419, 1456, 1976, 964, 969, 1491, 2008, 1501, 2014, 1505, 487, 2036, 2038, 504, 1531, 1021, 510}\n",
      "hyps cls is {32, 2326} and ref cls is {1024, 1028, 519, 2060, 1037, 15, 1040, 529, 1552, 531, 1043, 2069, 1051, 28, 1054, 2079, 32, 1571, 1060, 1066, 2090, 1582, 1586, 1587, 1588, 60, 575, 64, 66, 1604, 1095, 1096, 1100, 1101, 2126, 1615, 83, 84, 2131, 1111, 89, 1114, 1625, 604, 605, 2140, 95, 1632, 98, 1122, 2149, 102, 103, 1640, 105, 1129, 2153, 108, 112, 1648, 1650, 2164, 1151, 1664, 2175, 1154, 2178, 644, 2180, 136, 137, 648, 1673, 140, 142, 1166, 2193, 1171, 148, 1683, 1174, 1175, 1686, 2196, 2197, 2200, 2202, 669, 2205, 161, 675, 165, 2214, 1193, 170, 172, 1196, 1200, 181, 183, 1720, 697, 186, 2233, 188, 1213, 194, 1731, 1223, 201, 1227, 204, 720, 1237, 728, 729, 733, 2276, 231, 1255, 1770, 2283, 1261, 750, 2287, 1271, 1784, 252, 765, 253, 254, 1279, 771, 2310, 776, 2312, 2318, 1295, 273, 1301, 1302, 1303, 2325, 2326, 283, 2332, 1314, 803, 1318, 807, 809, 1833, 2345, 1837, 815, 305, 2354, 2357, 312, 2361, 827, 828, 1341, 830, 322, 1350, 1356, 335, 854, 855, 344, 857, 858, 347, 348, 859, 1374, 1884, 864, 353, 866, 867, 1886, 1385, 1897, 1387, 1898, 881, 1905, 883, 1908, 1398, 1399, 1912, 1913, 891, 1405, 1407, 387, 1412, 1923, 1415, 1929, 1420, 911, 916, 918, 1434, 1435, 1437, 927, 933, 934, 935, 936, 1445, 1958, 1962, 1963, 429, 1966, 945, 1459, 1975, 957, 1470, 447, 448, 1985, 1477, 454, 1478, 1989, 1993, 1482, 1488, 979, 980, 469, 2006, 985, 477, 989, 990, 1503, 996, 1520, 1012, 1526, 507}\n",
      "[   0   10   19   21   26   36   38   43   53   54   58   70   76   86\n",
      "   90   93   97   99  111  121  128  129  134  138  141  147  151  152\n",
      "  154  155  156  158  162  163  179  180  187  191  195  199  212  215\n",
      "  218  221  222  225  227  228  229  236  237  239  240  248  249  251\n",
      "  256  260  262  263  265  269  270  276  279  281  282  288  290  297\n",
      "  302  306  307  318  320  324  326  327  328  332  336  340  341  343\n",
      "  355  357  360  362  364  366  369  372  373  378  379  381  386  394\n",
      "  400  401  402  408  413  416  417  420  423  426  433  439  440  442\n",
      "  444  452  453  456  457  458  462  466  468  470  471  474  478  482\n",
      "  489  491  494  495  497  499  508  514  520  527  530  533  534  536\n",
      "  537  542  545  546  551  563  569  572  573  586  603  617  627  628\n",
      "  631  635  637  642  643  654  665  671  676  677  678  685  707  709\n",
      "  713  714  739  740  741  742  751  752  767  768  779  783  784  786\n",
      "  789  791  793  794  797  810  816  818  820  826  829  833  844  847\n",
      "  849  851  853  868  875  880  888  895  904  917  921  925  930  937\n",
      "  938  939  950  953  974  978  984  987  997 1009 1018 1026 1029 1032\n",
      " 1035 1038 1049 1062 1068 1075 1088 1093 1099 1103 1112 1123 1133 1135\n",
      " 1136 1138 1139 1142 1155 1162 1176 1177 1180 1182 1189 1197 1207 1209\n",
      " 1211 1212 1221 1224 1225 1229 1238 1241 1249 1264 1278 1282 1291 1292\n",
      " 1294 1307 1308 1312 1323 1326 1329 1330 1333 1336 1338 1342 1348 1351\n",
      " 1355 1359 1367 1368 1369 1371 1376 1382 1386 1388 1389 1395 1401 1404\n",
      " 1408 1416 1421 1423 1424 1433 1438 1440 1442 1454 1455 1457 1458 1463\n",
      " 1471 1479 1480 1486 1495 1496 1499 1512 1517 1518 1530 1533 1539 1542\n",
      " 1546 1550 1551 1559 1562 1573 1574 1575 1576 1577 1578 1581 1584 1594\n",
      " 1602 1613 1616 1621 1622 1630 1635 1636 1637 1641 1647 1662 1665 1667\n",
      " 1670 1675 1680 1682 1689 1692 1699 1701 1702 1713 1716 1721 1732 1735\n",
      " 1738 1739 1751 1755 1759 1760 1763 1764 1773 1788 1798 1810 1828 1831\n",
      " 1832 1836 1845 1847 1870 1877 1881 1882 1889 1896 1910 1934 1937 1948\n",
      " 1953 1956 1957 1959 1964 1968 1970 1979 1988 2001 2003 2005 2007 2009\n",
      " 2012 2013 2016 2020 2034 2044 2045 2046 2047 2052 2062 2077 2080 2084\n",
      " 2088 2096 2105 2106 2107 2108 2112 2119 2121 2141 2148 2155 2158 2162\n",
      " 2163 2168 2170 2176 2182 2185 2190 2195 2204 2209 2213 2218 2227 2229\n",
      " 2231 2232 2246 2247 2249 2253 2259 2273 2282 2288 2292 2297 2300 2301\n",
      " 2304 2305 2319 2324 2333 2336 2351 2356 2360 2362 2364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps cls is {930, 326, 2218, 1133, 880, 86} and ref cls is {128, 514, 1282, 1539, 1670, 904, 2185, 10, 1294, 1423, 1551, 1934, 151, 1049, 1689, 1692, 797, 158, 1948, 1312, 2209, 930, 1442, 2336, 1574, 426, 1964, 179, 180, 563, 439, 440, 569, 58, 1463, 572, 573, 1212, 2105, 320, 833, 2301, 1351, 328, 456, 457, 458, 1224, 1355, 1486, 1359, 1613, 212, 853, 221, 478, 1630, 2016, 2360, 229, 489, 1133, 366, 495, 880, 1264, 1517, 2292, 1142, 249, 378, 2045}\n",
      "hyps cls is {0, 2052, 10, 19, 21, 26, 2077, 2080, 36, 2084, 38, 2096, 53, 54, 58, 2106, 2107, 2108, 2112, 70, 2119, 2121, 76, 90, 93, 2141, 97, 99, 2148, 2155, 111, 2162, 2163, 2168, 2170, 2176, 129, 134, 2182, 138, 141, 2190, 151, 152, 154, 155, 156, 2204, 158, 2209, 162, 163, 179, 180, 2227, 2229, 2231, 187, 195, 2246, 199, 2247, 2253, 2259, 212, 215, 218, 225, 2273, 227, 229, 237, 239, 240, 2288, 2292, 248, 249, 2297, 251, 2300, 2301, 2304, 260, 262, 265, 269, 270, 276, 2324, 281, 282, 2333, 288, 2336, 290, 297, 302, 2351, 306, 307, 2356, 2360, 2362, 2364, 318, 320, 327, 328, 336, 340, 343, 357, 360, 364, 366, 369, 372, 373, 378, 379, 394, 400, 401, 402, 417, 426, 433, 442, 444, 452, 453, 456, 462, 466, 470, 478, 482, 494, 497, 499, 508, 520, 527, 533, 534, 536, 537, 542, 545, 546, 551, 563, 569, 572, 573, 603, 617, 627, 628, 637, 642, 643, 665, 671, 676, 677, 678, 685, 707, 709, 713, 714, 739, 741, 742, 751, 767, 768, 784, 789, 791, 793, 794, 797, 816, 818, 820, 826, 829, 833, 844, 847, 849, 851, 853, 868, 875, 888, 895, 917, 921, 925, 937, 938, 939, 950, 953, 974, 978, 984, 987, 1009, 1018, 1026, 1029, 1035, 1038, 1049, 1068, 1075, 1088, 1093, 1099, 1103, 1112, 1123, 1135, 1138, 1139, 1142, 1176, 1177, 1180, 1182, 1189, 1197, 1207, 1209, 1211, 1212, 1221, 1224, 1225, 1229, 1238, 1249, 1264, 1291, 1292, 1307, 1308, 1312, 1323, 1326, 1329, 1330, 1333, 1336, 1338, 1348, 1351, 1355, 1359, 1367, 1368, 1369, 1376, 1382, 1388, 1389, 1395, 1401, 1408, 1416, 1421, 1423, 1424, 1433, 1438, 1442, 1454, 1455, 1471, 1480, 1499, 1517, 1518, 1530, 1539, 1542, 1546, 1550, 1559, 1562, 1575, 1577, 1578, 1581, 1602, 1613, 1616, 1621, 1622, 1630, 1635, 1636, 1637, 1641, 1647, 1662, 1665, 1667, 1670, 1680, 1682, 1689, 1692, 1699, 1701, 1716, 1721, 1735, 1739, 1751, 1755, 1759, 1760, 1764, 1773, 1788, 1798, 1828, 1831, 1832, 1836, 1845, 1847, 1870, 1882, 1889, 1910, 1934, 1937, 1948, 1953, 1956, 1959, 1964, 1970, 1979, 1988, 2001, 2003, 2005, 2007, 2016, 2020, 2034, 2044, 2045, 2046, 2047} and ref cls is {156, 1182}\n",
      "hyps cls is {740, 1702, 2088, 586, 1675, 783, 1551, 786, 1463, 1594, 222} and ref cls is {768, 2273, 678, 551, 1735, 394, 401, 2007, 791, 2170, 2364, 1662, 671}\n",
      "hyps cls is set() and ref cls is {262, 265, 1675, 654, 402, 152, 1177, 154, 794, 1180, 2077, 676, 38, 1062, 1959, 1197, 1455, 818, 1970, 820, 2229, 54, 2231, 2106, 2107, 318, 326, 327, 586, 218, 1760, 225, 875, 1389, 1773, 2046}\n",
      "hyps cls is {514, 1032, 2062, 530, 1573, 1062, 1574, 1576, 43, 1584, 2105, 2158, 1136, 631, 121, 635, 128, 1155, 2185, 1162, 654, 147, 2195, 2213, 1713, 2232, 191, 1732, 2249, 1738, 1241, 221, 1763, 228, 2282, 236, 752, 1278, 256, 2305, 1282, 263, 779, 1294, 2319, 1810, 279, 810, 1342, 324, 332, 341, 1877, 1881, 1371, 355, 1896, 362, 1386, 1404, 381, 386, 904, 408, 413, 416, 1440, 420, 423, 1968, 1457, 1458, 439, 440, 1479, 457, 458, 1486, 468, 471, 1495, 1496, 474, 2009, 2012, 2013, 997, 1512, 489, 491, 495, 1533} and ref cls is {2052, 1542, 520, 1032, 1546, 1550, 534, 1559, 537, 26, 1576, 2088, 1584, 2096, 1075, 1594, 1093, 70, 2121, 1616, 86, 1123, 1636, 627, 628, 1139, 631, 121, 1155, 134, 1682, 1176, 1699, 677, 1189, 1702, 2218, 685, 2232, 2246, 713, 714, 2249, 739, 228, 740, 741, 2288, 2297, 251, 2300, 256, 260, 779, 783, 793, 282, 1828, 1323, 302, 307, 1847, 1336, 829, 324, 844, 1870, 851, 1877, 343, 364, 372, 888, 381, 408, 413, 925, 416, 417, 1440, 1953, 420, 423, 1454, 953, 1979, 444, 2001, 1495, 1496, 2009, 474, 987, 1499, 2012, 491, 1009, 1018, 2044, 2047}\n",
      "hyps cls is {1957} and ref cls is {0, 1026, 1029, 1035, 1038, 527, 2062, 530, 19, 21, 533, 536, 1562, 542, 2080, 545, 546, 36, 1573, 2084, 1575, 1577, 1578, 43, 1068, 1581, 53, 2108, 1088, 2112, 1602, 2119, 1099, 76, 1103, 1621, 1622, 1112, 90, 603, 93, 2141, 97, 99, 1635, 1637, 2148, 617, 1641, 2155, 2158, 111, 1135, 1136, 1138, 1647, 2162, 2163, 2168, 635, 637, 2176, 129, 642, 643, 1665, 1667, 2182, 138, 1162, 141, 2190, 1680, 147, 2195, 665, 155, 2204, 162, 163, 1701, 2213, 1713, 2227, 1716, 1207, 1209, 1721, 187, 1211, 191, 195, 707, 709, 1221, 199, 1732, 1225, 1738, 1739, 2247, 1229, 2253, 2259, 1238, 215, 1751, 1241, 1755, 222, 1759, 1249, 227, 1763, 1764, 742, 2282, 236, 237, 239, 240, 751, 752, 248, 1788, 1278, 767, 2304, 2305, 1798, 263, 1291, 1292, 269, 270, 2319, 784, 786, 1810, 276, 789, 2324, 279, 281, 1307, 1308, 2333, 288, 290, 1831, 1832, 297, 810, 1836, 1326, 2351, 816, 1329, 306, 1330, 2356, 1333, 1845, 826, 1338, 2362, 1342, 1348, 332, 847, 336, 849, 340, 341, 1367, 1368, 1369, 1881, 1371, 1882, 1376, 1889, 355, 868, 357, 1382, 360, 1896, 362, 1386, 1388, 369, 1395, 373, 1910, 1401, 379, 1404, 895, 1408, 386, 1416, 1421, 400, 1424, 1937, 917, 921, 1433, 1438, 1956, 1957, 937, 938, 939, 1968, 433, 1457, 1458, 950, 442, 1471, 452, 453, 1988, 1479, 1480, 462, 974, 466, 978, 468, 2003, 470, 471, 984, 2005, 2013, 482, 2020, 997, 1512, 494, 1518, 497, 2034, 499, 1530, 508, 1533}\n",
      "[   3    9   11   13   23   27   29   30   37   40   46   51   52   61\n",
      "   73   75   78   79   80   82   85   91  104  110  113  119  122  131\n",
      "  132  139  144  146  157  160  167  168  176  177  192  193  196  203\n",
      "  207  208  216  224  226  233  234  241  244  259  261  267  268  271\n",
      "  275  277  284  286  289  293  308  310  313  316  319  325  334  342\n",
      "  351  361  370  371  374  377  383  384  390  393  405  406  410  415\n",
      "  425  430  434  436  438  441  450  459  463  464  467  472  488  493\n",
      "  500  501  505  506  522  538  540  544  549  558  564  566  571  578\n",
      "  579  584  585  591  594  600  602  615  630  638  639  640  646  647\n",
      "  658  659  667  668  674  680  682  686  690  692  696  701  703  708\n",
      "  719  727  734  743  747  749  753  754  755  759  763  764  773  775\n",
      "  778  787  792  798  800  801  804  806  808  823  834  839  841  843\n",
      "  869  870  872  873  877  887  893  898  899  901  907  908  922  923\n",
      "  926  928  940  943  944  946  949  951  954  956  959  965  966  968\n",
      "  975  982  995 1000 1002 1003 1010 1013 1017 1041 1044 1045 1047 1053\n",
      " 1057 1063 1074 1077 1078 1086 1092 1105 1107 1110 1113 1116 1120 1124\n",
      " 1126 1127 1132 1140 1143 1148 1152 1157 1160 1161 1163 1165 1167 1183\n",
      " 1195 1204 1206 1216 1217 1231 1235 1240 1243 1247 1252 1258 1265 1267\n",
      " 1272 1274 1276 1287 1288 1311 1317 1327 1328 1335 1337 1340 1343 1346\n",
      " 1347 1349 1352 1361 1363 1366 1370 1375 1377 1379 1380 1383 1390 1406\n",
      " 1410 1411 1418 1422 1425 1428 1443 1451 1462 1489 1497 1498 1502 1504\n",
      " 1522 1529 1535 1536 1540 1548 1555 1560 1564 1565 1566 1585 1597 1598\n",
      " 1603 1605 1606 1610 1611 1612 1617 1619 1620 1624 1626 1629 1631 1638\n",
      " 1642 1644 1646 1651 1671 1672 1677 1679 1681 1688 1693 1694 1695 1697\n",
      " 1700 1710 1711 1712 1714 1718 1725 1736 1737 1740 1741 1743 1750 1753\n",
      " 1756 1761 1769 1774 1776 1781 1785 1787 1794 1795 1799 1800 1806 1812\n",
      " 1816 1819 1824 1825 1834 1840 1844 1849 1851 1852 1853 1866 1871 1873\n",
      " 1878 1879 1885 1892 1901 1904 1906 1907 1920 1921 1925 1928 1930 1932\n",
      " 1936 1940 1941 1945 1947 1950 1954 1961 1972 1980 1981 1986 1996 2002\n",
      " 2015 2017 2030 2042 2053 2058 2059 2064 2067 2071 2075 2091 2095 2097\n",
      " 2099 2113 2114 2118 2122 2123 2130 2132 2146 2152 2154 2156 2165 2171\n",
      " 2174 2177 2183 2184 2186 2187 2194 2198 2203 2206 2219 2225 2226 2236\n",
      " 2241 2243 2248 2252 2255 2257 2261 2263 2266 2270 2274 2284 2289 2293\n",
      " 2302 2303 2334 2343 2344 2349 2352 2358 2359 2365 2367]\n",
      "hyps cls is {325, 843, 1231, 755, 1852} and ref cls is {640, 1536, 1921, 132, 1540, 1925, 1671, 139, 267, 1529, 1677, 146, 2067, 277, 23, 1047, 1688, 538, 1947, 2075, 157, 544, 1700, 1834, 558, 2352, 1585, 564, 692, 438, 1340, 1343, 1216, 2241, 2114, 579, 1092, 2243, 839, 968, 841, 1352, 203, 459, 1740, 1741, 2248, 1105, 82, 1107, 2002, 1143, 1750, 1878, 2263, 1497, 1116, 1756, 1377, 2146, 1252, 1638, 2152, 234, 1132, 877, 1265, 500, 1781, 374, 119, 505}\n",
      "hyps cls is {3, 2053, 2058, 11, 2059, 13, 2064, 2067, 23, 2071, 27, 2075, 29, 30, 37, 40, 46, 2095, 2097, 51, 52, 2099, 61, 2113, 2114, 2118, 73, 2122, 75, 2123, 78, 79, 82, 2130, 2132, 85, 91, 104, 2152, 2154, 2156, 113, 2165, 2171, 2174, 2177, 132, 2186, 139, 2187, 144, 146, 157, 2206, 160, 167, 168, 176, 177, 2225, 2226, 2236, 192, 2241, 2243, 196, 2248, 203, 2252, 208, 2257, 2261, 2263, 216, 2266, 2270, 224, 226, 2274, 233, 234, 2289, 244, 2293, 2302, 261, 267, 268, 271, 275, 277, 284, 286, 2334, 289, 293, 2343, 2344, 2349, 308, 310, 2358, 2359, 2365, 319, 351, 361, 371, 374, 377, 383, 384, 390, 406, 425, 430, 434, 436, 450, 459, 463, 464, 467, 472, 488, 493, 500, 501, 505, 522, 538, 540, 544, 549, 566, 571, 578, 579, 584, 585, 591, 594, 600, 602, 615, 638, 639, 640, 646, 647, 658, 667, 674, 680, 682, 690, 692, 696, 701, 703, 708, 719, 727, 734, 743, 747, 749, 753, 763, 773, 775, 778, 787, 792, 800, 801, 804, 806, 808, 823, 834, 839, 841, 870, 877, 887, 893, 898, 899, 901, 908, 923, 926, 928, 940, 943, 944, 946, 949, 951, 954, 956, 959, 966, 968, 975, 982, 995, 1000, 1010, 1013, 1017, 1041, 1044, 1047, 1053, 1057, 1063, 1074, 1077, 1078, 1092, 1105, 1113, 1124, 1126, 1127, 1132, 1140, 1143, 1148, 1152, 1157, 1160, 1161, 1163, 1165, 1167, 1183, 1206, 1217, 1235, 1240, 1243, 1252, 1258, 1265, 1267, 1272, 1274, 1276, 1287, 1288, 1311, 1317, 1327, 1328, 1340, 1343, 1346, 1349, 1352, 1361, 1366, 1370, 1375, 1377, 1383, 1390, 1406, 1410, 1418, 1422, 1425, 1428, 1443, 1462, 1489, 1498, 1502, 1504, 1522, 1529, 1535, 1536, 1540, 1564, 1565, 1585, 1597, 1598, 1603, 1606, 1612, 1617, 1619, 1620, 1624, 1626, 1629, 1638, 1642, 1644, 1646, 1651, 1671, 1672, 1677, 1679, 1688, 1694, 1695, 1697, 1700, 1710, 1711, 1712, 1718, 1725, 1736, 1740, 1750, 1753, 1756, 1769, 1776, 1781, 1785, 1787, 1795, 1806, 1812, 1819, 1824, 1825, 1834, 1840, 1844, 1851, 1853, 1866, 1878, 1879, 1892, 1904, 1907, 1920, 1921, 1925, 1930, 1932, 1936, 1940, 1945, 1950, 1954, 1961, 1980, 1981, 1986, 1996, 2002, 2017, 2030, 2042} and ref cls is {1612, 2118}\n",
      "hyps cls is {1761, 393, 2284, 2194, 1363, 564, 1497, 122, 668} and ref cls is {1504, 1761, 578, 1795, 1824, 1954, 2156, 464, 113, 2064, 2289, 122, 2171}\n",
      "hyps cls is {1743} and ref cls is {393, 1806, 1695, 801, 1057, 1825, 804, 167, 1840, 177, 2099, 823, 696, 1337, 1853, 2365, 319, 325, 1606, 585, 843, 591, 1231, 2257, 594, 1235, 85, 1879, 1629, 734, 1247, 1885, 743, 1390, 244, 1140}\n",
      "hyps cls is {9, 1548, 1555, 1045, 1560, 1566, 2091, 558, 1086, 1605, 1610, 1611, 80, 1107, 1110, 1116, 1631, 1120, 2146, 110, 630, 119, 131, 2183, 2184, 1681, 659, 2198, 2203, 1693, 1195, 2219, 686, 1714, 1204, 1216, 193, 1737, 1741, 207, 2255, 1247, 1774, 241, 754, 759, 764, 2303, 1794, 259, 1799, 1800, 1816, 798, 2352, 1335, 313, 1337, 1849, 316, 2367, 1347, 1871, 1873, 342, 1885, 1379, 1380, 869, 872, 873, 1901, 370, 1906, 1411, 1928, 907, 405, 1941, 410, 922, 1947, 415, 1451, 1972, 438, 441, 965, 2015, 1002, 1003, 506} and ref cls is {3, 2053, 9, 2058, 1555, 1560, 1565, 549, 46, 2095, 2097, 1074, 1077, 566, 1078, 1597, 584, 73, 1631, 1120, 1124, 1127, 110, 630, 2183, 1160, 2186, 2187, 1681, 658, 2198, 667, 1693, 1694, 674, 168, 682, 1725, 703, 192, 1217, 2252, 207, 2261, 2270, 2284, 1774, 1776, 759, 2302, 259, 261, 778, 1816, 284, 286, 798, 800, 1317, 2349, 1844, 2359, 1849, 1851, 1852, 2367, 1866, 1871, 1873, 1363, 342, 1370, 1380, 1383, 872, 1901, 1904, 370, 371, 887, 383, 899, 1932, 1940, 405, 406, 1941, 410, 926, 415, 928, 940, 949, 441, 1980, 1489, 1498, 1502, 2015, 1535}\n",
      "hyps cls is {334} and ref cls is {522, 11, 1548, 13, 2059, 1041, 1044, 1045, 2071, 27, 540, 29, 30, 1053, 1564, 1566, 2042, 37, 1063, 40, 2091, 51, 52, 571, 61, 1086, 1598, 2113, 1603, 1605, 1610, 75, 1611, 2122, 78, 79, 80, 1617, 2123, 1619, 1620, 2130, 1110, 2132, 600, 1113, 602, 91, 1624, 1626, 1126, 615, 104, 1642, 2154, 1644, 1646, 1651, 2165, 1148, 638, 639, 1152, 2174, 2177, 131, 1157, 646, 647, 1672, 1161, 2184, 1163, 1165, 1167, 144, 1679, 2194, 659, 2203, 668, 2206, 1183, 160, 1697, 680, 1195, 2219, 686, 1710, 176, 1711, 690, 1712, 1204, 1714, 1206, 1718, 2225, 2226, 2236, 701, 193, 196, 708, 1736, 1737, 719, 208, 1743, 2255, 727, 216, 1240, 1753, 1243, 2266, 224, 226, 2274, 233, 1258, 747, 1769, 749, 241, 753, 754, 755, 1267, 2293, 1272, 1785, 1274, 763, 764, 1276, 1787, 2303, 1794, 773, 775, 1287, 1288, 1799, 1800, 268, 271, 275, 787, 1812, 792, 1819, 2334, 1311, 289, 293, 806, 2343, 808, 2344, 1327, 1328, 308, 310, 1335, 2358, 313, 316, 834, 1346, 1347, 1349, 334, 1361, 1366, 351, 1375, 1379, 1892, 869, 870, 361, 873, 1906, 1907, 377, 893, 1406, 384, 1920, 898, 1410, 1411, 901, 390, 1928, 1418, 907, 908, 1930, 1422, 1936, 1425, 1428, 1945, 922, 923, 1950, 1443, 425, 1961, 1451, 430, 943, 944, 434, 946, 436, 1972, 1462, 951, 954, 956, 1981, 959, 450, 1986, 965, 966, 1996, 463, 975, 467, 982, 472, 2017, 995, 488, 1000, 1002, 1003, 493, 2030, 1010, 1522, 501, 1013, 1017, 506}\n",
      "pedersen precision: 0.24693333333333334\n",
      "pedersen recall: 0.20816666666666664\n",
      "pedersen f_measure: 0.24693333333333334\n",
      "pedersen accuracy: 0.07100000000000001\n",
      "\n",
      "[   4   15   16   18   21   23   28   38   55   58   59   67   68   74\n",
      "   80   92   96   98  104  108  113  115  122  129  134  138  144  150\n",
      "  152  158  174  178  179  182  190  200  201  213  215  220  234  235\n",
      "  237  244  249  252  256  258  271  288  290  294  300  301  313  323\n",
      "  326  330  338  344  349  350  355  357  361  363  368  371  381  383\n",
      "  394  395  397  406  407  413  414  420  433  448  450  453  455  471\n",
      "  472  479  481  486  491  494  496  498  501  513  517  523  526  530\n",
      "  536  540  542  550  551  552  558  559  561  564  573  585  589  590\n",
      "  592  593  597  606  607  608  615  616  626  630  634  635  636  638\n",
      "  640  650  658  668  672  673  676  684  690  695  704  724  725  730\n",
      "  731  745  749  750  752  754  757  758  761  774  776  784  785  787\n",
      "  790  795  798  801  805  808  822  826  827  830  831  833  837  838\n",
      "  841  844  848  850  852  855  859  861  863  872  876  880  884  891\n",
      "  893  896  906  909  914  917  920  922  923  933  936  938  942  944\n",
      "  950  955  958  965  968  971  975  976  977  979  981  991  994  995\n",
      "  999 1000 1008 1010 1016 1028 1041 1044 1047 1049 1057 1060 1061 1064\n",
      " 1070 1071 1085 1086 1091 1102 1106 1108 1111 1113 1131 1135 1136 1137\n",
      " 1140 1141 1142 1143 1146 1148 1155 1156 1157 1167 1169 1183 1192 1194\n",
      " 1199 1208 1213 1220 1223 1230 1237 1248 1251 1253 1254 1266 1268 1272\n",
      " 1273 1286 1292 1293 1296 1306 1311 1312 1318 1326 1331 1333 1334 1340\n",
      " 1358 1363 1368 1373 1378 1384 1394 1397 1410 1416 1419 1421 1432 1440\n",
      " 1441 1445 1449 1450 1452 1457 1460 1461 1464 1466 1467 1468 1471 1475\n",
      " 1484 1487 1498 1499 1506 1513 1514 1515 1517 1521 1526 1530 1531 1536\n",
      " 1549 1556 1559 1567 1574 1584 1585 1589 1608 1613 1617 1622 1624 1629\n",
      " 1643 1656 1659 1679 1687 1699 1702 1712 1714 1718 1720 1721 1726 1736\n",
      " 1742 1746 1750 1765 1767 1771 1772 1780 1784 1785 1789 1791 1796 1797\n",
      " 1798 1811 1812 1813 1823 1826 1835 1837 1840 1845 1846 1854 1861 1864\n",
      " 1874 1879 1888 1893 1907 1910 1912 1914 1916 1929 1938 1941 1942 1947\n",
      " 1948 1953 1959 1961 1966 1969 1983 1984 1985 1986 1998 2000 2010 2016\n",
      " 2019 2022 2029 2038 2041 2045 2049 2050 2051 2054 2069 2072 2081 2082\n",
      " 2087 2089 2090 2093 2103 2107 2111 2113 2115 2121 2124 2126 2129 2130\n",
      " 2154 2156 2160 2163 2168 2173 2175 2178 2188 2197 2202 2205 2209 2212\n",
      " 2216 2221 2230 2232 2236 2240 2242 2249 2256 2258 2268 2274 2282 2288\n",
      " 2295 2305 2310 2312 2314 2329 2331 2336 2344 2350 2353 2357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps cls is {2049, 1796, 1549, 672, 1826, 551, 552, 1449, 1584, 1585, 2353, 833, 838, 455, 968, 2249, 1487, 1106, 1746, 724, 1874, 1750, 731, 220, 234, 235, 880, 630} and ref cls is {640, 513, 1536, 1273, 1286, 16, 1296, 18, 914, 150, 23, 790, 1047, 1049, 1687, 1942, 1947, 158, 1948, 672, 673, 1312, 1441, 1826, 2209, 1574, 2336, 300, 301, 558, 559, 684, 1585, 2353, 179, 564, 1460, 55, 58, 1340, 573, 1085, 1468, 833, 67, 68, 837, 838, 1475, 968, 841, 1613, 1487, 2331, 338, 1106, 852, 597, 1750, 2258, 2016, 1251, 234, 235, 1131, 1517, 880, 1142, 1143, 249, 1916, 2045, 1791}\n",
      "hyps cls is {1441, 1155, 4, 1765, 134, 1608, 1864, 394, 395, 971, 1515, 561, 1331, 831, 2038, 23, 1854, 479} and ref cls is {220, 2230}\n",
      "hyps cls is {2050, 2054, 523, 15, 21, 1047, 1559, 1049, 2072, 28, 540, 542, 1057, 550, 2087, 2089, 2090, 558, 564, 1589, 2103, 80, 592, 1624, 1113, 1629, 606, 608, 98, 104, 2154, 1131, 1643, 1141, 1143, 2168, 634, 638, 640, 129, 138, 650, 1167, 1169, 150, 1687, 152, 2205, 158, 1183, 1192, 684, 1199, 182, 2242, 201, 1230, 213, 1237, 730, 2274, 237, 750, 754, 1780, 1272, 1273, 1797, 1798, 1292, 271, 1813, 1311, 288, 300, 301, 1845, 822, 827, 1340, 330, 349, 861, 1378, 357, 1893, 872, 1384, 876, 368, 371, 1910, 383, 1929, 397, 909, 914, 1941, 406, 413, 1953, 933, 938, 1452, 433, 950, 1464, 1466, 1484, 976, 2000, 981, 994, 486, 999, 2022, 1514, 491, 1517, 2029, 1521, 498, 1526, 1530} and ref cls is {2242, 551, 2087, 1513, 394, 395, 1450, 1515, 2156, 113, 178, 122, 1823}\n",
      "hyps cls is {1536, 513, 2051, 1028, 526, 16, 1041, 18, 530, 1044, 1556, 536, 1567, 2082, 1060, 38, 1064, 2093, 559, 55, 58, 2107, 573, 1091, 68, 2115, 2121, 74, 2124, 589, 590, 1102, 1613, 593, 1617, 2130, 597, 1622, 1111, 92, 607, 615, 1136, 1137, 626, 2160, 1142, 1656, 122, 636, 2173, 2188, 1679, 2197, 2202, 668, 2209, 676, 1702, 2216, 1194, 2221, 690, 179, 2230, 695, 2232, 1213, 190, 704, 1223, 1742, 2256, 2258, 725, 215, 2268, 1248, 1251, 1254, 745, 2282, 1771, 1772, 752, 1266, 244, 1784, 761, 1785, 252, 1789, 1791, 256, 1286, 776, 2312, 1296, 1811, 790, 2331, 798, 1823, 2336, 801, 290, 805, 2344, 1835, 2350, 1840, 1333, 1846, 2357, 323, 837, 1861, 841, 844, 338, 850, 852, 1363, 1879, 344, 1368, 350, 355, 361, 1907, 884, 1914, 891, 381, 1410, 1416, 1938, 917, 1947, 1948, 420, 1445, 1959, 1450, 942, 1966, 1457, 1969, 1460, 1467, 958, 448, 1985, 1986, 1475, 1998, 1498, 1499, 2016, 1506, 2019, 1000, 1513, 1008, 501, 2041, 1531, 2045} and ref cls is {152, 2329, 801, 1057, 676, 38, 1959, 2089, 1840, 1969, 2107, 1984, 326, 585, 971, 589, 590, 1230, 592, 593, 976, 1742, 724, 1998, 1879, 1629, 1253, 2022, 876, 1772, 368, 244, 1140, 1780, 2041}\n",
      "hyps cls is {1419, 1421, 658, 407, 2329, 1440, 2212, 174, 178, 1468, 1726, 1984, 67, 326, 585, 991, 1253, 2156, 494, 1135, 2288, 1916, 2175} and ref cls is {2049, 2050, 2051, 4, 517, 2054, 1549, 526, 1556, 1559, 1567, 2082, 550, 552, 1584, 561, 1589, 2103, 1091, 2115, 2121, 74, 606, 607, 1137, 626, 630, 1656, 1659, 2173, 1155, 134, 2188, 1169, 658, 1699, 1702, 2216, 174, 2232, 1726, 704, 2249, 2256, 1746, 730, 731, 2268, 1248, 1765, 1254, 1771, 2288, 1789, 256, 1796, 2314, 798, 805, 2350, 1846, 1854, 831, 1861, 1864, 330, 844, 850, 1363, 1874, 1378, 872, 1384, 371, 884, 381, 383, 397, 1941, 406, 407, 413, 414, 1440, 1953, 420, 1449, 942, 455, 2000, 981, 1498, 1499, 2010, 486, 491, 2029, 1008, 2038, 1531}\n",
      "hyps cls is {517, 2069, 2081, 1061, 1574, 1070, 1071, 59, 1085, 1086, 2111, 2113, 2126, 2129, 1108, 96, 616, 108, 113, 115, 1140, 2163, 1146, 635, 1148, 1659, 2178, 1156, 1157, 144, 673, 1699, 1712, 1714, 1718, 1208, 1720, 1721, 2236, 2240, 1220, 200, 1736, 1767, 749, 1268, 757, 758, 2295, 249, 2305, 258, 774, 2310, 2314, 1293, 784, 785, 787, 1812, 1306, 795, 1312, 294, 1318, 808, 1837, 1326, 1334, 313, 826, 830, 1358, 848, 855, 859, 1373, 863, 1888, 363, 1394, 1397, 1912, 893, 896, 906, 1942, 920, 1432, 922, 923, 414, 936, 1961, 944, 1461, 955, 1471, 1983, 450, 453, 965, 975, 977, 979, 471, 472, 2010, 481, 995, 496, 1010, 1016} and ref cls is {1028, 523, 15, 1041, 530, 1044, 21, 2069, 536, 2072, 28, 540, 542, 2081, 1060, 1061, 1064, 2090, 2093, 1070, 1071, 59, 1086, 2111, 2113, 1608, 2124, 1102, 2126, 80, 1617, 2129, 2130, 1108, 1622, 1111, 1624, 1113, 92, 96, 608, 98, 615, 104, 616, 2154, 1643, 108, 1135, 1136, 2160, 115, 2163, 1141, 2168, 634, 635, 636, 1146, 638, 1148, 2175, 129, 2178, 1156, 1157, 138, 650, 1167, 144, 1679, 2197, 2202, 668, 2205, 1183, 2212, 1192, 1194, 2221, 1199, 1712, 690, 1714, 182, 695, 1208, 1718, 1720, 1721, 2236, 1213, 190, 2240, 1220, 1223, 200, 201, 1736, 213, 725, 215, 1237, 2274, 1767, 745, 2282, 237, 749, 750, 752, 754, 1266, 1268, 757, 758, 2295, 1272, 761, 1784, 1785, 252, 2305, 258, 1797, 774, 1798, 776, 2310, 2312, 1292, 1293, 271, 784, 785, 787, 1811, 1812, 1813, 1306, 795, 1311, 288, 290, 294, 1318, 808, 2344, 1835, 1837, 1326, 1331, 1333, 822, 1334, 1845, 313, 826, 827, 2357, 830, 323, 1358, 848, 855, 344, 1368, 859, 349, 350, 861, 863, 1373, 1888, 355, 357, 1893, 361, 363, 1394, 1907, 1397, 1910, 1912, 1914, 891, 893, 896, 1410, 1416, 1929, 906, 1419, 909, 1421, 1938, 917, 920, 1432, 922, 923, 933, 1445, 936, 1961, 938, 1452, 1966, 944, 433, 1457, 1461, 950, 1464, 1466, 955, 1467, 958, 1471, 448, 1983, 450, 1985, 1986, 453, 965, 1484, 975, 977, 979, 471, 472, 479, 991, 481, 994, 995, 1506, 2019, 999, 1000, 1514, 494, 496, 1521, 498, 1010, 501, 1526, 1016, 1530}\n",
      "[   9   12   14   17   19   25   31   33   39   41   50   51   56   64\n",
      "   69   89   93  100  103  118  120  126  132  141  147  157  167  169\n",
      "  170  175  183  187  197  202  206  208  212  216  218  219  233  238\n",
      "  240  253  262  263  265  267  270  273  275  279  280  282  285  293\n",
      "  295  296  297  305  310  311  315  317  318  321  327  336  340  341\n",
      "  346  351  352  356  358  366  388  390  391  398  399  415  416  423\n",
      "  428  429  430  436  440  441  443  452  458  461  466  468  470  476\n",
      "  484  489  492  497  502  508  512  521  529  544  547  557  577  579\n",
      "  581  586  591  595  599  613  617  618  624  628  642  652  657  665\n",
      "  669  670  671  674  679  693  697  698  699  703  706  717  720  736\n",
      "  739  742  748  751  755  756  772  778  779  781  800  809  814  819\n",
      "  828  839  840  853  866  871  875  877  887  897  899  904  905  908\n",
      "  910  911  913  915  918  921  931  937  940  948  963  970  973  985\n",
      "  998 1003 1005 1013 1018 1023 1043 1048 1052 1062 1065 1068 1069 1075\n",
      " 1082 1083 1096 1097 1099 1109 1115 1116 1119 1121 1123 1127 1130 1134\n",
      " 1160 1161 1166 1168 1174 1175 1176 1177 1181 1185 1186 1187 1188 1195\n",
      " 1196 1197 1203 1205 1206 1211 1217 1221 1222 1224 1226 1231 1238 1242\n",
      " 1244 1249 1263 1269 1280 1290 1301 1309 1324 1335 1338 1345 1347 1349\n",
      " 1353 1354 1356 1362 1366 1374 1385 1392 1395 1407 1409 1417 1425 1427\n",
      " 1439 1442 1443 1451 1455 1456 1459 1462 1470 1474 1478 1480 1481 1482\n",
      " 1489 1496 1500 1501 1504 1519 1523 1525 1532 1533 1534 1538 1542 1543\n",
      " 1545 1550 1555 1560 1561 1570 1573 1576 1577 1578 1588 1591 1592 1597\n",
      " 1601 1602 1611 1615 1620 1625 1628 1645 1647 1650 1655 1662 1665 1670\n",
      " 1674 1676 1677 1678 1684 1688 1689 1690 1691 1698 1707 1708 1713 1715\n",
      " 1719 1723 1733 1740 1744 1745 1751 1754 1757 1760 1761 1763 1770 1795\n",
      " 1809 1816 1819 1829 1831 1838 1841 1843 1844 1850 1857 1862 1866 1867\n",
      " 1871 1878 1881 1882 1883 1889 1892 1903 1909 1911 1913 1919 1920 1921\n",
      " 1923 1924 1931 1933 1940 1944 1945 1949 1950 1956 1960 1978 1980 1982\n",
      " 1989 1991 1994 1995 1996 1997 2001 2013 2020 2027 2046 2056 2060 2062\n",
      " 2064 2065 2067 2068 2070 2078 2086 2088 2099 2101 2102 2110 2116 2117\n",
      " 2119 2123 2138 2140 2144 2147 2150 2158 2166 2171 2172 2176 2184 2189\n",
      " 2190 2191 2192 2195 2198 2199 2203 2206 2217 2218 2226 2231 2234 2237\n",
      " 2239 2244 2248 2265 2270 2271 2276 2277 2283 2284 2285 2287 2294 2298\n",
      " 2303 2304 2311 2322 2325 2337 2338 2339 2340 2343 2355 2358]\n",
      "hyps cls is {1417, 1931, 657, 1940, 1816, 670, 800, 39, 2088, 2218, 1197, 819, 436, 948, 1844, 2234, 577, 2116, 2248, 1867, 1745, 1504, 1761, 739, 1127, 1525, 2166, 887} and ref cls is {512, 1921, 132, 772, 1670, 904, 1290, 267, 1677, 1427, 1684, 2067, 2068, 1048, 25, 1688, 1689, 1690, 157, 1691, 1439, 544, 1949, 1442, 1698, 2337, 41, 1065, 1324, 557, 693, 1205, 440, 1592, 1850, 315, 1083, 1978, 2234, 321, 706, 579, 1474, 1601, 839, 840, 1224, 458, 970, 1354, 1740, 1997, 2248, 212, 853, 1878, 1116, 1628, 2271, 352, 2147, 2150, 489, 618, 1130, 2027, 877, 366, 1134, 1263, 1519, 2294}\n",
      "hyps cls is {1542, 263, 1543, 1550, 2198, 1560, 1181, 1698, 931, 1062, 1456, 2046, 441, 1723, 1980, 1597, 1862, 1740, 1119, 2285, 2287, 118, 1911, 2172, 1662} and ref cls is {1744, 670}\n",
      "hyps cls is {512, 1545, 12, 2060, 529, 1043, 2067, 2068, 2070, 2078, 544, 1577, 1075, 2102, 56, 1083, 2110, 581, 1615, 599, 1625, 2140, 617, 628, 642, 132, 1160, 1166, 2190, 1168, 665, 671, 674, 1707, 175, 1203, 1206, 187, 699, 2237, 1217, 1231, 208, 720, 1751, 219, 2270, 1760, 1770, 238, 1263, 756, 1280, 2311, 267, 779, 781, 1809, 279, 280, 1819, 285, 2337, 2338, 1831, 296, 305, 1841, 2358, 311, 1345, 1857, 1349, 839, 1866, 1871, 346, 1882, 1374, 352, 866, 1892, 358, 875, 1903, 899, 388, 905, 911, 915, 918, 921, 1439, 1956, 430, 1978, 1982, 1474, 452, 1989, 1995, 461, 1489, 1496, 476, 998, 492, 497, 1523, 1018} and ref cls is {1504, 1761, 1795, 484, 2311, 1417, 2191, 2064, 1841, 2192, 948, 2171, 1662, 671}\n",
      "hyps cls is {1538, 2056, 9, 14, 2064, 2065, 1555, 1048, 1561, 33, 547, 1573, 41, 1065, 50, 2101, 1591, 1592, 1601, 1602, 579, 69, 2117, 2119, 1096, 586, 2123, 595, 1620, 2138, 1116, 1628, 1121, 2147, 613, 1130, 1647, 624, 1655, 120, 2184, 1161, 652, 2189, 2191, 2192, 147, 1684, 1175, 1688, 1177, 1689, 1690, 2203, 157, 2206, 1186, 1187, 1188, 167, 679, 169, 2217, 1195, 1196, 1205, 183, 1719, 2231, 703, 706, 1224, 202, 717, 206, 1744, 212, 2265, 1242, 1244, 1757, 2271, 1249, 2283, 2284, 755, 2294, 2298, 2303, 1795, 772, 262, 265, 778, 1290, 270, 275, 282, 2339, 2340, 2343, 297, 809, 1324, 1843, 310, 1335, 1338, 1850, 828, 318, 321, 327, 840, 1353, 1354, 1362, 341, 853, 1878, 351, 1889, 1385, 877, 366, 1392, 1913, 1919, 1409, 1921, 390, 904, 1933, 910, 1427, 1945, 415, 416, 1442, 1443, 423, 1960, 937, 1451, 940, 1455, 1459, 440, 443, 1478, 458, 970, 1482, 973, 1997, 2001, 466, 468, 985, 1500, 1501, 484, 1003, 2027, 1005, 1519, 502, 1533, 1023} and ref cls is {388, 262, 265, 1545, 1676, 2189, 657, 1177, 2078, 1062, 167, 39, 1708, 1197, 1455, 2099, 2231, 318, 577, 1345, 2117, 327, 1481, 586, 591, 1231, 595, 218, 1760, 998, 875, 1392, 756, 1655, 2046}\n",
      "hyps cls is {1670, 1677, 1944, 1176, 1576, 557, 2099, 1082, 315, 317, 2239, 1097, 1481, 1996, 591, 336, 218, 2013, 1123, 356, 489, 618, 1134} and ref cls is {1538, 1542, 2056, 9, 1550, 2065, 1555, 1560, 547, 1576, 2088, 50, 1075, 2101, 2102, 1591, 1082, 1597, 2110, 2116, 581, 2138, 1119, 1121, 1123, 1127, 624, 628, 118, 2166, 120, 1160, 652, 2198, 1176, 1181, 1185, 674, 1187, 1188, 679, 169, 2217, 2218, 175, 1719, 699, 1723, 703, 1217, 202, 717, 1745, 1757, 2270, 739, 2277, 2284, 2298, 778, 779, 781, 1816, 282, 285, 800, 2339, 2340, 1843, 1844, 1862, 1866, 1867, 1871, 1362, 887, 1919, 899, 1931, 1933, 1940, 1944, 415, 416, 931, 423, 1960, 940, 1456, 441, 443, 1980, 461, 1489, 2001, 1496, 1500, 1501, 1525, 1018}\n",
      "hyps cls is {521, 2062, 17, 19, 25, 1052, 31, 1570, 2086, 1578, 1068, 1069, 51, 1588, 64, 1099, 1611, 1109, 89, 1115, 93, 2144, 100, 2150, 103, 1645, 2158, 1650, 2171, 126, 2176, 1665, 1674, 1676, 141, 1678, 2195, 1174, 2199, 1691, 669, 1185, 170, 1708, 1713, 2226, 1715, 693, 697, 698, 1211, 2244, 197, 1221, 1222, 1733, 1226, 1238, 216, 1754, 736, 1763, 2276, 2277, 742, 233, 748, 751, 240, 1269, 253, 2304, 273, 2322, 1301, 2325, 1309, 293, 1829, 295, 814, 1838, 2355, 1347, 1356, 340, 1366, 1881, 1883, 871, 1395, 1909, 1407, 1920, 897, 1923, 1924, 391, 908, 398, 399, 913, 1425, 1949, 1950, 428, 429, 1462, 1470, 963, 1991, 1480, 1994, 470, 2020, 508, 1013, 1532, 1534} and ref cls is {1543, 521, 12, 2060, 14, 2062, 17, 529, 19, 1043, 2070, 1561, 1052, 1532, 31, 33, 1570, 1573, 2086, 1577, 1578, 1068, 1069, 51, 1588, 56, 64, 1602, 69, 2119, 1096, 1097, 1099, 1611, 2123, 1615, 1620, 1109, 599, 89, 1625, 1115, 2140, 93, 2144, 100, 613, 103, 617, 1645, 2158, 1647, 1650, 2172, 126, 2176, 1665, 642, 2184, 1161, 1674, 141, 1166, 1678, 1168, 2190, 147, 2195, 1174, 1175, 2199, 665, 2203, 669, 2206, 1186, 170, 1195, 1196, 1707, 1713, 2226, 1203, 1715, 1206, 183, 697, 698, 187, 1211, 2237, 2239, 2244, 197, 1221, 1222, 1733, 1226, 206, 208, 720, 1238, 1751, 216, 2265, 1242, 219, 1244, 1754, 736, 1249, 1763, 2276, 742, 233, 1770, 2283, 748, 2285, 238, 751, 240, 2287, 755, 1269, 253, 2303, 1280, 2304, 263, 270, 273, 1809, 275, 2322, 1301, 2325, 279, 280, 1819, 1309, 2338, 293, 1829, 295, 296, 297, 809, 1831, 2343, 814, 1838, 305, 819, 2355, 310, 311, 1335, 2358, 1338, 828, 317, 1857, 1347, 1349, 1353, 1356, 336, 340, 341, 1366, 1881, 346, 1882, 1883, 1374, 351, 1889, 866, 356, 1892, 358, 871, 1385, 1903, 1395, 1909, 1911, 1913, 1407, 1920, 897, 1409, 1923, 1924, 390, 391, 905, 908, 398, 399, 910, 911, 913, 915, 1425, 918, 921, 1945, 1950, 1443, 1956, 937, 1451, 428, 429, 430, 1459, 436, 1462, 1470, 1982, 963, 452, 1989, 1478, 1991, 1480, 1482, 1994, 1995, 973, 1996, 466, 468, 470, 985, 476, 2013, 2020, 1003, 492, 1005, 497, 1523, 1013, 502, 508, 1533, 1534, 1023}\n",
      "[   0    1   11   40   42   47   48   52   54   60   63   66   70   72\n",
      "   76   77   95  106  109  112  119  127  130  137  142  146  155  160\n",
      "  162  171  172  181  186  189  191  192  194  199  203  207  210  214\n",
      "  224  226  228  229  245  246  266  268  277  283  289  303  304  324\n",
      "  325  337  343  347  353  360  362  379  382  386  389  392  393  402\n",
      "  404  405  417  418  419  421  425  426  431  432  437  439  444  449\n",
      "  451  457  465  475  478  488  493  499  506  519  525  528  532  533\n",
      "  538  543  553  554  555  563  568  569  571  580  587  598  600  601\n",
      "  603  604  623  637  649  653  654  655  661  664  675  680  681  683\n",
      "  686  705  707  708  711  721  722  726  729  733  753  760  765  768\n",
      "  769  770  771  777  780  783  793  794  803  807  813  817  824  825\n",
      "  832  835  836  845  847  849  856  858  874  878  885  907  926  927\n",
      "  928  932  934  935  939  941  943  946  947  954  962  966  969  972\n",
      "  974  978  986  987  996  997 1001 1002 1006 1007 1012 1014 1017 1020\n",
      " 1022 1025 1027 1031 1033 1035 1036 1039 1040 1046 1053 1055 1056 1066\n",
      " 1067 1072 1079 1080 1081 1087 1088 1090 1092 1093 1100 1103 1110 1112\n",
      " 1114 1118 1125 1138 1147 1149 1158 1162 1163 1170 1179 1180 1184 1202\n",
      " 1207 1218 1219 1227 1232 1234 1236 1256 1257 1260 1265 1270 1275 1276\n",
      " 1279 1291 1297 1298 1300 1302 1310 1316 1319 1320 1321 1329 1332 1336\n",
      " 1341 1346 1350 1355 1360 1361 1364 1365 1372 1386 1387 1391 1399 1401\n",
      " 1415 1423 1426 1435 1436 1444 1473 1476 1483 1485 1490 1491 1502 1505\n",
      " 1509 1510 1527 1528 1529 1539 1541 1554 1562 1565 1566 1571 1575 1583\n",
      " 1586 1590 1593 1594 1605 1614 1616 1619 1623 1626 1627 1631 1632 1635\n",
      " 1640 1642 1651 1660 1666 1673 1680 1681 1686 1692 1693 1694 1695 1701\n",
      " 1703 1704 1711 1716 1717 1727 1731 1734 1737 1738 1739 1747 1753 1758\n",
      " 1762 1775 1778 1781 1782 1790 1793 1802 1808 1810 1818 1820 1824 1825\n",
      " 1832 1833 1834 1839 1849 1851 1852 1853 1856 1858 1860 1865 1868 1869\n",
      " 1873 1876 1884 1891 1896 1901 1902 1904 1908 1918 1930 1932 1934 1957\n",
      " 1958 1967 1968 1975 1987 1988 1990 1992 1993 1999 2002 2004 2005 2009\n",
      " 2012 2017 2021 2023 2028 2032 2036 2037 2040 2042 2052 2059 2066 2073\n",
      " 2076 2080 2091 2098 2100 2104 2105 2106 2112 2118 2133 2135 2146 2151\n",
      " 2157 2165 2177 2186 2200 2201 2207 2208 2213 2219 2223 2235 2241 2251\n",
      " 2253 2255 2257 2259 2261 2275 2278 2280 2286 2289 2290 2291 2293 2301\n",
      " 2317 2318 2321 2323 2333 2345 2346 2352 2356 2360 2364 2367]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps cls is {1025, 1423, 277, 664, 538, 1179, 1180, 928, 2080, 418, 554, 568, 1849, 1852, 1853, 962, 972, 1631, 623, 1904, 2289, 2291, 2036, 1660, 2301} and ref cls is {1793, 130, 1527, 1539, 1036, 525, 1934, 1039, 1423, 146, 1426, 532, 277, 1046, 2040, 2073, 538, 1692, 543, 1056, 1660, 553, 42, 426, 1834, 2346, 2223, 2352, 1202, 563, 437, 439, 2104, 569, 825, 2105, 2360, 1087, 449, 1218, 451, 580, 836, 1092, 1219, 2241, 457, 203, 1355, 1485, 337, 2002, 1627, 1372, 478, 2146, 229, 1510, 2151, 106, 2157, 1902, 1265, 2290, 2291, 1781, 1782, 119, 1528, 1529, 1020, 2301}\n",
      "hyps cls is {770, 324, 1092, 1510, 393, 1001, 939, 1002, 1257, 1614, 2076, 1680, 2042, 155, 1276, 1693, 1502} and ref cls is {2118, 1473, 1790}\n",
      "hyps cls is {1, 1539, 1541, 2059, 525, 1562, 1053, 1565, 543, 40, 553, 2098, 563, 2104, 1081, 66, 1093, 76, 1616, 2133, 2135, 1114, 1626, 1632, 2146, 1635, 106, 1642, 109, 119, 2177, 137, 649, 654, 146, 1686, 2207, 2208, 1703, 681, 171, 2219, 181, 189, 705, 707, 199, 1738, 2251, 2257, 1236, 214, 733, 1762, 229, 1256, 2286, 1265, 1778, 2290, 1270, 1275, 768, 771, 777, 780, 1818, 283, 1820, 1316, 1832, 1834, 1839, 304, 817, 824, 825, 2364, 832, 1858, 835, 1865, 1876, 1365, 858, 1372, 1884, 353, 1896, 1908, 1399, 1401, 379, 1930, 1932, 1934, 404, 405, 1436, 927, 941, 431, 946, 444, 1987, 1992, 1485, 1999, 465, 978, 1491, 2002, 987, 2028, 2032, 1014, 1528, 1529} and ref cls is {768, 1824, 1762, 1158, 1703, 266, 47, 2289, 2098, 404, 2364, 1436, 189}\n",
      "hyps cls is {1027, 1031, 1036, 1039, 528, 1554, 2066, 532, 1046, 2073, 1055, 1056, 1571, 1575, 42, 555, 1066, 47, 48, 54, 2105, 1594, 2106, 60, 1087, 1090, 70, 2118, 587, 1100, 77, 1619, 598, 1623, 1627, 604, 1118, 95, 1125, 2157, 112, 2165, 637, 130, 1162, 2186, 653, 655, 1170, 661, 2200, 1694, 1695, 1701, 680, 2223, 1202, 1717, 1207, 2235, 191, 192, 2241, 194, 1218, 1219, 1731, 203, 2255, 1232, 210, 722, 1234, 2259, 2261, 729, 1753, 1758, 226, 2275, 228, 2278, 1775, 753, 245, 1782, 2293, 760, 765, 1790, 769, 1793, 266, 1802, 268, 2317, 2318, 783, 1808, 2321, 1810, 2323, 1302, 793, 1310, 807, 1320, 2346, 813, 303, 2352, 1329, 1332, 2356, 1336, 2360, 1851, 1341, 2367, 1346, 836, 1860, 1355, 1869, 337, 849, 1873, 1364, 343, 1891, 1387, 1901, 1391, 382, 386, 389, 392, 402, 1426, 926, 419, 932, 1958, 935, 426, 1967, 432, 437, 439, 1975, 1473, 451, 1476, 1988, 966, 457, 969, 1490, 2009, 478, 1505, 2017, 996, 2021, 1006, 2037, 1527, 2040, 1020} and ref cls is {769, 393, 1802, 2317, 654, 1808, 402, 2066, 2201, 794, 1179, 1180, 2076, 1310, 1695, 1825, 554, 555, 1967, 1332, 54, 1593, 2106, 1853, 325, 587, 972, 1232, 2257, 2021, 1256, 2028, 245, 2037, 1275, 1918}\n",
      "hyps cls is {2052, 11, 1681, 1298, 794, 1692, 1824, 417, 421, 2213, 1067, 2091, 1072, 947, 2100, 1593, 63, 449, 325, 1737, 1739, 1868, 207, 1361, 726, 601, 2151, 360, 362, 1902, 1781} and ref cls is {1025, 1027, 2052, 1541, 1031, 1554, 1565, 1055, 1072, 1590, 568, 1594, 1090, 1093, 70, 72, 1616, 598, 2135, 601, 1631, 109, 623, 2186, 653, 655, 1681, 1170, 661, 664, 1693, 1694, 171, 683, 2235, 192, 705, 2251, 207, 210, 722, 1747, 2261, 726, 1758, 228, 1257, 1775, 760, 770, 777, 780, 783, 793, 1316, 1320, 1321, 813, 303, 1336, 1849, 1851, 1852, 2367, 324, 1860, 1865, 1868, 1869, 1873, 1364, 1876, 343, 1891, 1901, 878, 1904, 382, 1932, 405, 926, 928, 417, 418, 419, 421, 444, 1987, 1476, 969, 1490, 1491, 2009, 987, 2012, 1502, 1505, 1006, 2032, 2036}\n",
      "hyps cls is {0, 519, 1033, 1035, 1040, 533, 1566, 1583, 1586, 52, 1590, 1079, 1080, 569, 571, 1088, 2112, 580, 1605, 72, 1103, 1110, 600, 1112, 603, 1640, 1138, 1651, 1147, 1149, 127, 1666, 1158, 1673, 1163, 142, 2201, 160, 1184, 162, 675, 1704, 683, 172, 686, 1711, 1716, 186, 1727, 708, 1734, 711, 1227, 2253, 721, 1747, 224, 2280, 1260, 246, 1279, 1291, 1297, 1300, 2333, 289, 1825, 803, 1319, 1321, 1833, 2345, 1856, 1350, 845, 847, 1360, 856, 347, 874, 1386, 878, 885, 1918, 1415, 907, 1435, 1444, 1957, 934, 425, 943, 1968, 954, 1990, 1993, 1483, 974, 2004, 2005, 986, 475, 2012, 997, 1509, 2023, 488, 493, 1007, 499, 1012, 1017, 506, 1022} and ref cls is {0, 1, 519, 1033, 11, 1035, 2059, 528, 1040, 533, 1562, 1053, 1566, 2080, 2042, 1571, 1575, 40, 1066, 1067, 2091, 1583, 48, 1586, 52, 2100, 1079, 1080, 1081, 571, 60, 63, 1088, 2112, 66, 1605, 76, 77, 1100, 1103, 1614, 1619, 2133, 1110, 1623, 600, 1112, 1114, 603, 604, 1626, 1118, 95, 1632, 1635, 1125, 1640, 1642, 112, 1138, 1651, 2165, 1147, 637, 1149, 127, 2177, 1666, 137, 649, 1162, 1163, 1673, 142, 1680, 1686, 2200, 155, 2207, 160, 1184, 162, 675, 2208, 1701, 2213, 680, 681, 1704, 2219, 172, 686, 1711, 1716, 181, 1717, 1207, 186, 191, 1727, 194, 707, 708, 1731, 1734, 199, 711, 1737, 1738, 1227, 1739, 2253, 2255, 721, 1234, 2259, 1236, 214, 729, 1753, 733, 224, 226, 2275, 2278, 2280, 1260, 2286, 753, 1778, 2293, 246, 1270, 1276, 765, 1279, 771, 1291, 268, 2318, 1297, 1298, 1810, 1300, 2321, 1302, 2323, 1818, 283, 1820, 2333, 289, 803, 807, 1319, 1832, 1833, 2345, 1839, 304, 817, 1329, 2356, 824, 1341, 832, 1856, 1346, 835, 1858, 1350, 845, 847, 1360, 849, 1361, 1365, 856, 858, 347, 1884, 353, 360, 1896, 362, 874, 1386, 1387, 1391, 1908, 885, 1399, 1401, 379, 386, 389, 1415, 392, 1930, 907, 1435, 927, 932, 1444, 934, 935, 1957, 425, 1958, 939, 941, 431, 432, 943, 946, 947, 1968, 1975, 954, 962, 1988, 966, 1990, 1992, 1993, 1483, 974, 1999, 465, 978, 2004, 2005, 986, 475, 2017, 996, 997, 1509, 2023, 488, 1001, 1002, 493, 1007, 499, 1012, 1014, 1017, 506, 1022}\n",
      "[   3    5    6   13   20   22   27   29   30   35   37   44   46   53\n",
      "   57   61   71   75   78   81   82   84   85   86   88   90   91   97\n",
      "   99  102  114  117  123  124  128  131  135  136  143  145  149  151\n",
      "  156  159  164  165  177  188  211  221  222  223  227  230  231  236\n",
      "  242  247  248  250  251  254  259  261  269  272  284  286  298  299\n",
      "  302  308  309  314  319  320  322  328  329  333  345  348  354  359\n",
      "  365  367  369  370  373  374  377  380  384  385  396  400  403  409\n",
      "  424  427  438  442  446  447  454  459  463  464  474  477  480  485\n",
      "  490  503  504  524  535  537  541  545  546  548  565  566  567  578\n",
      "  582  583  594  602  605  609  610  611  612  614  620  621  622  625\n",
      "  627  631  632  633  639  641  648  651  656  662  663  667  682  685\n",
      "  688  691  692  696  700  701  710  712  713  715  718  719  732  735\n",
      "  738  741  747  759  762  764  766  782  788  792  799  802  804  806\n",
      "  812  818  820  834  842  857  860  881  883  888  889  890  892  898\n",
      "  900  916  919  924  929  945  953  957  960  964  980  982  984  992\n",
      " 1004 1011 1015 1019 1021 1024 1045 1050 1054 1058 1059 1063 1073 1074\n",
      " 1089 1107 1117 1124 1126 1132 1144 1145 1150 1151 1153 1159 1171 1172\n",
      " 1189 1193 1210 1215 1225 1228 1233 1235 1240 1241 1245 1255 1258 1261\n",
      " 1267 1274 1277 1281 1282 1283 1284 1285 1287 1288 1289 1294 1299 1305\n",
      " 1307 1308 1313 1314 1317 1325 1328 1337 1339 1343 1344 1348 1352 1359\n",
      " 1367 1369 1371 1375 1376 1377 1380 1383 1390 1393 1398 1400 1406 1412\n",
      " 1413 1418 1420 1430 1431 1437 1446 1447 1463 1465 1477 1493 1494 1495\n",
      " 1508 1511 1516 1518 1524 1535 1537 1546 1548 1552 1553 1558 1564 1568\n",
      " 1569 1580 1581 1582 1587 1598 1600 1603 1606 1633 1634 1636 1638 1648\n",
      " 1653 1654 1663 1667 1668 1671 1672 1682 1683 1696 1697 1706 1709 1710\n",
      " 1722 1724 1735 1741 1743 1749 1752 1755 1756 1764 1769 1774 1777 1786\n",
      " 1788 1792 1794 1801 1806 1807 1815 1817 1836 1842 1859 1863 1887 1900\n",
      " 1905 1925 1926 1928 1935 1936 1939 1951 1952 1955 1962 1963 1964 1974\n",
      " 2003 2008 2014 2015 2024 2025 2031 2033 2034 2035 2039 2044 2047 2053\n",
      " 2058 2061 2063 2071 2092 2094 2096 2097 2109 2114 2120 2122 2125 2127\n",
      " 2132 2136 2137 2142 2145 2148 2152 2153 2159 2162 2179 2185 2187 2193\n",
      " 2204 2210 2215 2220 2224 2225 2228 2229 2233 2243 2245 2246 2250 2252\n",
      " 2262 2263 2264 2266 2272 2273 2279 2281 2292 2296 2297 2300 2307 2308\n",
      " 2316 2324 2328 2334 2341 2349 2351 2354 2359 2362 2363]\n",
      "hyps cls is {1282, 1935, 22, 537, 286, 1951, 929, 1955, 298, 1582, 1663, 2097, 2359, 1343, 842, 477, 735, 2272, 1132, 365, 2033, 504, 377, 1535} and ref cls is {128, 385, 1282, 1285, 1413, 135, 1671, 1925, 1926, 2185, 396, 524, 1294, 145, 1172, 151, 1050, 1951, 1952, 2210, 35, 2341, 299, 1964, 1709, 691, 692, 565, 438, 1463, 1339, 700, 1215, 320, 1343, 2114, 1859, 2243, 582, 328, 1352, 842, 459, 1741, 1359, 81, 82, 1107, 1749, 2262, 2263, 88, 2136, 1756, 221, 2272, 1377, 354, 2145, 614, 1511, 1638, 2152, 2279, 1132, 2292, 1653, 374, 2039, 1400, 250, 890}\n",
      "hyps cls is {1537, 3, 662, 1314, 818, 319, 1600, 71, 713, 2250, 715, 1359, 2127, 1233, 85, 86, 2137, 2145, 1636, 485, 359, 1511, 622, 367, 117, 890, 1019} and ref cls is {156, 567}\n",
      "hyps cls is {2053, 6, 2058, 13, 2061, 1558, 2071, 30, 1054, 1569, 548, 44, 1074, 53, 57, 2109, 578, 1603, 2114, 582, 82, 2132, 602, 91, 605, 1117, 97, 612, 620, 621, 625, 1653, 1654, 631, 1145, 123, 1151, 639, 1667, 2179, 135, 2185, 145, 1682, 1171, 1683, 667, 159, 1696, 1189, 2215, 682, 1710, 692, 2228, 2229, 2233, 700, 2243, 1225, 2252, 718, 211, 1235, 2266, 2273, 741, 231, 1255, 1258, 236, 1261, 242, 1786, 1277, 1281, 1283, 2308, 261, 1285, 1807, 788, 792, 1817, 802, 812, 309, 1339, 322, 1352, 329, 345, 1369, 1375, 1380, 881, 370, 883, 1393, 373, 1398, 889, 384, 1925, 1418, 396, 1939, 1430, 1446, 1962, 1964, 1974, 1465, 442, 1477, 463, 464, 1495, 2025, 1004, 2031, 2034, 1524, 2039, 2047} and ref cls is {1089, 578, 1955, 2273, 1735, 1663, 464, 1777, 149, 22, 57, 1019, 799}\n",
      "hyps cls is {5, 1546, 524, 1552, 20, 1045, 535, 1050, 1568, 35, 1063, 1581, 46, 2096, 1073, 566, 567, 1598, 1089, 1606, 2122, 75, 2125, 78, 81, 594, 1107, 88, 2136, 2142, 1634, 1124, 102, 614, 114, 2162, 632, 633, 1144, 124, 128, 641, 1153, 1668, 1671, 136, 651, 2187, 1172, 149, 151, 156, 2204, 1697, 2210, 164, 1193, 685, 688, 177, 2224, 691, 1722, 1215, 2246, 712, 1741, 1743, 1749, 2263, 1756, 221, 1764, 747, 1774, 1777, 759, 247, 2296, 250, 251, 1274, 2297, 2300, 1794, 2307, 1287, 1289, 1801, 2316, 269, 1294, 1299, 2324, 1815, 284, 799, 1313, 804, 1317, 806, 299, 1325, 302, 2349, 1842, 1337, 320, 1344, 1348, 1863, 328, 860, 1376, 1377, 354, 1383, 1390, 369, 1905, 374, 1400, 380, 1926, 403, 409, 924, 1952, 424, 945, 438, 1463, 953, 957, 446, 964, 459, 1493, 982, 1494, 984, 2008, 474, 480, 992, 490, 1516, 1011, 503, 2044, 1021} and ref cls is {1537, 1801, 2316, 1806, 1807, 656, 1935, 403, 788, 1815, 924, 802, 804, 1325, 177, 818, 820, 2229, 696, 1337, 1722, 319, 1606, 583, 329, 594, 211, 1235, 85, 735, 359, 490, 1390, 1393, 1654, 762}\n",
      "hyps cls is {37, 1413, 583, 1638, 1735, 1706, 333, 1709, 656, 627, 820, 565, 2035, 696, 762, 2363, 2015} and ref cls is {3, 2053, 1546, 2058, 1558, 537, 548, 46, 2096, 1073, 1074, 2097, 566, 2047, 2109, 1600, 2125, 86, 2137, 1634, 1124, 1636, 620, 621, 622, 625, 114, 627, 117, 631, 632, 1144, 1145, 641, 2179, 651, 2187, 1682, 662, 663, 667, 1696, 1189, 682, 685, 688, 1724, 2246, 713, 2250, 715, 2252, 1233, 732, 741, 1774, 759, 2297, 251, 2300, 259, 261, 782, 1299, 284, 286, 1313, 1317, 298, 2349, 302, 1842, 2359, 314, 2363, 1863, 1380, 1383, 365, 367, 370, 888, 380, 409, 929, 953, 964, 1493, 1494, 1495, 2008, 474, 2014, 2015, 485, 1516, 504, 2044, 1021, 1535}\n",
      "hyps cls is {1024, 1548, 2063, 1553, 27, 1564, 29, 541, 545, 546, 1058, 1059, 1580, 2092, 2094, 1587, 61, 2120, 84, 90, 609, 610, 99, 611, 1633, 1126, 2148, 2152, 2153, 2159, 1648, 1150, 131, 1159, 648, 1672, 143, 2193, 663, 165, 2220, 2225, 1210, 188, 701, 1724, 2245, 710, 1228, 719, 2262, 1240, 1241, 1752, 1755, 732, 1245, 222, 223, 2264, 738, 227, 230, 2279, 1769, 2281, 1267, 2292, 248, 764, 1788, 766, 254, 1792, 259, 1284, 1288, 782, 1806, 272, 2328, 1305, 1307, 1308, 2334, 2341, 1836, 2351, 1328, 2354, 308, 314, 2362, 834, 1859, 1367, 857, 1371, 348, 1887, 1900, 888, 892, 1406, 385, 898, 900, 1412, 1928, 1420, 400, 1936, 916, 919, 1431, 1437, 1447, 427, 1963, 447, 960, 454, 2003, 980, 2014, 1508, 2024, 1518, 1015} and ref cls is {1024, 5, 6, 1548, 13, 2061, 2063, 1552, 1553, 20, 1045, 535, 2071, 27, 1564, 29, 30, 541, 1054, 545, 546, 1058, 1059, 37, 1568, 1063, 1569, 44, 1580, 1581, 1582, 2092, 2094, 1587, 53, 61, 1598, 1603, 71, 2120, 2122, 75, 78, 2127, 84, 2132, 90, 91, 602, 605, 1117, 2142, 97, 609, 611, 99, 610, 102, 612, 1126, 1633, 2148, 2153, 2159, 1648, 2162, 633, 123, 124, 1150, 639, 1151, 1153, 131, 1667, 1668, 1159, 136, 648, 1672, 143, 2193, 1171, 1683, 2204, 159, 1697, 164, 165, 2215, 1193, 1706, 2220, 1710, 2224, 2225, 2228, 2233, 1210, 188, 701, 2245, 710, 712, 1225, 1228, 718, 719, 1743, 1240, 1241, 1752, 1755, 2264, 1245, 222, 223, 2266, 738, 227, 1764, 230, 231, 1255, 1769, 1258, 747, 236, 1261, 2281, 242, 1267, 247, 248, 2296, 1274, 1786, 764, 1277, 254, 766, 1788, 1281, 1792, 1283, 1284, 1794, 2307, 1287, 1288, 1289, 2308, 269, 272, 2324, 792, 1305, 1817, 1307, 1308, 2328, 2334, 1314, 806, 812, 1836, 2351, 1328, 2354, 308, 309, 2362, 1344, 322, 834, 1348, 333, 1367, 345, 857, 1369, 348, 860, 1371, 1375, 1376, 1887, 1900, 369, 881, 883, 1905, 373, 1398, 377, 889, 892, 1406, 384, 898, 900, 1412, 1928, 1418, 1420, 400, 1936, 1939, 916, 1430, 919, 1431, 1437, 1446, 1447, 424, 1962, 427, 1963, 945, 1974, 1465, 442, 957, 446, 447, 960, 1477, 454, 463, 2003, 980, 982, 984, 477, 480, 992, 1508, 2024, 2025, 1004, 1518, 2031, 2033, 2034, 1011, 1015, 1524, 2035, 503}\n",
      "[   2    7    8   10   24   26   32   34   36   43   45   49   62   65\n",
      "   73   79   83   87   94  101  105  107  110  111  116  121  125  133\n",
      "  139  140  148  153  154  161  163  166  168  173  176  180  184  185\n",
      "  193  195  196  198  204  205  209  217  225  232  239  241  243  255\n",
      "  257  260  264  274  276  278  281  287  291  292  306  307  312  316\n",
      "  331  332  334  335  339  342  364  372  375  376  378  387  401  408\n",
      "  410  411  412  422  434  435  445  456  460  462  467  469  473  482\n",
      "  483  487  495  500  505  507  509  510  511  514  515  516  518  520\n",
      "  522  527  531  534  539  549  556  560  562  570  572  574  575  576\n",
      "  584  588  596  619  629  643  644  645  646  647  659  660  666  677\n",
      "  678  687  689  694  702  709  714  716  723  727  728  734  737  740\n",
      "  743  744  746  763  767  773  775  786  789  791  796  797  810  811\n",
      "  815  816  821  823  829  843  846  851  854  862  864  865  867  868\n",
      "  869  870  873  879  882  886  894  895  901  902  903  912  925  930\n",
      "  949  951  952  956  959  961  967  983  988  989  990  993 1009 1026\n",
      " 1029 1030 1032 1034 1037 1038 1042 1051 1076 1077 1078 1084 1094 1095\n",
      " 1098 1101 1104 1105 1120 1122 1128 1129 1133 1139 1152 1154 1164 1165\n",
      " 1173 1178 1182 1190 1191 1198 1200 1201 1204 1209 1212 1214 1216 1229\n",
      " 1239 1243 1246 1247 1250 1252 1259 1262 1264 1271 1278 1295 1303 1304\n",
      " 1315 1322 1323 1327 1330 1342 1351 1357 1370 1379 1381 1382 1388 1389\n",
      " 1396 1402 1403 1404 1405 1408 1411 1414 1422 1424 1428 1429 1433 1434\n",
      " 1438 1448 1453 1454 1458 1469 1472 1479 1486 1488 1492 1497 1503 1507\n",
      " 1512 1520 1522 1540 1544 1547 1551 1557 1563 1572 1579 1595 1596 1599\n",
      " 1604 1607 1609 1610 1612 1618 1621 1630 1637 1639 1641 1644 1646 1649\n",
      " 1652 1657 1658 1661 1664 1669 1675 1685 1700 1705 1725 1728 1729 1730\n",
      " 1732 1748 1759 1766 1768 1773 1776 1779 1783 1787 1799 1800 1803 1804\n",
      " 1805 1814 1821 1822 1827 1828 1830 1847 1848 1855 1870 1872 1875 1877\n",
      " 1880 1885 1886 1890 1894 1895 1897 1898 1899 1906 1915 1917 1922 1927\n",
      " 1937 1943 1946 1954 1965 1970 1971 1972 1973 1976 1977 1979 1981 2006\n",
      " 2007 2011 2018 2026 2030 2043 2048 2055 2057 2074 2075 2077 2079 2083\n",
      " 2084 2085 2095 2108 2128 2131 2134 2139 2141 2143 2149 2155 2161 2164\n",
      " 2167 2169 2170 2174 2180 2181 2182 2183 2194 2196 2211 2214 2222 2227\n",
      " 2238 2247 2254 2260 2267 2269 2299 2302 2306 2309 2313 2315 2319 2320\n",
      " 2326 2327 2330 2332 2335 2342 2347 2348 2361 2365 2366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyps cls is {1804, 1295, 2075, 1828, 678, 687, 307, 180, 1209, 185, 570, 2365, 2254, 467, 1875, 342, 983, 993, 1507, 487, 1128, 1389, 1264, 1139, 629, 2043, 2302, 255} and ref cls is {514, 1540, 1927, 264, 2057, 10, 139, 2315, 1551, 2320, 1042, 153, 1946, 2075, 796, 797, 287, 2335, 930, 1315, 1700, 1827, 1830, 2085, 2211, 1579, 556, 45, 1453, 2222, 560, 2347, 2348, 180, 821, 185, 1977, 572, 1084, 574, 1212, 1216, 1469, 1472, 1351, 456, 1098, 1486, 1105, 1748, 87, 983, 1497, 1880, 988, 1630, 2143, 1250, 1507, 1252, 2018, 107, 619, 1133, 495, 1264, 500, 1783, 505, 378, 2299, 511}\n",
      "hyps cls is {1414, 1173, 666, 26, 1830, 1965, 2222, 689, 823, 1977, 1979, 1469, 62, 576, 714, 723, 1621, 740, 1009, 882, 372, 886, 2170, 1915, 509} and ref cls is {1612, 1182}\n",
      "hyps cls is {2048, 1026, 515, 516, 1540, 2055, 1544, 522, 1034, 1037, 1038, 24, 2074, 36, 549, 2085, 2095, 1078, 1596, 2108, 1094, 1095, 1098, 1610, 1618, 2141, 1637, 105, 1641, 1649, 2164, 121, 125, 1664, 2180, 133, 2181, 647, 2182, 139, 1165, 659, 2196, 154, 1178, 1201, 2238, 1728, 198, 204, 209, 1239, 728, 217, 2267, 2269, 225, 1252, 232, 744, 1768, 241, 1779, 1787, 767, 257, 775, 2313, 2315, 2326, 1303, 796, 1822, 291, 292, 1827, 2348, 816, 1330, 821, 312, 2366, 331, 843, 846, 1870, 851, 1370, 1886, 1890, 1379, 868, 1382, 1897, 1906, 1396, 376, 1402, 378, 1408, 1922, 387, 902, 903, 1428, 1429, 1433, 410, 1434, 925, 1954, 422, 1453, 1454, 434, 435, 1970, 1971, 1973, 951, 952, 1981, 1472, 1488, 2007, 1497, 2011, 988, 1512, 2026, 2030, 1522} and ref cls is {576, 1954, 678, 1448, 2254, 401, 2007, 723, 791, 2169, 2170, 1915, 1246}\n",
      "hyps cls is {514, 518, 7, 8, 520, 10, 2057, 1551, 1042, 534, 539, 2079, 2083, 43, 556, 45, 1579, 49, 562, 1077, 1595, 1084, 1599, 1604, 1607, 584, 73, 1609, 588, 1612, 2128, 1105, 2131, 87, 1630, 2143, 1120, 1122, 619, 116, 2167, 2169, 1658, 644, 1675, 140, 1164, 660, 163, 1700, 2211, 1190, 168, 1204, 1212, 1725, 702, 1214, 193, 196, 1748, 2260, 727, 1243, 1246, 1247, 1759, 1250, 743, 746, 1259, 1773, 1776, 243, 763, 2306, 260, 2309, 1799, 264, 2320, 274, 789, 1814, 281, 2330, 2335, 1315, 2347, 306, 1847, 829, 1855, 1351, 334, 335, 1872, 1877, 854, 1880, 1381, 870, 873, 364, 375, 1403, 1917, 894, 901, 1927, 912, 401, 1943, 411, 1448, 949, 1976, 967, 456, 462, 495, 1520, 500, 505, 511} and ref cls is {518, 7, 1414, 1675, 1164, 1814, 666, 154, 539, 2077, 1965, 562, 1970, 823, 570, 2365, 1214, 2366, 1729, 967, 843, 588, 716, 2128, 2260, 1885, 734, 1247, 225, 743, 1259, 1389, 1773, 1779, 1658, 1917}\n",
      "hyps cls is {2183, 1032, 791, 408, 153, 412, 2077, 1182, 930, 1076, 1848, 572, 574, 1216, 961, 1729, 460, 716, 1486, 79, 1492, 989, 734, 1503, 1885, 482, 1766, 107, 1899, 1657, 510} and ref cls is {2048, 1030, 2055, 520, 1032, 534, 24, 26, 2074, 549, 2095, 1076, 1077, 1078, 62, 1607, 584, 73, 1609, 2134, 1120, 1128, 110, 2161, 1139, 116, 629, 2167, 1657, 121, 1661, 133, 2183, 660, 1173, 1178, 677, 168, 687, 689, 1725, 702, 714, 209, 740, 1766, 744, 1768, 1776, 2302, 255, 260, 1803, 1804, 274, 2330, 1828, 1322, 1323, 307, 1847, 1848, 829, 331, 846, 1870, 1872, 851, 1875, 1877, 342, 1370, 364, 882, 372, 886, 1922, 1429, 1943, 408, 410, 411, 412, 925, 422, 1454, 435, 1971, 949, 1976, 1979, 460, 1492, 993, 487, 2026, 1009, 2043, 509, 510}\n",
      "hyps cls is {2, 1029, 1030, 1547, 527, 531, 1557, 1051, 1563, 32, 34, 1572, 2084, 560, 575, 65, 1101, 1104, 83, 596, 2134, 2139, 94, 101, 2149, 1639, 1129, 2155, 1644, 1133, 110, 111, 1646, 2161, 1652, 1661, 2174, 1152, 1154, 643, 645, 646, 1669, 2194, 148, 1685, 161, 677, 166, 1191, 2214, 1705, 173, 1198, 176, 1200, 2227, 694, 184, 1730, 195, 1732, 709, 2247, 205, 1229, 737, 1262, 239, 1271, 1783, 2299, 1278, 773, 1800, 1803, 1805, 2319, 786, 276, 278, 2327, 1304, 2332, 797, 1821, 287, 2342, 810, 811, 1322, 1323, 815, 1327, 2361, 316, 1342, 332, 1357, 339, 862, 864, 865, 867, 869, 1894, 1895, 1898, 1388, 879, 1404, 1405, 895, 1411, 1422, 1424, 1937, 1946, 1438, 1458, 1972, 956, 445, 959, 1479, 469, 2006, 473, 990, 2018, 483, 507} and ref cls is {2, 515, 516, 1026, 1029, 8, 1544, 522, 1034, 1547, 1037, 1038, 527, 531, 1557, 1051, 1563, 2079, 32, 34, 2083, 36, 1572, 2084, 43, 49, 1595, 1596, 2108, 575, 1599, 65, 1604, 1094, 1095, 1610, 1101, 79, 1104, 1618, 83, 596, 1621, 2131, 2139, 2141, 94, 1122, 101, 1637, 1639, 2149, 105, 1129, 1641, 1644, 2155, 1646, 111, 1649, 1652, 2164, 125, 2174, 1152, 1664, 1154, 643, 644, 645, 646, 647, 1669, 2180, 2181, 2182, 140, 1165, 2194, 659, 148, 1685, 2196, 161, 163, 166, 1190, 1191, 1705, 2214, 173, 1198, 176, 1200, 1201, 2227, 1204, 694, 184, 1209, 2238, 1728, 193, 1730, 195, 196, 709, 198, 1732, 2247, 204, 205, 1229, 727, 728, 217, 1239, 1243, 2267, 2269, 1759, 737, 232, 746, 1262, 239, 241, 243, 1271, 763, 1787, 1278, 767, 257, 2306, 773, 2309, 775, 1799, 1800, 2313, 1805, 1295, 2319, 786, 276, 789, 278, 1303, 1304, 281, 2326, 2327, 2332, 1821, 1822, 291, 292, 2342, 810, 811, 815, 816, 1327, 306, 1330, 312, 2361, 316, 1342, 1855, 332, 1357, 334, 335, 339, 854, 862, 1886, 864, 865, 1890, 867, 868, 869, 870, 1379, 1381, 873, 1382, 1894, 1388, 1895, 1897, 879, 1898, 1899, 1906, 1396, 375, 376, 1402, 1403, 1404, 1405, 894, 895, 1408, 387, 1411, 901, 902, 903, 1422, 912, 1424, 1937, 1428, 1433, 1434, 1438, 434, 1458, 1972, 1973, 951, 952, 956, 445, 1981, 959, 961, 1479, 462, 1488, 467, 469, 2006, 473, 2011, 989, 990, 1503, 482, 483, 1512, 2030, 1520, 1522, 507}\n",
      "lesk precision: 0.2547666666666667\n",
      "lesk recall: 0.22100000000000003\n",
      "lesk f_measure: 0.2547666666666667\n",
      "lesk accuracy: 0.29700000000000004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from nltk.metrics.scores import precision, recall, f_measure, accuracy\n",
    "from nltk.corpus import senseval\n",
    "from nltk.util import ngrams\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "import nltk\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet_ic')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "NGRAM_WINDOW = 3\n",
    "\n",
    "mapping = {\n",
    "    'interest_1': 'interest.n.01',\n",
    "    'interest_2': 'interest.n.03',\n",
    "    'interest_3': 'pastime.n.01',\n",
    "    'interest_4': 'sake.n.01',\n",
    "    'interest_5': 'interest.n.05',\n",
    "    'interest_6': 'interest.n.04',\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    mapping = {\"NOUN\": wordnet.NOUN, \"VERB\": wordnet.VERB, \"ADJ\": wordnet.ADJ, \"ADV\": wordnet.ADV}\n",
    "    sw_list = stopwords.words('english')\n",
    "    \n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    # tokenize, if input is text\n",
    "    tokens = nltk.word_tokenize(text) if type(text) is str else text\n",
    "    # pos-tag\n",
    "    tagged = nltk.pos_tag(tokens, tagset=\"universal\")\n",
    "    # lowercase\n",
    "    tagged = [(w.lower(), p) for w, p in tagged]\n",
    "    # optional: remove all words that are not NOUN, VERB, ADJ, or ADV (i.e. no sense in WordNet)\n",
    "    tagged = [(w, p) for w, p in tagged if p in mapping]\n",
    "    # re-map tags to WordNet (return orignal if not in-mapping, if above is not used)\n",
    "    tagged = [(w, mapping.get(p, p)) for w, p in tagged]\n",
    "    # remove stopwords\n",
    "    tagged = [(w, p) for w, p in tagged if w not in sw_list]\n",
    "    # lemmatize\n",
    "    tagged = [(w, lem.lemmatize(w, pos=p), p) for w, p in tagged]\n",
    "    # unique the list\n",
    "    tagged = list(set(tagged))\n",
    "    return tagged\n",
    "\n",
    "def get_sense_definitions(context):\n",
    "    # input is text or list of strings\n",
    "    lemma_tags = preprocess(context)\n",
    "    # let's get senses for each\n",
    "    senses = [(w, wordnet.synsets(l, p)) for w, l, p in lemma_tags]\n",
    "    \n",
    "    # let's get their definitions\n",
    "    definitions = []\n",
    "    for raw_word, sense_list in senses:\n",
    "        if len(sense_list) > 0:\n",
    "            # let's tokenize, lowercase & remove stop words \n",
    "            def_list = []\n",
    "            for s in sense_list:\n",
    "                defn = s.definition()\n",
    "                # let's use the same preprocessing\n",
    "                tags = preprocess(defn)\n",
    "                toks = [l for w, l, p in tags]\n",
    "                def_list.append((s, toks))\n",
    "            definitions.append((raw_word, def_list))\n",
    "    return definitions\n",
    "\n",
    "\n",
    "def get_top_sense(words, sense_list):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    val, sense = max((len(set(words).intersection(set(defn))), ss) for ss, defn in sense_list)\n",
    "    return val, sense\n",
    "'''\n",
    "def lesk_simplified(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "    context = set(context_sentence)\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = wordnet.synsets(ambiguous_word)\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    # Measure the overlap between context and definitions\n",
    "    _, sense = max(\n",
    "        (len(context.intersection(ss.definition().split())), ss) for ss in synsets\n",
    "    )\n",
    "\n",
    "    return sense\n",
    "'''\n",
    "\n",
    "def original_lesk(context_sentence, ambiguous_word, pos=None, synsets=None, majority=False):\n",
    "\n",
    "    context_senses = get_sense_definitions(set(context_sentence)-set([ambiguous_word]))\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    scores = []\n",
    "    # print(synsets)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense(sense[1], synsets))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        # We remove 0 scores senses without overlapping\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    return best_sense\n",
    "\n",
    "##GRAPH BASED\n",
    "def get_top_sense_sim(context_sense, sense_list, similarity):\n",
    "    # get top sense from the list of sense-definition tuples\n",
    "    # assumes that words and definitions are preprocessed identically\n",
    "    scores = []\n",
    "    for sense in sense_list:\n",
    "        ss = sense[0]\n",
    "        if similarity == \"path\":\n",
    "            try:\n",
    "                scores.append((context_sense.path_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))    \n",
    "        elif similarity == \"lch\":\n",
    "            try:\n",
    "                scores.append((context_sense.lch_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"wup\":\n",
    "            try:\n",
    "                scores.append((context_sense.wup_similarity(ss), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"resnik\":\n",
    "            try:\n",
    "                scores.append((context_sense.res_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"lin\":\n",
    "            try:\n",
    "                scores.append((context_sense.lin_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        elif similarity == \"jiang\":\n",
    "            try:\n",
    "                scores.append((context_sense.jcn_similarity(ss, semcor_ic), ss))\n",
    "            except:\n",
    "                scores.append((0, ss))\n",
    "        else:\n",
    "            print(\"Similarity metric not found\")\n",
    "            return None\n",
    "    val, sense = max(scores)\n",
    "    return val, sense\n",
    "\n",
    "def lesk_similarity(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, majority=True):\n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    # Here you may have some room for improvement\n",
    "    # For instance instead of using all the definitions from the context\n",
    "    # you pick the most common one of each word (i.e. the first)\n",
    "    for senses in context_senses:\n",
    "        for sense in senses[1]:\n",
    "            scores.append(get_top_sense_sim(sense[0], synsets, similarity))\n",
    "            \n",
    "    if len(scores) == 0:\n",
    "        return synsets[0][0]\n",
    "    \n",
    "    if majority:\n",
    "        filtered_scores = [x[1] for x in scores if x[0] != 0]\n",
    "        if len(filtered_scores) > 0:\n",
    "            best_sense = Counter(filtered_scores).most_common(1)[0][0]\n",
    "        else:\n",
    "            # Almost random selection\n",
    "            best_sense = Counter([x[1] for x in scores]).most_common(1)[0][0]\n",
    "    else:\n",
    "        _, best_sense = max(scores)\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def pedersen(context_sentence, ambiguous_word, similarity=\"resnik\", pos=None, \n",
    "                    synsets=None, threshold=0.1):\n",
    "                        \n",
    "                        \n",
    "    context_senses = get_sense_definitions(set(context_sentence) - set([ambiguous_word]))\n",
    "\n",
    "    if synsets is None:\n",
    "        synsets = get_sense_definitions(ambiguous_word)[0][1]\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss[0].pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "    \n",
    "    synsets_scores = {}\n",
    "    for ss_tup in synsets:\n",
    "        ss = ss_tup[0]\n",
    "        if ss not in synsets_scores:\n",
    "            synsets_scores[ss] = 0\n",
    "        for senses in context_senses:\n",
    "            scores = []\n",
    "            for sense in senses[1]:\n",
    "                if similarity == \"path\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].path_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))    \n",
    "                elif similarity == \"lch\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].lch_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"wup\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].wup_similarity(ss), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"resnik\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].res_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"lin\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].lin_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                elif similarity == \"jiang\":\n",
    "                    try:\n",
    "                        scores.append((sense[0].jcn_similarity(ss, semcor_ic), ss))\n",
    "                    except:\n",
    "                        scores.append((0, ss))\n",
    "                else:\n",
    "                    print(\"Similarity metric not found\")\n",
    "                    return None\n",
    "            value, sense = max(scores)\n",
    "            if value > threshold:\n",
    "                synsets_scores[sense] = synsets_scores[sense] + value\n",
    "    \n",
    "    values = list(synsets_scores.values())\n",
    "    senses = list(synsets_scores.keys())\n",
    "    best_sense_id = values.index(max(values))\n",
    "    return senses[best_sense_id]\n",
    "\n",
    "def collocational_features(inst, ngram_window=NGRAM_WINDOW):\n",
    "    p = inst.position\n",
    "    feats_dict = {\n",
    "        \"w-2_word\": 'NULL' if p < 2 else inst.context[p-2][0],\n",
    "        \"w-1_word\": 'NULL' if p < 1 else inst.context[p-1][0],\n",
    "        \"w+1_word\": 'NULL' if len(inst.context) - 1 < p+1 else inst.context[p+1][0],\n",
    "        \"w+2_word\": 'NULL' if len(inst.context) - 1 < p+2 else inst.context[p+2][0],\n",
    "        \"POS-tags\": inst.context[p][1],\n",
    "    }\n",
    "    #Computing raw string \n",
    "    sent_before = [inst.context[p-i-1][0] for i in reversed(range(ngram_window-1)) if p>i+1]\n",
    "    sent_after = [inst.context[p+i+1][0] for i in (range(ngram_window-1)) if len(inst.context) - 1 > p+1]\n",
    "    word = [inst.context[p][0]]\n",
    "    sent_for_ngrams = ' '.join(sent_before+word+sent_after)\n",
    "    add_dict = {}\n",
    "    \n",
    "    #Computing ngrams from raw string \n",
    "    for i in range(ngram_window):\n",
    "        values = []\n",
    "        key_name = str(i+1)+'-gram'\n",
    "        value_with_tuples = ngrams(nltk.word_tokenize(sent_for_ngrams), i+1)\n",
    "        for item in value_with_tuples:\n",
    "            value_str = ' '.join(item)\n",
    "            values.append(value_str)\n",
    "        add_dict.update({key_name: values})\n",
    "\n",
    "    #Updating the features dict with the ngram dictionary\n",
    "    feats_dict.update(add_dict)\n",
    "    return feats_dict\n",
    "      \n",
    "    \n",
    "\n",
    "data = [\" \".join([t[0] for t in inst.context]) for inst in senseval.instances('interest.pos')]\n",
    "lbls = [inst.senses[0] for inst in senseval.instances('interest.pos')]\n",
    "\n",
    "#Supervised approach with BOW to solve word-sense disambiguation  \n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "classifier = MultinomialNB()\n",
    "lblencoder = LabelEncoder()\n",
    "\n",
    "stratified_split = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "vectors = vectorizer.fit_transform(data)\n",
    "\n",
    "# encoding labels for multi-calss\n",
    "lblencoder.fit(lbls)\n",
    "labels = lblencoder.transform(lbls)\n",
    "\n",
    "\n",
    "#Supervised approach using dictionary of features to solve word-sense disambiguation\n",
    "\n",
    "data_col = [collocational_features(inst, NGRAM_WINDOW) for inst in senseval.instances('interest.pos')]\n",
    "dvectorizer = DictVectorizer(sparse=False)\n",
    "dvectors = dvectorizer.fit_transform(data_col)\n",
    "\n",
    "concatenated_vectors = np.concatenate((vectors.toarray(), dvectors), axis=1)\n",
    "\n",
    "scores = cross_validate(classifier, concatenated_vectors, labels, cv=stratified_split, scoring=['f1_micro'])\n",
    "\n",
    "print(sum(scores['test_f1_micro'])/len(scores['test_f1_micro']))\n",
    "print('')\n",
    "\n",
    "\n",
    "    \n",
    "# Evaluate lesk and lesk graph on same split\n",
    "\n",
    "\n",
    "def run_experiment(data, lbls, stratified_split, mapping, synsets, method = 'pedersen'):\n",
    "    exp_scores = {}\n",
    "    exp_scores['precision'] = []\n",
    "    exp_scores['accuracy'] = []\n",
    "    exp_scores['recall'] = []\n",
    "    exp_scores['f_measure'] = []\n",
    "    \n",
    "    for train_index, test_index in stratified_split.split(data, lbls):\n",
    "        print(test_index)\n",
    "        refs, hyps, refs_list, hyps_list = get_hyps(test_index, data, lbls, mapping, synsets, method)\n",
    "        \n",
    "        acc = round(accuracy(refs_list, hyps_list), 3)\n",
    "        exp_scores['accuracy'].append(acc)\n",
    "        for cls in hyps.keys():\n",
    "            if refs[cls] == set():\n",
    "                refs[cls].add(-1)\n",
    "            if hyps[cls] == set():\n",
    "                hyps[cls].add(-1)\n",
    "            p= round(precision(refs[cls], hyps[cls]),3)\n",
    "            r = round(recall(refs[cls], hyps[cls]),3)\n",
    "            f = round(f_measure(refs[cls], hyps[cls], alpha=1),3)\n",
    "            \n",
    "            exp_scores['precision'].append(p)\n",
    "            exp_scores['recall'].append(r)\n",
    "            exp_scores['f_measure'].append(f)\n",
    "        \n",
    "    print(f\"{method} precision: {sum(exp_scores['precision'])/len(exp_scores['precision'])}\")\n",
    "    print(f\"{method} recall: {sum(exp_scores['recall'])/len(exp_scores['recall'])}\")\n",
    "    print(f\"{method} f_measure: {sum(exp_scores['f_measure'])/len(exp_scores['f_measure'])}\")\n",
    "    print(f\"{method} accuracy: {sum(exp_scores['accuracy'])/len(exp_scores['accuracy'])}\")\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "\n",
    "def get_hyps(test_index, data, lbls, mapping ,synsets, method):\n",
    "    refs = {k: set() for k in mapping.values()}\n",
    "    hyps = {k: set() for k in mapping.values()}\n",
    "    refs_list = []\n",
    "    hyps_list = []\n",
    "    for index in test_index:\n",
    "        if method == 'pedersen':\n",
    "            hyp = pedersen(data[index].split(), 'interest', similarity='path',synsets = synsets).name()\n",
    "        elif method == 'lesk':\n",
    "            hyp = original_lesk(data[index].split(), 'interest',synsets = synsets, majority =True).name()\n",
    "        else:\n",
    "            print('specify another method, options: [pedersen, lesk]')\n",
    "\n",
    "        ref = mapping[lbls[index]]\n",
    "\n",
    "        # for precision, recall, f-measure        \n",
    "        refs[ref].add(index)\n",
    "        hyps[hyp].add(index)\n",
    "\n",
    "        # for accuracy\n",
    "        refs_list.append(ref)\n",
    "        hyps_list.append(hyp)\n",
    "    \n",
    "    return refs, hyps, refs_list, hyps_list\n",
    "  \n",
    "\n",
    "\n",
    "# since WordNet defines more senses, let's restrict predictions\n",
    "synsets = []\n",
    "for ss in wordnet.synsets('interest', pos='n'):\n",
    "    if ss.name() in mapping.values():\n",
    "        defn = ss.definition()\n",
    "        tags = preprocess(defn)\n",
    "        toks = [l for w, l, p in tags]\n",
    "        synsets.append((ss,toks))\n",
    "\n",
    "run_experiment(data, lbls, stratified_split, mapping, synsets, 'pedersen')\n",
    "run_experiment(data, lbls, stratified_split, mapping, synsets, 'lesk')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
