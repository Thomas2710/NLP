{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Corpus and Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "- Understanding: \n",
    "    - relation between corpus and lexicon\n",
    "    - effects of pre-processing (tokenization) on lexicon\n",
    "    \n",
    "- Learning how to:\n",
    "    - load basic corpora for processing\n",
    "    - compute basic descriptive statistic of a corpus\n",
    "    - building lexicon and frequency lists from a corpus\n",
    "    - perform basic lexicon operations\n",
    "    - perform basic text pre-processing (tokenization and sentence segmentation) using python libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading\n",
    "- Dan Jurafsky and James H. Martin. [__Speech and Language Processing__ (SLP)](https://web.stanford.edu/~jurafsky/slp3/) (3rd ed. draft)\n",
    "- Steven Bird, Ewan Klein, and Edward Loper. [__Natural Language Processing with Python__ (NLTK)](https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Covered Material\n",
    "- SLP\n",
    "    - [Chapter 2: Regular Expressions, Text Normalization, Edit Distance](https://web.stanford.edu/~jurafsky/slp3/2.pdf) \n",
    "- NLTK \n",
    "    - [Chapter 2: Accessing Text Corpora and Lexical Resources](https://www.nltk.org/book/ch02.html)\n",
    "    - [Chapter 3: Processing Raw Text](https://www.nltk.org/book/ch03.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Requirements\n",
    "\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - run `pip install nltk`\n",
    "    \n",
    "- [spaCy](https://spacy.io/)\n",
    "    - run `pip install spacy`\n",
    "    - run `python -m spacy download en_core_web_sm` to install English language model (`spacy>=3.0`)\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "    - run `pip install scikit-learn`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Corpora and Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.1. Corpus\n",
    "\n",
    "[Corpus](https://en.wikipedia.org/wiki/Text_corpus) is a collection of written or spoken texts that is used for language research. Before doing anything with a corpus we need to know its properties:\n",
    "\n",
    "__Corpus Properties__:\n",
    "- *Format* -- how to read/load it?\n",
    "- *Language* -- which tools/models can I use?\n",
    "- *Annotation* -- what it is intended for?\n",
    "- *Split* for __Evaluation__: (terminology varies from source to source)\n",
    "\n",
    "| Set         | Purpose                                       |\n",
    "|:------------|:----------------------------------------------|\n",
    "| Training    | training model, extracting rules, etc.        |\n",
    "| Development | tuning, optimization, intermediate evaluation |\n",
    "| Test        | final evaluation (remains unseen)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. Text Corpora in NLTK\n",
    "NLTK provides several corpora with loading functions. Plain text corpora come from a _Project Gutenberg_.\n",
    "\n",
    "`nltk.corpus.gutenberg.fileids()` lists available books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Units of Text Corpus\n",
    "Depending on a goal, corpus can be seen as a sequence of:\n",
    "- characters\n",
    "- words (tokens)\n",
    "- sentences\n",
    "- paragraphs\n",
    "- document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level, in turn, can be seen as a sequence of elements of the previous level.\n",
    "\n",
    "- word -- a sequence of characters\n",
    "- sentence -- a sequence of words\n",
    "- paragraph -- a sequence of sentences\n",
    "- document -- a sequence of paragraphs (or sentences, depending on our purpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Loading NLTK Corpora\n",
    "\n",
    "NLTK provides functions to load a corpus using these different levels, as `raw` (characters), `words`, and `sentences`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: [\n",
      "words: [\n",
      "sents: ['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']']\n"
     ]
    }
   ],
   "source": [
    "alice_chars = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "print('chars:', alice_chars[0])\n",
    "alice_words = nltk.corpus.gutenberg.words('carroll-alice.txt')\n",
    "print('words:', alice_words[0])\n",
    "alice_sents = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "print('sents:', alice_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* can be described in terms of:\n",
    "\n",
    "- total number of characters\n",
    "- total number of words (_tokens_: includes punctuation, etc.)\n",
    "- total number of sentences\n",
    "\n",
    "- minimum/maximum/average number of character per token\n",
    "- minimum/maximum/average number of words per sentence\n",
    "- minimum/maximum/average number of sentences per document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example__\n",
    "\n",
    "$$\\text{Av. Token Count} = \\frac{\\text{count}(tokens)}{\\text{count}(sentences)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's compute average sentence length & round to closes integer\n",
    "round(len(alice_words)/len(alice_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVG sent len 20\n",
      "MIN sent len 2\n",
      "MAX sent len 204\n"
     ]
    }
   ],
   "source": [
    "# let's compute length of each sentence\n",
    "sent_lens = [len(sent) for sent in alice_sents]\n",
    "# let's compute length of each word\n",
    "word_lens = [len(word) for word in alice_words]\n",
    "# let's compute length the number of characters in each sentence\n",
    "chars_lens = [len(''.join(sent)) for sent in alice_sents]\n",
    "\n",
    "avg_sent_len = round(sum(sent_lens)/len(sent_lens))\n",
    "min_sent_len = min(sent_lens)\n",
    "max_sent_len = max(sent_lens)\n",
    "print(\"AVG sent len\", avg_sent_len)\n",
    "print(\"MIN sent len\", min_sent_len)\n",
    "print(\"MAX sent len\", max_sent_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "H⭐e⭐l⭐l⭐o\n"
     ]
    }
   ],
   "source": [
    "# JOIN built-in function example\n",
    "tmp = ['H', 'e', 'l', 'l', 'o']\n",
    "print(''.join(tmp))\n",
    "print('⭐'.join(tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- Define a function to compute corpus descriptive statistics\n",
    "\n",
    "    - input:\n",
    "        - raw text (Chars)\n",
    "        - words\n",
    "        - sentences\n",
    "    - output (print): \n",
    "        - average number of:\n",
    "            - chars per word\n",
    "            - words per sentence\n",
    "            - chars per sentence\n",
    "        - Size of the longest word and sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word per sentence 20\n",
      "Char per word 3\n",
      "Char per sentence 68\n",
      "Longest sentence 204\n",
      "Longest word 14\n"
     ]
    }
   ],
   "source": [
    "def statistics(chars, words, sents):\n",
    "    word_lens = [len(word) for word in alice_words]\n",
    "    sent_lens = [len(sentence) for sentence in alice_sents]\n",
    "    chars_in_sents = [len(''.join(sent)) for sent in alice_sents]\n",
    "    \n",
    "    word_per_sent = round(sum(sent_lens) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sent_lens)\n",
    "    longest_word = max(word_lens)\n",
    "    \n",
    "    return word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word\n",
    "\n",
    "word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word = statistics(alice_chars, alice_words, alice_sents)\n",
    "\n",
    "print('Word per sentence', word_per_sent)\n",
    "print('Char per word', char_per_word )\n",
    "print('Char per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sent)\n",
    "print('Longest word', longest_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Lexicon\n",
    "\n",
    "[Lexicon](https://en.wikipedia.org/wiki/Lexicon) is the *vocabulary* of a language. In linguistics, a lexicon is a language's inventory of lexemes.\n",
    "\n",
    "Linguistic theories generally regard human languages as consisting of two parts: a lexicon, essentially a catalog of a language's words; and a grammar, a system of rules which allow for the combination of those words into meaningful sentences. \n",
    "\n",
    "*Lexicon (or Vocabulary) Size* is one of the statistics reported for corpora. While *Word Count* is the number of __tokens__, *Lexicon Size* is the number of __types__ (unique words).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1. Lexicon and Its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Constructing Lexicon and Computing its Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since lexicon is a list of unique elemets, it is a `set` of corpus words (i.e. tokens).\n",
    "Consequently, its size is the size of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3016"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set(alice_words)\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "We did not process our corpus in any way. Consequently, words with case variations are different entries in our lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Lowercased Lexicon\n",
    "Let's lowercase our corpus and re-compute the lexicon size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2636"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lexicon = set([w.lower() for w in alice_words])\n",
    "len(alice_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print('ALL' in alice_lexicon)\n",
    "print('All' in alice_lexicon)\n",
    "print('all' in alice_lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.2. Frequency List\n",
    "\n",
    "In Natural Language Processing (NLP), [a frequency list](https://en.wikipedia.org/wiki/Word_lists_by_frequency) is a sorted list of words (word types) together with their frequency, where frequency here usually means the number of occurrences in a given corpus, from which the rank can be derived as the position in the list.\n",
    "\n",
    "What is a \"word\"?\n",
    "\n",
    "- case sensitive counts\n",
    "- case insensitive counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Computing Frequency List with python\n",
    "\n",
    "In python, frequency list can be constructed in several ways. The most convenient is the `Counter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "alice_freq_list = Counter(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_list.get('ALL', 0))\n",
    "print(alice_freq_list.get('All', 0))\n",
    "print(alice_freq_list.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Computing Frequency List with NLTK\n",
    "NLTK provides `FreqDist` class to construct a Frequency List (`FreqDist` == _Frequency Distribution_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_freq_dist = nltk.FreqDist(alice_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "173\n"
     ]
    }
   ],
   "source": [
    "print(alice_freq_dist.get('ALL', 0))\n",
    "print(alice_freq_dist.get('All', 0))\n",
    "print(alice_freq_dist.get('all', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "- compute frequency list of __lowercased__ \"alice\" corpus (you can use either method)\n",
    "- report `5` most frequent words (use can use provided `nbest` function to get a dict of top N items)\n",
    "- compare the frequencies to the reference values below\n",
    "\n",
    "| Word   | Frequency |\n",
    "|--------|----------:|\n",
    "| ,      |     1,993 |\n",
    "| '      |     1,731 |\n",
    "| the    |     1,642 |\n",
    "| and    |       872 |\n",
    "| .      |       764 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',': 1993, \"'\": 1731, 'the': 1642, 'and': 872, '.': 764}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_lowercase_freq_list =  Counter([w.lower() for w in alice_words]) # Replace X with the word list of the corpus in lower case (see above))\n",
    "nbest(alice_lowercase_freq_list, n=5) # Change N form 1 to 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3. Lexicon Operations\n",
    "\n",
    "It is common to process the lexicon according to the task at hand (not every transformation makes sense for all tasks). The common operations are removing words by frequency (minimum or maximum, i.e. *Frequency Cut-Off*) and removing words for a specific lists (i.e. *Stop Word Removal*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 2.3.1. Frequency Cut-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "\n",
    "<!-- - define a function to compute a lexicon from a frequency list applying minimum and maximum frequency cut-offs\n",
    "    \n",
    "    - input: frequence list (dict)\n",
    "    - output: list\n",
    "    - use default values for min and max\n",
    "     -->\n",
    "- Using the function cut_off\n",
    "    \n",
    "    - compute lexicon applying:\n",
    "    \n",
    "        - minimum cut-off 2 (remove words that appear less than 2 times, i.e. remove [hapax legomena](https://en.wikipedia.org/wiki/Hapax_legomenon))\n",
    "        - maximum cut-off 100 (remove words that appear more that 100 times)\n",
    "        - both minimum and maximum thresholds together\n",
    "        \n",
    "    - report size for each comparing to the reference values in the table (on the lowercased lexicon)\n",
    "\n",
    "| Operation  | Min | Max | Size |\n",
    "|------------|----:|----:|-----:|\n",
    "| original   | N/A | N/A | 2636 |\n",
    "| cut-off    |   2 | N/A | 1503 |\n",
    "| cut-off    | N/A | 100 | 2586 |\n",
    "| cut-off    |   2 | 100 | 1453 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "CutOFF Min: 2.0 MAX: 100.0  Lexicon Size: 1453\n"
     ]
    }
   ],
   "source": [
    "def cut_off(vocab, n_min=100, n_max=100):\n",
    "    new_vocab = []\n",
    "    for word, count in vocab.items():\n",
    "        if count >= n_min and count <= n_max:\n",
    "            new_vocab.append(word)\n",
    "    return new_vocab\n",
    "\n",
    "lower_bound = float(\"2\") # Change these two number to compute the required cut offs\n",
    "upper_bound = float(\"100\")\n",
    "lexicon_cut_off = len(cut_off(alice_lowercase_freq_list, n_min=lower_bound, n_max=upper_bound))\n",
    "\n",
    "print('Original', len(alice_lowercase_freq_list))\n",
    "print('CutOFF Min:', lower_bound, 'MAX:', upper_bound, ' Lexicon Size:', lexicon_cut_off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. StopWord Removal\n",
    "\n",
    "In computing, [stop words](https://en.wikipedia.org/wiki/Stop_words) are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search.\n",
    "\n",
    "Any group of words can be chosen as the stop words for a given purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's check the stop word lists from the popular python libraries.\n",
    "\n",
    "- spaCy\n",
    "- NLTK\n",
    "- scikit-learn\n",
    "\n",
    "    \n",
    "For NLTK we need to download them first\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy: 326\n",
      "NLTK: 179\n",
      "sklearn: 318\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as SKLEARN_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "NLTK_STOP_WORDS = set(stopwords.words('english'))\n",
    "\n",
    "print('spaCy: {}'.format(len(SPACY_STOP_WORDS)))\n",
    "print('NLTK: {}'.format(len(NLTK_STOP_WORDS)))\n",
    "print('sklearn: {}'.format(len(SKLEARN_STOP_WORDS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Exercise\n",
    "- using Python's built it `set` [methods](https://docs.python.org/2/library/stdtypes.html#set):\n",
    "    - compute the intersection between the 100 most frequent words in frequency list of the alice corpus and the list of stopwords (report count)\n",
    "    - remove stopwords from the lexicon\n",
    "    - print the size of:\n",
    "            - original lexicon\n",
    "            - lexicon without stopwords\n",
    "            - overlap between 100 most freq. words and stopwords\n",
    "\n",
    "| Operation       | Size |\n",
    "|-----------------|-----:|\n",
    "| original        | 2636 |\n",
    "| no stop words   | 2490 |\n",
    "| top 100 overlap |   65 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 'b'}\n",
      "{'e', 'd', 'c'}\n"
     ]
    }
   ],
   "source": [
    "# Set built-in Function\n",
    "set_a = set(['a', 'b', 'c', 'd', 'e'])\n",
    "set_b = set(['a', 'b', 'f'])\n",
    "\n",
    "print(set_a.intersection(set_b)) # Compute overlap\n",
    "print(set_a.difference(set_b)) # Remove Elements by computing the set diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 2636\n",
      "No stopwords 2490\n",
      "Top100 overlap 65\n"
     ]
    }
   ],
   "source": [
    "alice_vocab = set([w.lower() for w in alice_words])\n",
    "top100 = list(nbest(alice_lowercase_freq_list,n=100).keys())\n",
    "stop_words = NLTK_STOP_WORDS\n",
    "overlap = set(top100).intersection(stop_words)# Compute the intersation between top100 and stop_words\n",
    "alice_vocab_no_stopwords = alice_vocab.difference(stop_words)\n",
    "print('Original', len(alice_vocab))\n",
    "print('No stopwords', len(alice_vocab_no_stopwords))\n",
    "print('Top100 overlap', len(overlap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic Text Pre-processing\n",
    "\n",
    "Both frequency cut-off and stop word removal are frequently used text pre-processing steps. Depending on the application, there are several other common text pre-processing steps that are usually applied for tranforming text for Machine Learning tasks.\n",
    "\n",
    "__Text Normalization Steps__\n",
    "\n",
    "- removing extra white spaces\n",
    "\n",
    "- tokenization\n",
    "    - documents to sentences (sentence segmentation/tokenization)\n",
    "    - sentences to tokens\n",
    "\n",
    "- lowercasing/uppercasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- removing punctuation\n",
    "\n",
    "- removing accent marks and other diacritics \n",
    "\n",
    "- removing stop words (see above)\n",
    "\n",
    "- removing sparse terms (frequency cut-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- number normalization\n",
    "    - numbers to words (i.e. `10` to `ten`)\n",
    "    - number words to numbers (i.e. `ten` to `10`)\n",
    "    - removing numbers\n",
    "\n",
    "- verbalization (specifically for speech applications)\n",
    "\n",
    "    - numbers to words\n",
    "    - expanding abbreviations (or spelling out)\n",
    "    - reading out dates, etc.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation)\n",
    "    - the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "- [stemming](https://en.wikipedia.org/wiki/Stemming)\n",
    "    - the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Tokenization and Sentence Segmentation\n",
    "\n",
    "Given a \"clean\" text, in order to allow any analysis, we need to identify its units.\n",
    "In other words, we need to _segment_ the text into sentences and words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NOTE__:\n",
    "Since both _tokenization_ and _sentence segmentation_ are automatic, different tools yield different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1. Tokenization and Sentence Segmentation with spaCy\n",
    "The default spaCy NLP pipeline does several processing steps including __tokenization__, *part of speech tagging*, lemmatization, *dependency parsing* and *Named Entity Recognition* (ignore the ones in *italics* for today). \n",
    "\n",
    "\n",
    "SpaCy produces a `Doc` object that contains `Span`s (sentences) and `Token`s (words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = en_core_web_sm.load()\n",
    "# un-comment the lines above, if you get 'ModuleNotFoundError'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "txt = alice_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/Desktop/Didattica/AIS/2semestre/NLU/nlu_venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# process the document\n",
    "doc = nlp(txt, disable=[\"tagger\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: 'Alice'\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was beginning to get very tired of sitting by her sister on the\n",
      "bank, and of having nothing to do: once or twice she had peeped into the\n",
      "book her sister was reading, but it had no pictures or conversations in\n",
      "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
      "conversation?'\n",
      "\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(doc[1]))\n",
    "print(\"first sentence: '{}'\".format(list(doc.sents)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37033\n",
      "1558\n"
     ]
    }
   ],
   "source": [
    "# access list of tokens (Token objects)\n",
    "print(len(doc))\n",
    "# access list of sentences (Span objects)\n",
    "print(len(list(doc.sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2. Tokenization and Sentence Segmentation with NLTK\n",
    "NLTK's [tokenize](https://www.nltk.org/api/nltk.tokenize.html) package provides similar functionality using the methods below.\n",
    "\n",
    "- `word_tokenize` \n",
    "- `sent_tokenize`\n",
    "\n",
    "There are several tokenizer available (read documentation for more information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK tokenizer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33494\n",
      "1625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alice_words_nltk = nltk.word_tokenize(alice_chars)\n",
    "alice_sents_nltk = nltk.sent_tokenize(alice_chars)\n",
    "print(len(alice_words_nltk))\n",
    "print(len(alice_sents_nltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first token: '['\n",
      "first sentence: '[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I.'\n"
     ]
    }
   ],
   "source": [
    "print(\"first token: '{}'\".format(alice_words_nltk[0]))\n",
    "print(\"first sentence: '{}'\".format(alice_sents_nltk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lab Exercise\n",
    "- Load another corpus from Gutenberg (e.g. `milton-paradise.txt`)\n",
    "- Compute descriptive statistics on the __reference__ sentences and tokens.\n",
    "- Compute descriptive statistics in the __automatically__ processed corpus\n",
    "    - both with `spacy` and `nltk`\n",
    "- Compute lowercased lexicons for all 3 versions of the corpus\n",
    "    - compare lexicon sizes\n",
    "- Compute frequency distribution for all 3 versions of the corpus\n",
    "    - compare top N frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars: [\n",
      "words: [\n",
      "sents: ['[', 'Paradise', 'Lost', 'by', 'John', 'Milton', '1667', ']']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/thomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "\n",
    "corpus = 'milton-paradise.txt'\n",
    "\n",
    "corpus_chars = nltk.corpus.gutenberg.raw(corpus)\n",
    "print('chars:', corpus_chars[0])\n",
    "corpus_words = nltk.corpus.gutenberg.words(corpus)\n",
    "print('words:', corpus_words[0])\n",
    "corpus_sents = nltk.corpus.gutenberg.sents(corpus)\n",
    "print('sents:', corpus_sents[0])\n",
    "\n",
    "#number of rows to select from computed frequencies\n",
    "N=10\n",
    "\n",
    "def my_statistics(chars, words, sents):\n",
    "    \n",
    "    total_chars = len(chars)\n",
    "    total_words = len(words)\n",
    "    total_sents = len(sents)\n",
    "    word_lens = [len(word) for word in words]\n",
    "    sent_lens = [len(sentence) for sentence in sents]\n",
    "    chars_in_sents = [sum(len(word) for word in sent) for sent in sents]\n",
    "    #chars_in_sents = [len(''.join(sent)) for sent in sents]\n",
    "    \n",
    "    word_per_sent = round(len(words) / len(sents))\n",
    "    char_per_word = round(sum(word_lens) / len(words))\n",
    "    #char_per_sent = round(sum(word_lens) / len(sents))\n",
    "    #Alternatively\n",
    "    char_per_sent = round(sum(chars_in_sents) / len(sents))\n",
    "    \n",
    "    longest_sentence = max(sent_lens)\n",
    "    longest_word = max(word_lens)\n",
    "    shortest_sentence = min(sent_lens)\n",
    "    shortest_word = min(word_lens)\n",
    "    \n",
    "    return total_chars, total_words, total_sents, word_per_sent, char_per_word, char_per_sent, longest_sentence, longest_word, shortest_sentence, shortest_word\n",
    "\n",
    "def nbest(d, n=1):\n",
    "    \"\"\"\n",
    "    get n max values from a dict\n",
    "    :param d: input dict (values are numbers, keys are stings)\n",
    "    :param n: number of values to get (int)\n",
    "    :return: dict of top n key-value pairs\n",
    "    \"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Corpus Descriptive Statistics (Counting)\n",
    "\n",
    "*Corpus* can be described in terms of:\n",
    "\n",
    "- total number of characters\n",
    "- total number of words (_tokens_: includes punctuation, etc.)\n",
    "- total number of sentences\n",
    "\n",
    "- minimum/maximum/average number of character per token\n",
    "- minimum/maximum/average number of words per sentence\n",
    "- minimum/maximum/average number of sentences per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------\n",
      "\n",
      "Total number of characters 468220\n",
      "Total number of words 96825\n",
      "Total number of sents 1851\n",
      "Words per sentence 52\n",
      "Chars per word 4\n",
      "Chars per sentence 203\n",
      "Longest sentence 533\n",
      "Longest word 16\n",
      "Shortest sentence 2\n",
      "shortest word 1\n",
      "---------------------------------------------------\n",
      "\n",
      "SPACY automatic tokenization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomas/Desktop/Didattica/AIS/2semestre/NLU/nlu_venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/thomas/Desktop/Didattica/AIS/2semestre/NLU/nlu_venv/lib/python3.10/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters 468220\n",
      "Total number of words 107373\n",
      "Total number of sents 2234\n",
      "Words per sentence 48\n",
      "Chars per word 4\n",
      "Chars per sentence 174\n",
      "Longest sentence 321\n",
      "Longest word 63\n",
      "Shortest sentence 1\n",
      "shortest word 1\n",
      "---------------------------------------------------\n",
      "\n",
      "NLTK automatic tokenization\n",
      "Total number of characters 468220\n",
      "Total number of words 95716\n",
      "Total number of sents 1835\n",
      "Words per sentence 52\n",
      "Chars per word 4\n",
      "Chars per sentence 253\n",
      "Longest sentence 2658\n",
      "Longest word 18\n",
      "Shortest sentence 2\n",
      "shortest word 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference corpus lowercase lexicon size:  9021\n",
      "Spacy processed lowercase lexicon size:  9144\n",
      "Nltk processed lowercase lexicon size:  9280\n",
      "------------------------------------------------------------\n",
      "\n",
      "Frequency table:  {',': 10198, 'and': 3395, 'the': 2968, ';': 2317, 'to': 2228, 'of': 2050, 'in': 1366, '.': 1254, 'his': 1170, 'with': 1160}\n",
      "Spacy frequency table:  {'\\n': 10425, ',': 10224, 'and': 3392, 'the': 2965, ';': 2303, 'to': 2226, 'of': 2049, 'in': 1366, '.': 1276, 'his': 1169}\n",
      "nltk frequency table:  {',': 10228, 'and': 3391, 'the': 2964, ';': 2326, 'to': 2223, 'of': 2046, 'in': 1364, '.': 1275, 'his': 1169, 'with': 1157}\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('---------------------------------------------------\\n')\n",
    "total_chars, total_words, total_sents, word_per_sent, char_per_word, char_per_sent, longest_sent, longest_word, shortest_sent, shortest_word = my_statistics(corpus_chars, corpus_words, corpus_sents)\n",
    "print('Total number of characters', total_chars)\n",
    "print('Total number of words', total_words)\n",
    "print('Total number of sents', total_sents)\n",
    "print('Words per sentence', word_per_sent)\n",
    "print('Chars per word', char_per_word )\n",
    "print('Chars per sentence', char_per_sent)\n",
    "print('Longest sentence', longest_sent)\n",
    "print('Longest word', longest_word)\n",
    "print('Shortest sentence', shortest_sent)\n",
    "print('shortest word', shortest_word)\n",
    "\n",
    "\n",
    "#Tokenization with spacy\n",
    "print('---------------------------------------------------\\n')\n",
    "print('SPACY tokenization')\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['tagger', 'ner'])\n",
    "txt = corpus_chars\n",
    "doc = nlp(txt)\n",
    "spacy_corpus_chars = doc.text\n",
    "spacy_corpus_words = [word.text for word in doc]\n",
    "spacy_corpus_sents = list(doc.sents)\n",
    "\n",
    "\n",
    "spacy_total_chars, spacy_total_words, spacy_total_sents, spacy_word_per_sent, spacy_char_per_word, spacy_char_per_sent, spacy_longest_sent, spacy_longest_word, spacy_shortest_sent, spacy_shortest_word = my_statistics(spacy_corpus_chars, spacy_corpus_words, spacy_corpus_sents)\n",
    "\n",
    "print('Total number of characters',spacy_total_chars)\n",
    "print('Total number of words', spacy_total_words)\n",
    "print('Total number of sents', spacy_total_sents)\n",
    "print('Words per sentence', spacy_word_per_sent)\n",
    "print('Chars per word', spacy_char_per_word )\n",
    "print('Chars per sentence', spacy_char_per_sent)\n",
    "print('Longest sentence', spacy_longest_sent)\n",
    "print('Longest word', spacy_longest_word)\n",
    "print('Shortest sentence', spacy_shortest_sent)\n",
    "print('shortest word', spacy_shortest_word)\n",
    "\n",
    "\n",
    "\n",
    "#Tokenization with nltk\n",
    "print('---------------------------------------------------\\n')\n",
    "print('NLTK tokenization')\n",
    "nltk_corpus_words = nltk.word_tokenize(corpus_chars)\n",
    "nltk_corpus_sents = nltk.sent_tokenize(corpus_chars)\n",
    "nltk_total_chars, nltk_total_words, nltk_total_sents, nltk_word_per_sent, nltk_char_per_word, nltk_char_per_sent, nltk_longest_sent, nltk_longest_word, nltk_shortest_sent, nltk_shortest_word = my_statistics(corpus_chars, nltk_corpus_words, nltk_corpus_sents)\n",
    "\n",
    "print('Total number of characters', nltk_total_chars)\n",
    "print('Total number of words', nltk_total_words)\n",
    "print('Total number of sents', nltk_total_sents)\n",
    "print('Words per sentence', nltk_word_per_sent)\n",
    "print('Chars per word', nltk_char_per_word )\n",
    "print('Chars per sentence', nltk_char_per_sent)\n",
    "print('Longest sentence', nltk_longest_sent)\n",
    "print('Longest word', nltk_longest_word)\n",
    "print('Shortest sentence', nltk_shortest_sent)\n",
    "print('shortest word', nltk_shortest_word)\n",
    "print('------------------------------------------------------------\\n')\n",
    "\n",
    "\n",
    "\n",
    "#---------------LEXICON-----------------------\n",
    "corpus_lexicon_set = set([w.lower() for w in corpus_words])\n",
    "print('Reference corpus lowercase lexicon size: ',len(corpus_lexicon_set))\n",
    "\n",
    "\n",
    "spacy_corpus_lexicon_set = set([str(w).lower() for w in spacy_corpus_words])\n",
    "print('Spacy processed lowercase lexicon size: ',len(spacy_corpus_lexicon_set))\n",
    "\n",
    "nltk_corpus_lexicon_set = set([w.lower() for w in nltk_corpus_words])\n",
    "print('Nltk processed lowercase lexicon size: ',len(nltk_corpus_lexicon_set))\n",
    "\n",
    "#----------------N most frequent words------------\n",
    "print('------------------------------------------------------------\\n')\n",
    "corpus_lexicon_list = [w.lower() for w in corpus_words]\n",
    "corpus_lowercase_freq_list =  Counter(corpus_lexicon_list) \n",
    "print('Frequency table: ',nbest(corpus_lowercase_freq_list, n=N))\n",
    "\n",
    "\n",
    "spacy_corpus_lexicon_list = [str(w).lower() for w in spacy_corpus_words]\n",
    "spacy_corpus_lowercase_freq_list =  Counter(spacy_corpus_lexicon_list) \n",
    "print('Spacy frequency table: ' , nbest(spacy_corpus_lowercase_freq_list, n=N)) \n",
    "\n",
    "nltk_corpus_lexicon_list = list([w.lower() for w in nltk_corpus_words])\n",
    "nltk_corpus_lowercase_freq_list =  Counter(nltk_corpus_lexicon_list) \n",
    "print('nltk frequency table: ',nbest(nltk_corpus_lowercase_freq_list, n=N)) \n",
    "\n",
    "print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
